[
    {
        "link": "https://arxiv.org/abs/2401.15079",
        "title": "P not equal to NP",
        "authors": [
            "Daniel Cardona Delgado"
        ],
        "primary_subject": "Computational Complexity (cs.CC)",
        "abstract": "This article finds the answer to the question: for any problem from which anon-deterministic algorithm can be derived which verifies whether an answer iscorrect or not in polynomial time (complexity class NP), is it possible tocreate an algorithm that finds the right answer to the problem in polynomialtime (complexity class P)? For this purpose, this article shows a decisionproblem and analyzes it to demonstrate that this problem does not belong to thecomplexity class P, but it belongs to the class NP; doing so it will be provedthat it exists at least one problem that belongs to class NP but not to classP, which means that this article will prove that not all NP problems are P."
    },
    {
        "link": "https://arxiv.org/abs/2401.15081",
        "title": "Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?",
        "authors": [
            "Xiaoming Zhai",
            "Matthew Nyaaba",
            "Wenchao Ma"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "This study aimed to examine an assumption that generative artificialintelligence (GAI) tools can overcome the cognitive intensity that humanssuffer when solving problems. We compared the performance of ChatGPT and GPT-4on 2019 NAEP science assessments with students by cognitive demands of theitems. Fifty-four tasks were coded by experts using a two-dimensional cognitiveload framework, including task cognitive complexity and dimensionality. ChatGPTand GPT-4 responses were scored using the scoring keys of NAEP. The analysis ofthe available data was based on the average student ability scores for studentswho answered each item correctly and the percentage of students who respondedto individual items. Results showed that both ChatGPT and GPT-4 consistentlyoutperformed most students who answered the NAEP science assessments. As thecognitive demand for NAEP tasks increases, statistically higher average studentability scores are required to correctly address the questions. This patternwas observed for students in grades 4, 8, and 12, respectively. However,ChatGPT and GPT-4 were not statistically sensitive to the increase in cognitivedemands of the tasks, except for Grade 4. As the first study focusing oncomparing GAI and K-12 students in problem-solving in science, this findingimplies the need for changes to educational objectives to prepare students withcompetence to work with GAI tools in the future. Education ought to emphasizethe cultivation of advanced cognitive skills rather than depending solely ontasks that demand cognitive intensity. This approach would foster criticalthinking, analytical skills, and the application of knowledge in novelcontexts. Findings also suggest the need for innovative assessment practices bymoving away from cognitive intensity tasks toward creativity and analyticalskills to avoid the negative effects of GAI on testing more efficiently."
    },
    {
        "link": "https://arxiv.org/abs/2401.15088",
        "title": "Design & Implementation of Automatic Machine Condition Monitoring and Maintenance System in Limited Resource Situations",
        "authors": [
            "Abu Hanif Md. Ripon",
            "Muhammad Ahsan Ullah",
            "Arindam Kumar Paul",
            "Md. Mortaza Morshed"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "In the era of the fourth industrial revolution, it is essential to automatefault detection and diagnosis of machineries so that a warning system can bedeveloped that will help to take an appropriate action before any catastrophicdamage. Some machines health monitoring systems are used globally but they areexpensive and need trained personnel to operate and analyse. Predictivemaintenance and occupational health and safety culture are not available due toinadequate infrastructure, lack of skilled manpower, financial crisis, andothers in developing countries. Starting from developing a cost-effective DASfor collecting fault data in this study, the effect of limited data andresources has been investigated while automating the process. To solve thisproblem, A feature engineering and data reduction method has been developedcombining the concepts from wavelets, differential calculus, and signalprocessing. Finally, for automating the whole process, all the necessarytheoretical and practical considerations to develop a predictive model havebeen proposed. The DAS successfully collected the required data from themachine that is 89% accurate compared to the professional manual monitoringsystem. SVM and NN were proposed for the prediction purpose because of theirhigh predicting accuracy greater than 95% during training and 100% duringtesting the new samples. In this study, the combination of the simple algorithmwith a rule-based system instead of a data-intensive system turned out to behybridization by validating with collected data. The outcome of this researchcan be instantly applied to small and medium-sized industries for finding otherissues and developing accordingly. As one of the foundational studies inautomatic FDD, the findings and procedure of this study can lead others toextend, generalize, or add other dimensions to FDD automation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15089",
        "title": "Accelerating Material Property Prediction using Generically Complete Isometry Invariants",
        "authors": [
            "Jonathan Balasingham",
            "Viktor Zamaraev",
            "Vitaliy Kurlin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Material or crystal property prediction using machine learning has grownpopular in recent years as it provides a computationally efficient replacementto classical simulation methods. A crucial first step for any of thesealgorithms is the representation used for a periodic crystal. While similarobjects like molecules and proteins have a finite number of atoms and theirrepresentation can be built based upon a finite point cloud interpretation,periodic crystals are unbounded in size, making their representation morechallenging. In the present work, we adapt the Pointwise Distance Distribution(PDD), a continuous and generically complete isometry invariant for periodicpoint sets, as a representation for our learning algorithm. While the PDD iseffective in distinguishing periodic point sets up to isometry, there is noconsideration for the composition of the underlying material. We develop atransformer model with a modified self-attention mechanism that can utilize thePDD and incorporate compositional information via a spatial encoding method.This model is tested on the crystals of the Materials Project and Jarvis-DFTdatabases and shown to produce accuracy on par with state-of-the-art methodswhile being several times faster in both training and prediction time."
    },
    {
        "link": "https://arxiv.org/abs/2401.15098",
        "title": "Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning",
        "authors": [
            "Chaofan Pan",
            "Xin Yang",
            "Hao Wang",
            "Wei Wei",
            "Tianrui Li"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Continual reinforcement learning (CRL) empowers RL agents with the ability tolearn from a sequence of tasks, preserving previous knowledge and leveraging itto facilitate future learning. However, existing methods often focus ontransferring low-level knowledge across similar tasks, which neglects thehierarchical structure of human cognitive control, resulting in insufficientknowledge transfer across diverse tasks. To enhance high-level knowledgetransfer, we propose a novel framework named Hi-Core (Hierarchical knowledgetransfer for Continual reinforcement learning), which is structured in twolayers: 1) the high-level policy formulation which utilizes the powerfulreasoning ability of the Large Language Model (LLM) to set goals and 2) thelow-level policy learning through RL which is oriented by high-level goals.Moreover, the knowledge base (policy library) is constructed to store policiesthat can be retrieved for hierarchical knowledge transfer. Experimentsconducted in MiniGrid have demonstrated the effectiveness of Hi-Core inhandling diverse CRL tasks, outperforming popular baselines."
    },
    {
        "link": "https://arxiv.org/abs/2401.15103",
        "title": "PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression",
        "authors": [
            "Min Wu",
            "Weijun Li",
            "Lina Yu",
            "Wenqiang Li",
            "Jingyi Liu",
            "Yanjie Li",
            "Meilan Hao"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Symbolic regression aims to derive interpretable symbolic expressions fromdata in order to better understand and interpret data. %which plays animportant role in knowledge discovery and interpretable machine learning.In this study, a symbolic network called PruneSymNet is proposed for symbolicregression. This is a novel neural network whose activation function consistsof common elementary functions and operators. The whole network isdifferentiable and can be trained by gradient descent method. Each subnetworkin the network corresponds to an expression, and our goal is to extract suchsubnetworks to get the desired symbolic expression.Therefore, a greedy pruning algorithm is proposed to prune the network into asubnetwork while ensuring the accuracy of data fitting. The proposed greedypruning algorithm preserves the edge with the least loss in each pruning, butgreedy algorithm often can not get the optimal solution. In order to alleviatethis problem, we combine beam search during pruning to obtain multiplecandidate expressions each time, and finally select the expression with thesmallest loss as the final result. It was tested on the public data set andcompared with the current popular algorithms. The results showed that theproposed algorithm had better accuracy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15106",
        "title": "Decision Theoretic Foundations for Experiments Evaluating Human Decisions",
        "authors": [
            "Jessica Hullman",
            "Alex Kale",
            "Jason Hartline"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Decision-making with information displays is a key focus of research in areaslike explainable AI, human-AI teaming, and data visualization. However, whatconstitutes a decision problem, and what is required for an experiment to becapable of concluding that human decisions are flawed in some way, remain opento speculation. We present a widely applicable definition of a decision problemsynthesized from statistical decision theory and information economics. Weargue that to attribute loss in human performance to forms of bias, anexperiment must provide participants with the information that a rational agentwould need to identify the normative decision. We evaluate the extent to whichrecent evaluations of decision-making from the literature on AI-assisteddecisions achieve this criteria. We find that only 6 (17\\%) of 35 studies thatclaim to identify biased behavior present participants with sufficientinformation to characterize their behavior as deviating from gooddecision-making. We motivate the value of studying well-defined decisionproblems by describing a characterization of performance losses they allow usto conceive. In contrast, the ambiguities of a poorly communicated decisionproblem preclude normative interpretation. We conclude with recommendations forpractice."
    },
    {
        "link": "https://arxiv.org/abs/2401.15108",
        "title": "Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition",
        "authors": [
            "Diwas Paudel",
            "Tapas K. Das"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Fast-charging hubs for electric vehicles will soon become part of the newlybuilt infrastructure for transportation electrification across the world. Thesehubs are expected to host many DC fast-charging stations and will admit EVsonly for charging. Like the gasoline refueling stations, fast-charging hubs ina neighborhood will dynamically vary their prices to compete for the same poolof EV owners. These hubs will interact with the electric power network bymaking purchase commitments for a significant part of their power needs in theday-ahead (DA) electricity market and meeting the difference from the real-time(RT) market. Hubs may have supplemental battery storage systems (BSS), whichthey will use for arbitrage. In this paper, we develop a two-step data-drivendynamic pricing methodology for hubs in price competition. We first obtain theDA commitment by solving a stochastic DA commitment model. Thereafter we obtainthe hub pricing strategies by modeling the game as a competitive Markovdecision process (CMDP) and solving it using a multi-agent deep reinforcementlearning (MADRL) approach. We develop a numerical case study for a pricing gamebetween two charging hubs. We solve the case study with our methodology byusing combinations of two different DRL algorithms, DQN and SAC, and twodifferent neural networks (NN) architectures, a feed-forward (FF) neuralnetwork, and a multi-head attention (MHA) neural network. We construct ameasure of collusion (index) using the hub profits. A value of zero for thisindex indicates no collusion (perfect competition) and a value of one indicatesfull collusion (monopolistic behavior). Our results show that the collusionindex varies approximately between 0.14 and 0.45 depending on the combinationsof the algorithms and the architectures chosen by the hubs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15109",
        "title": "Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms",
        "authors": [
            "Louis Rosenberg",
            "Gregg Willcox",
            "Hans Schumann",
            "Ganesh Mani"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Swarm Intelligence (SI) is a natural phenomenon that enables biologicalgroups to amplify their combined intellect by forming real-time systems.Artificial Swarm Intelligence (or Swarm AI) is a technology that enablesnetworked human groups to amplify their combined intelligence by formingsimilar systems. In the past, swarm-based methods were constrained to narrowlydefined tasks like probabilistic forecasting and multiple-choice decisionmaking. A new technology called Conversational Swarm Intelligence (CSI) wasdeveloped in 2023 that amplifies the decision-making accuracy of networkedhuman groups through natural conversational deliberations. The current studyevaluated the ability of real-time groups using a CSI platform to take a commonIQ test known as Raven's Advanced Progressive Matrices (RAPM). First, abaseline group of participants took the Raven's IQ test by traditional survey.This group averaged 45.6% correct. Then, groups of approximately 35 individualsanswered IQ test questions together using a CSI platform called Thinkscape.These groups averaged 80.5% correct. This places the CSI groups in the 97thpercentile of IQ test-takers and corresponds to an effective IQ increase of 28points (p<0.001). This is an encouraging result and suggests that CSI is apowerful method for enabling conversational collective intelligence in large,networked groups. In addition, because CSI is scalable across groups ofpotentially any size, this technology may provide a viable pathway to buildinga Collective Superintelligence."
    },
    {
        "link": "https://arxiv.org/abs/2401.15113",
        "title": "Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data",
        "authors": [
            "Konstantin A. Maslov",
            "Claudio Persello",
            "Thomas Schellenberger",
            "Alfred Stein"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate global glacier mapping is critical for understanding climate changeimpacts. It is challenged by glacier diversity, difficult-to-classify debrisand big data processing. Here we propose Glacier-VisionTransformer-U-Net(GlaViTU), a convolutional-transformer deep learning model, and five strategiesfor multitemporal global-scale glacier mapping using open satellite imagery.Assessing the spatial, temporal and cross-sensor generalisation shows that ourbest strategy achieves intersection over union >0.85 on previously unobservedimages in most cases, which drops to >0.75 for debris-rich areas such asHigh-Mountain Asia and increases to >0.90 for regions dominated by clean ice.Additionally, adding synthetic aperture radar data, namely, backscatter andinterferometric coherence, increases the accuracy in all regions whereavailable. The calibrated confidence for glacier extents is reported making thepredictions more reliable and interpretable. We also release a benchmarkdataset that covers 9% of glaciers worldwide. Our results support effortstowards automated multitemporal and global glacier mapping."
    },
    {
        "link": "https://arxiv.org/abs/2401.15116",
        "title": "Efficient Online Crowdsourcing with Complex Annotations",
        "authors": [
            "Reshef Meir",
            "Viet-An Nguyen",
            "Xu Chen",
            "Jagdish Ramakrishnan",
            "Udi Weinsberg"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Crowdsourcing platforms use various truth discovery algorithms to aggregateannotations from multiple labelers. In an online setting, however, the mainchallenge is to decide whether to ask for more annotations for each item toefficiently trade off cost (i.e., the number of annotations) for quality of theaggregated annotations. In this paper, we propose a novel approach for generalcomplex annotation (such as bounding boxes and taxonomy paths), that works inan online crowdsourcing setting. We prove that the expected average similarityof a labeler is linear in their accuracy \\emph{conditional on the reportedlabel}. This enables us to infer reported label accuracy in a broad range ofscenarios. We conduct extensive evaluations on real-world crowdsourcing datafrom Meta and show the effectiveness of our proposed online algorithms inimproving the cost-quality trade-off."
    },
    {
        "link": "https://arxiv.org/abs/2401.15118",
        "title": "GeoDecoder: Empowering Multimodal Map Understanding",
        "authors": [
            "Feng Qi",
            "Mian Dai",
            "Zixian Zheng",
            "Chao Wang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents GeoDecoder, a dedicated multimodal model designed forprocessing geospatial information in maps. Built on the BeitGPT architecture,GeoDecoder incorporates specialized expert modules for image and textprocessing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlyingbase map, which inherently encompasses essential details about road andbuilding shapes, relative positions, and other attributes. Through theutilization of rendering techniques, the model seamlessly integrates externaldata and features such as symbol markers, drive trajectories, heatmaps, anduser-defined markers, eliminating the need for extra feature engineering. Thetext module of GeoDecoder accepts various context texts and question prompts,generating text outputs in the style of GPT. Furthermore, the GPT-based modelallows for the training and execution of multiple tasks within the same modelin an end-to-end manner. To enhance map cognition and enable GeoDecoder toacquire knowledge about the distribution of geographic entities in Beijing, wedevised eight fundamental geospatial tasks and conducted pretraining of themodel using large-scale text-image samples. Subsequently, rapid fine-tuning wasperformed on three downstream tasks, resulting in significant performanceimprovements. The GeoDecoder model demonstrates a comprehensive understandingof map elements and their associated operations, enabling efficient andhigh-quality application of diverse geospatial tasks in different businessscenarios."
    },
    {
        "link": "https://arxiv.org/abs/2401.15119",
        "title": "Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections",
        "authors": [
            "Md Khairul Islam",
            "Tyler Valentine",
            "Timothy Joowon Sue",
            "Ayush Karmacharya",
            "Luke Neil Benham",
            "Zhengguang Wang",
            "Kingsley Kim",
            "Judy Fox"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Interpreting deep learning time series models is crucial in understanding themodel's behavior and learning patterns from raw data for real-timedecision-making. However, the complexity inherent in transformer-based timeseries models poses challenges in explaining the impact of individual featureson predictions. In this study, we leverage recent local interpretation methodsto interpret state-of-the-art time series models. To use real-world datasets,we collected three years of daily case data for 3,142 US counties. Firstly, wecompare six transformer-based models and choose the best prediction model forCOVID-19 infection. Using 13 input features from the last two weeks, we canpredict the cases for the next two weeks. Secondly, we present an innovativeway to evaluate the prediction sensitivity to 8 population age groups overhighly dynamic multivariate infection data. Thirdly, we compare our proposedperturbation-based interpretation method with related work, including a totalof eight local interpretation methods. Finally, we apply our framework totraffic and electricity datasets, demonstrating that our approach is genericand can be applied to other time-series domains."
    },
    {
        "link": "https://arxiv.org/abs/2401.15120",
        "title": "Context-driven self-supervised visual learning: Harnessing the environment as a data source",
        "authors": [
            "Lizhen Zhu",
            "James Z. Wang",
            "Wonseuk Lee",
            "Brad Wyble"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual learning often occurs in a specific context, where an agent acquiresskills through exploration and tracking of its location in a consistentenvironment. The historical spatial context of the agent provides a similaritysignal for self-supervised contrastive learning. We present a unique approach,termed Environmental Spatial Similarity (ESS), that complements existingcontrastive learning methods. Using images from simulated, photorealisticenvironments as an experimental setting, we demonstrate that ESS outperformstraditional instance discrimination approaches. Moreover, sampling additionaldata from the same environment substantially improves accuracy and provides newaugmentations. ESS allows remarkable proficiency in room classification andspatial prediction tasks, especially in unfamiliar environments. This learningparadigm has the potential to enable rapid visual learning in agents operatingin new environments with unique visual characteristics. Potentiallytransformative applications span from robotics to space exploration. Our proofof concept demonstrates improved efficiency over methods that rely onextensive, disconnected datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.15121",
        "title": "Expressive Power of ReLU and Step Networks under Floating-Point Operations",
        "authors": [
            "Yeachan Park",
            "Geonho Hwang",
            "Wonyeol Lee",
            "Sejun Park"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The study of the expressive power of neural networks has investigated thefundamental limits of neural networks. Most existing results assume real-valuedinputs and parameters as well as exact operations during the evaluation ofneural networks. However, neural networks are typically executed on computersthat can only represent a tiny subset of the reals and apply inexactoperations. In this work, we analyze the expressive power of neural networksunder a more realistic setup: when we use floating-point numbers andoperations. Our first set of results assumes floating-point operations wherethe significand of a float is represented by finite bits but its exponent cantake any integer value. Under this setup, we show that neural networks using abinary threshold unit or ReLU can memorize any finite input/output pairs andcan approximate any continuous function within a small error. We also showsimilar results on memorization and universal approximation when floating-pointoperations use finite bits for both significand and exponent; these results areapplicable to many popular floating-point formats such as those defined in theIEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16."
    },
    {
        "link": "https://arxiv.org/abs/2401.15122",
        "title": "A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics",
        "authors": [
            "Shengchao Liu",
            "Weitao Du",
            "Yanjing Li",
            "Zhuoxinran Li",
            "Vignesh Bhethanabotla",
            "Nakul Rampal",
            "Omar Yaghi",
            "Christian Borgs",
            "Anima Anandkumar",
            "Hongyu Guo",
            "Jennifer Chayes"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In drug discovery, molecular dynamics (MD) simulation for protein-ligandbinding provides a powerful tool for predicting binding affinities, estimatingtransport properties, and exploring pocket sites. There has been a long historyof improving the efficiency of MD simulations through better numerical methodsand, more recently, by augmenting them with machine learning (ML) methods. Yet,challenges remain, such as accurate modeling of extended-timescale simulations.To address this issue, we propose NeuralMD, the first ML surrogate that canfacilitate numerical MD and provide accurate simulations of protein-ligandbinding dynamics. We propose a principled approach that incorporates a novelphysics-informed multi-grained group symmetric framework. Specifically, wepropose (1) a BindingNet model that satisfies group symmetry using vectorframes and captures the multi-level protein-ligand interactions, and (2) anaugmented neural differential equation solver that learns the trajectory underNewtonian mechanics. For the experiment, we design ten single-trajectory andthree multi-trajectory binding simulation tasks. We show the efficiency andeffectiveness of NeuralMD, with a 2000\u00d7 speedup over standard numericalMD simulation and outperforming all other ML approaches by up to 80\\% under thestability metric. We further qualitatively show that NeuralMD reaches morestable binding predictions compared to other machine learning methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15123",
        "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
        "authors": [
            "Chen Liu",
            "Shibo He",
            "Qihang Zhou",
            "Shizhong Li",
            "Wenchao Meng"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Self-supervised methods have gained prominence in time series anomalydetection due to the scarcity of available annotations. Nevertheless, theytypically demand extensive training data to acquire a generalizablerepresentation map, which conflicts with scenarios of a few available samples,thereby limiting their performance. To overcome the limitation, we propose\\textbf{AnomalyLLM}, a knowledge distillation-based time series anomalydetection approach where the student network is trained to mimic the featuresof the large language model (LLM)-based teacher network that is pretrained onlarge-scale datasets. During the testing phase, anomalies are detected when thediscrepancy between the features of the teacher and student networks is large.To circumvent the student network from learning the teacher network's featureof anomalous samples, we devise two key strategies. 1) Prototypical signals areincorporated into the student network to consolidate the normal featureextraction. 2) We use synthetic anomalies to enlarge the representation gapbetween the two networks. AnomalyLLM demonstrates state-of-the-art performanceon 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset."
    },
    {
        "link": "https://arxiv.org/abs/2401.15124",
        "title": "Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities",
        "authors": [
            "E. Wianto",
            "H. Toba",
            "M. Malinda",
            "Chien-Hsu Chen"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Maintaining a high quality of life through physical activities (PA) toprevent health decline is crucial. However, the relationship betweenindividuals health status, PA preferences, and motion factors is complex. PAdiscussions consistently show a positive correlation with healthy agingexperiences, but no explicit relation to specific types of musculoskeletalexercises. Taking advantage of the increasingly widespread existence ofsmartphones, especially in Indonesia, this research utilizes embedded sensorsfor Human Activity Recognition (HAR). Based on 25 participants data, performingnine types of selected motion, this study has successfully identified importantsensor attributes that play important roles in the right and left hands formuscle strength motions as the basis for developing machine learning modelswith the LSTM algorithm."
    },
    {
        "link": "https://arxiv.org/abs/2401.15127",
        "title": "Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness",
        "authors": [
            "Samaneh Shafee",
            "Alysson Bessani",
            "Pedro M. Ferreira"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Knowledge sharing about emerging threats is crucial in the rapidly advancingfield of cybersecurity and forms the foundation of Cyber Threat Intelligence.In this context, Large Language Models are becoming increasingly significant inthe field of cybersecurity, presenting a wide range of opportunities. Thisstudy explores the capability of chatbots such as ChatGPT, GPT4all,Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identifycybersecurity-related text within Open Source Intelligence. We assess thecapabilities of existing chatbot models for Natural Language Processing tasks.We consider binary classification and Named Entity Recognition as tasks. Thisstudy analyzes well-established data collected from Twitter, derived fromprevious research efforts. Regarding cybersecurity binary classification,Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94,and the open-source GPT4all model achieved an F1-score of 0.90. However,concerning cybersecurity entity recognition, chatbot models have limitationsand are less effective. This study demonstrates the capability of thesechatbots only for specific tasks, such as cybersecurity binary classification,while highlighting the need for further refinement in other tasks, such asNamed Entity Recognition tasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15129",
        "title": "Instantaneous Power Theory Revisited with Classical Mechanics",
        "authors": [
            "Federico Milano",
            "Georgios Tzounas",
            "Ioannis Dassios"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "The paper revisits the concepts of instantaneous active and reactive powersand provides a novel definition for basic circuit elements based on quantitiesutilized in classical mechanics, such as absolute and relative velocity,momentum density, angular momentum and apparent forces. The discussionleverages from recent publications by the authors that interpret the voltageand current as velocities in generalized Lagrangian coordinates. The mainresult of the paper is a general and compact expression for the instantaneousactive and reactive power of inductances, capacitances and resistances as amultivector proportional to the generalized kinetic energy and the geometricfrequency multivector. Several numerical examples considering stationary andtransient sinusoidal and non-sinusoidal conditions are discussed in the casestudy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15132",
        "title": "On the Emergence of Symmetrical Reality",
        "authors": [
            "Zhenliang Zhang",
            "Zeyu Zhang",
            "Ziyuan Jiao",
            "Yao Su",
            "Hangxin Liu",
            "Wei Wang",
            "Song-Chun Zhu"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Artificial intelligence (AI) has revolutionized human cognitive abilities andfacilitated the development of new AI entities capable of interacting withhumans in both physical and virtual environments. Despite the existence ofvirtual reality, mixed reality, and augmented reality for several years,integrating these technical fields remains a formidable challenge due to theirdisparate application directions. The advent of AI agents, capable ofautonomous perception and action, further compounds this issue by exposing thelimitations of traditional human-centered research approaches. It is imperativeto establish a comprehensive framework that accommodates the dual perceptualcenters of humans and AI agents in both physical and virtual worlds. In thispaper, we introduce the symmetrical reality framework, which offers a unifiedrepresentation encompassing various forms of physical-virtual amalgamations.This framework enables researchers to better comprehend how AI agents cancollaborate with humans and how distinct technical pathways of physical-virtualintegration can be consolidated from a broader perspective. We then delve intothe coexistence of humans and AI, demonstrating a prototype system thatexemplifies the operation of symmetrical reality systems for specific tasks,such as pouring water. Subsequently, we propose an instance of an AI-drivenactive assistance service that illustrates the potential applications ofsymmetrical reality. This paper aims to offer beneficial perspectives andguidance for researchers and practitioners in different fields, thuscontributing to the ongoing research about human-AI coexistence in bothphysical and virtual environments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15138",
        "title": "Chaotic Encryption for 10-Gb Ethernet Optical Links",
        "authors": [
            "Adri\u00e1n P\u00e9rez-Resa",
            "Miguel Garcia-Bosque",
            "Carlos S\u00e1nchez-Azqueta",
            "Santiago Celma"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "In this paper, a new physical layer encryption method for optical 10-GbEthernet links is proposed. Necessary modifications to introduce encryption inEthernet 10GBase-R standard have been considered. This security enhancement hasconsisted of a symmetric streaming encryption of the 64b/66b data flow atphysical coding sublayer level thanks to two keystream generators based on achaotic algorithm. The overall system has been implemented and tested in afield programmable gate array. Ethernet traffic has been encrypted,transmitted, and decrypted over a multimode optical link. Experimental resultsare analyzed concluding that it is possible to cipher traffic at this level andhide the complete Ethernet traffic pattern from any passive eavesdropper. Inaddition, no overhead is introduced during encryption, getting no losses in thetotal throughput."
    },
    {
        "link": "https://arxiv.org/abs/2401.15159",
        "title": "RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception and Integrated Compliance",
        "authors": [
            "Rishabh Madan",
            "Skyler Valdez",
            "David Kim",
            "Sujie Fang",
            "Luoyan Zhong",
            "Diego Virtue",
            "Tapomayukh Bhattacharjee"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This paper introduces RABBIT, a novel robot-assisted bed bathing systemdesigned to address the growing need for assistive technologies in personalhygiene tasks. It combines multimodal perception and dual (software andhardware) compliance to perform safe and comfortable physical human-robotinteraction. Using RGB and thermal imaging to segment dry, soapy, and wet skinregions accurately, RABBIT can effectively execute washing, rinsing, and dryingtasks in line with expert caregiving practices. Our system includescustom-designed motion primitives inspired by human caregiving techniques, anda novel compliant end-effector called Scrubby, optimized for gentle andeffective interactions. We conducted a user study with 12 participants,including one participant with severe mobility limitations, demonstrating thesystem's effectiveness and perceived comfort. Supplementary material and videoscan be found on our website https://emprise.cs.cornell.edu/rabbit."
    },
    {
        "link": "https://arxiv.org/abs/2401.15164",
        "title": "AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations",
        "authors": [
            "Naresh Kumar Devulapally",
            "Sidharth Anand",
            "Sreyasee Das Bhattacharjee",
            "Junsong Yuan",
            "Yu-Ping Chang"
        ],
        "primary_subject": "Sound (cs.SD)",
        "abstract": "Analyzing individual emotions during group conversation is crucial indeveloping intelligent agents capable of natural human-machine interaction.While reliable emotion recognition techniques depend on different modalities(text, audio, video), the inherent heterogeneity between these modalities andthe dynamic cross-modal interactions influenced by an individual's uniquebehavioral patterns make the task of emotion recognition very challenging. Thisdifficulty is compounded in group settings, where the emotion and its temporalevolution are not only influenced by the individual but also by externalcontexts like audience reaction and context of the ongoing conversation. Tomeet this challenge, we propose a Multimodal Attention Network that capturescross-modal interactions at various levels of spatial abstraction by jointlylearning its interactive bunch of mode-specific Peripheral and Centralnetworks. The proposed MAN injects cross-modal attention via its Peripheralkey-value pairs within each layer of a mode-specific Central query network. Theresulting cross-attended mode-specific descriptors are then combined using anAdaptive Fusion technique that enables the model to integrate thediscriminative and complementary mode-specific data patterns within aninstance-specific multimodal descriptor. Given a dialogue represented by asequence of utterances, the proposed AMuSE model condenses both spatial andtemporal features into two dense descriptors: speaker-level andutterance-level. This helps not only in delivering better classificationperformance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)in large-scale public datasets but also helps the users in understanding thereasoning behind each emotion prediction made by the model via its MultimodalExplainability Visualization module."
    },
    {
        "link": "https://arxiv.org/abs/2401.15166",
        "title": "Probabilistic Design of Multi-Dimensional Spatially-Coupled Codes",
        "authors": [
            "Canberk \u0130rima\u011fz\u0131",
            "Ata Tanr\u0131kulu",
            "Ahmed Hareedy"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Because of their excellent asymptotic and finite-length performance,spatially-coupled (SC) codes are a class of low-density parity-check codes thatis gaining increasing attention. Multi-dimensional (MD) SC codes areconstructed by connecting copies of an SC code via relocations in order tomitigate various sources of non-uniformity and improve performance in many datastorage and data transmission systems. As the number of degrees of freedom inthe MD-SC code design increases, appropriately exploiting them becomes moredifficult because of the complexity growth of the design process. In thispaper, we propose a probabilistic framework for the MD-SC code design, which isbased on the gradient-descent (GD) algorithm, to design better MD codes andaddress this challenge. In particular, we express the expected number of shortcycles, which we seek to minimize, in the graph representation of the code interms of entries of a probability-distribution matrix that characterizes theMD-SC code design. We then find a locally-optimal probability distribution,which serves as the starting point of a finite-length algorithmic optimizerthat produces the final MD-SC code. We offer the theoretical analysis as wellas the algorithms, and we present experimental results demonstrating that ourMD codes, conveniently called GD-MD codes, have notably lower short cyclenumbers compared with the available state-of-the-art. Moreover, our algorithmsconverge on solutions in few iterations, which confirms the complexityreduction as a result of limiting the search space via the locally-optimalGD-MD distributions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15169",
        "title": "Estimating Cloth Elasticity Parameters From Homogenized Yarn-Level Models",
        "authors": [
            "Joy Xiaoji Zhang",
            "Gene Wei-Chin Lin",
            "Lukas Bode",
            "Hsiao-yu Chen",
            "Tuur Stuyck",
            "Egor Larionov"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "Virtual garment simulation has become increasingly important withapplications in garment design and virtual try-on. However, reproducinggarments faithfully remains a cumbersome process. We propose an end-to-endmethod for estimating parameters of shell material models corresponding to realfabrics with minimal priors. Our method determines yarn model properties frominformation directly obtained from real fabrics, unlike methods that requireexpensive specialized capture systems. We use an extended homogenization methodto match yarn-level and shell-level hyperelastic energies with respect to arange of surface deformations represented by the first and second fundamentalforms, including bending along the diagonal to warp and weft directions. Weoptimize the parameters of a shell deformation model involving uncoupledbending and membrane energies. This allows the simulated model to exhibitnonlinearity and anisotropy seen in real cloth. Finally, we validate ourresults with quantitative and visual comparisons against real world fabricsthrough stretch tests and drape experiments. Our homogenized shell models notonly capture the characteristics of underlying yarn patterns, but also exhibitdistinct behaviors for different yarn materials."
    },
    {
        "link": "https://arxiv.org/abs/2401.15170",
        "title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks",
        "authors": [
            "Zackary Okun Dunivin"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Qualitative coding, or content analysis, extracts meaning from text todiscern quantitative patterns across a corpus of texts. Recently, advances inthe interpretive abilities of large language models (LLMs) offer potential forautomating the coding process (applying category labels to texts), therebyenabling human researchers to concentrate on more creative research aspects,while delegating these interpretive tasks to AI. Our case study comprises a setof socio-historical codes on dense, paragraph-long passages representative of ahumanistic study. We show that GPT-4 is capable of human-equivalentinterpretations, whereas GPT-3.5 is not. Compared to our human-derived goldstandard, GPT-4 delivers excellent intercoder reliability (Cohen's \u03ba\u22650.79) for 3 of 9 codes, and substantial reliability (\u03ba\u22650.6) for 8of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes(mean(\u03ba)=0.34; max(\u03ba)=0.55). Importantly, we find that codingfidelity improves considerably when the LLM is prompted to give rationalejustifying its coding decisions (chain-of-thought reasoning). We present theseand other findings along with a set of best practices for adapting traditionalcodebooks for LLMs. Our results indicate that for certain codebooks,state-of-the-art LLMs are already adept at large-scale content analysis.Furthermore, they suggest the next generation of models will likely render AIcoding a viable option for a majority of codebooks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15174",
        "title": "Large Language Models for Multi-Modal Human-Robot Interaction",
        "authors": [
            "Chao Wang",
            "Stephan Hasler",
            "Daniel Tanneberg",
            "Felix Ocker",
            "Frank Joublin",
            "Antonello Ceravola",
            "Joerg Deigmoeller",
            "Michael Gienger"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This paper presents an innovative large language model (LLM)-based roboticsystem for enhancing multi-modal human-robot interaction (HRI). Traditional HRIsystems relied on complex designs for intent estimation, reasoning, andbehavior generation, which were resource-intensive. In contrast, our systemempowers researchers and practitioners to regulate robot behavior through threekey aspects: providing high-level linguistic guidance, creating \"atomics\" foractions and expressions the robot can use, and offering a set of examples.Implemented on a physical robot, it demonstrates proficiency in adapting tomulti-modal inputs and determining the appropriate manner of action to assisthumans with its arms, following researchers' defined guidelines.Simultaneously, it coordinates the robot's lid, neck, and ear movements withspeech output to produce dynamic, multi-modal expressions. This showcases thesystem's potential to revolutionize HRI by shifting from conventional, manualstate-and-flow design methods to an intuitive, guidance-based, andexample-driven approach."
    },
    {
        "link": "https://arxiv.org/abs/2401.15175",
        "title": "Kitchen Food Waste Image Segmentation and Classification for Compost Nutrients Estimation",
        "authors": [
            "Raiyan Rahman",
            "Mohsena Chowdhury",
            "Yueyang Tang",
            "Huayi Gao",
            "George Yin",
            "Guanghui Wang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The escalating global concern over extensive food wastage necessitatesinnovative solutions to foster a net-zero lifestyle and reduce emissions. TheLILA home composter presents a convenient means of recycling kitchen scraps anddaily food waste into nutrient-rich, high-quality compost. To capture thenutritional information of the produced compost, we have created and annotateda large high-resolution image dataset of kitchen food waste with segmentationmasks of 19 nutrition-rich categories. Leveraging this dataset, we benchmarkedfour state-of-the-art semantic segmentation models on food waste segmentation,contributing to the assessment of compost quality of Nitrogen, Phosphorus, orPotassium. The experiments demonstrate promising results of using segmentationmodels to discern food waste produced in our daily lives. Based on theexperiments, SegFormer, utilizing MIT-B5 backbone, yields the best performancewith a mean Intersection over Union (mIoU) of 67.09. Class-based results arealso provided to facilitate further analysis of different food waste classes."
    },
    {
        "link": "https://arxiv.org/abs/2401.15182",
        "title": "App Planner: Utilizing Generative AI for Design Thinking in K-12 Mobile App Development Education",
        "authors": [
            "David Kim",
            "Prerna Ravi",
            "Randi Williams",
            "Daeun Yoo"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "App Planner is an interactive Creativity Support Tool for K-12 students,designed to assist in the human-centered problem-solving and design thinkingprocess. By utilizing generative AI chatbot features, App Planner helpsstudents articulate the problem and solution in structured and diverse waysthrough guided conversations via a chat-based interface. This interfacecollaborates with students as a partner rather than a mentor. It assists themin brainstorming and formulating new ideas for applications, provides feedbackon those ideas, and stimulates creative thinking. We mediate theseconversations to follow a design thinking framework that enhances andencourages students to adopt human-centered problem-solving and criticalthinking perspectives. Here we report usability tests with high-school studentswho appreciated App Planner for aiding the app design process and providing newviewpoints on human aspects especially the potential negative impact of theircreation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15186",
        "title": "Algebraic Approach to Approximation",
        "authors": [
            "Libor Barto",
            "Silvia Butti",
            "Alexandr Kazda",
            "Caterina Viola",
            "Stanislav \u017divn\u00fd"
        ],
        "primary_subject": "Computational Complexity (cs.CC)",
        "abstract": "Following the success of the so-called algebraic approach to the study ofdecision constraint satisfaction problems (CSPs), exact optimization of valuedCSPs, and most recently promise CSPs, we propose an algebraic framework forvalued promise CSPs.To every valued promise CSP we associate an algebraic object, its so-calledvalued minion. Our main result shows that the existence of a homomorphismbetween the associated valued minions implies a polynomial-time reductionbetween the original CSPs. We also show that this general reduction theoremincludes important inapproximability results, for instance, theinapproximability of almost solvable systems of linear equations beyond therandom assignment threshold."
    },
    {
        "link": "https://arxiv.org/abs/2401.15188",
        "title": "CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health",
        "authors": [
            "Sheng Yu",
            "Narjes Nourzad",
            "Randye J. Semple",
            "Yixue Zhao",
            "Emily Zhou",
            "Bhaskar Krishnamachari"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "The COVID-19 pandemic has intensified the urgency for effective andaccessible mental health interventions in people's daily lives. Mobile Health(mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gainedtraction as they expand beyond traditional clinical settings to support dailylife. However, the effectiveness of current mHealth solutions is impeded by thelack of context-awareness, personalization, and modularity to foster theirreusability. This paper introduces CAREForMe, a contextual multi-armed bandit(CMAB) recommendation framework for mental health. Designed withcontext-awareness, personalization, and modularity at its core, CAREForMeharnesses mobile sensing and integrates online learning algorithms with userclustering capability to deliver timely, personalized recommendations. With itsmodular design, CAREForMe serves as both a customizable recommendationframework to guide future research, and a collaborative platform to facilitateinterdisciplinary contributions in mHealth research. We showcase CAREForMe'sversatility through its implementation across various platforms (e.g., Discord,Telegram) and its customization to diverse recommendation features."
    },
    {
        "link": "https://arxiv.org/abs/2401.15189",
        "title": "SBFT Tool Competition 2024 -- Python Test Case Generation Track",
        "authors": [
            "Nicolas Erni",
            "Al-Ameen Mohammed Ali Mohammed",
            "Christian Birchler",
            "Pouria Derakhshanfar",
            "Stephan Lukasczyk",
            "Sebastiano Panichella"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Test case generation (TCG) for Python poses distinctive challenges due to thelanguage's dynamic nature and the absence of strict type information. Previousresearch has successfully explored automated unit TCG for Python, withsolutions outperforming random test generation methods. Nevertheless,fundamental issues persist, hindering the practical adoption of existing testcase generators. To address these challenges, we report on the organization,challenges, and results of the first edition of the Python Testing Competition.Four tools, namely UTBotPython, Klara, Hypothesis Ghostwriter, and Pynguin wereexecuted on a benchmark set consisting of 35 Python source files sampled from 7open-source Python projects for a time budget of 400 seconds. We considered oneconfiguration of each tool for each test subject and evaluated the tools'effectiveness in terms of code and mutation coverage. This paper describes ourmethodology, the analysis of the results together with the competing tools, andthe challenges faced while running the competition experiments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15193",
        "title": "Overview of Sensing Attacks on Autonomous Vehicle Technologies and Impact on Traffic Flow",
        "authors": [
            "Zihao Li",
            "Sixu Li",
            "Hao Zhang",
            "Yang Zhou",
            "Siyang Xie",
            "Yunlong Zhang"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "While perception systems in Connected and Autonomous Vehicles (CAVs), whichencompass both communication technologies and advanced sensors, promise tosignificantly reduce human driving errors, they also expose CAVs to variouscyberattacks. These include both communication and sensing attacks, whichpotentially jeopardize not only individual vehicles but also overall trafficsafety and efficiency. While much research has focused on communicationattacks, sensing attacks, which are equally critical, have garnered lessattention. To address this gap, this study offers a comprehensive review ofpotential sensing attacks and their impact on target vehicles, focusing oncommonly deployed sensors in CAVs such as cameras, LiDAR, Radar, ultrasonicsensors, and GPS. Based on this review, we discuss the feasibility ofintegrating hardware-in-the-loop experiments with microscopic trafficsimulations. We also design baseline scenarios to analyze the macro-levelimpact of sensing attacks on traffic flow. This study aims to bridge theresearch gap between individual vehicle sensing attacks and broader macroscopicimpacts, thereby laying the foundation for future systemic understanding andmitigation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15194",
        "title": "Multimodality in Group Communication Research",
        "authors": [
            "Robin Lange",
            "Brooke Foucault Welles",
            "Gyanendra Sharma",
            "Richard J. Radke",
            "Javier O. Garcia",
            "Christoph Riedl"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Team interactions are often multisensory, requiring members to pick up onverbal, visual, spatial and body language cues. Multimodal research, researchthat captures multiple modes of communication such as audio and visual signals,is therefore integral to understanding these multisensory group communicationprocesses. This type of research has gained traction in biomedical engineeringand neuroscience, but it is unclear the extent to which communication andmanagement researchers conduct multimodal research. Our study finds thatdespite its' utility, multimodal research is underutilized in the communicationand management literature's. This paper then covers introductory guidelines forcreating new multimodal research including considerations for sensors, dataintegration and ethical considerations."
    },
    {
        "link": "https://arxiv.org/abs/2401.15195",
        "title": "Bounded-degree Low Rank Parity Check Codes",
        "authors": [
            "Ermes Franch",
            "Chunlei Li"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Low rank parity check (LRPC) codes are the rank-metric analogue of lowdensity parity check codes. In this paper we investigate a sub-family of LRPCcodes, which have a parity check matrix defined over a subspaceV\u03b1,d=\u27e81,\u03b1,\u2026,\u03b1d\u22121\u27e9Fq\u228aFqm, whereFqm is the finite field of qm elements and d issignificantly smaller than m. These codes are named bounded-degree LRPC(BD-LRPC) codes and are the same as the standard LRPC codes of density 2 whenthe degree d=2, while BD-LRPC codes of degree d>2 constitute a propersubset of LRPC codes of density d. Exploiting the particular structure oftheir parity check matrix, we show that the BD-LRPC codes of degree d canuniquely correct errors of rank weight r when n\u2212k\u2265r+u for certain u\u22651, in contrast to the condition n\u2212k\u2265dr required for the standardLRPC codes, where d\u2265n/(n\u2212k). This underscores the superior decodingcapability of the proposed BD-LRPC codes. As the code length n approachesinfinity, when n/m\u21920, it is shown that u can be chosen as acertain constant, which indicates that the BD-LRPC codes with a code rate ofR can be, with a high probability, uniquely decodable with the decodingradius \u03c1=r/n approaching the Singleton bound 1\u2212R for n\u2192\u221e; andwhen b=n/m is a constant, the BD-LRPC codes can have unique decoding radius\u03c1=1\u2212R\u2212\u03f5 for a small \u03f5, which can easily lead to\u03c1>(1\u2212R)/2 with properly chosen parameters."
    },
    {
        "link": "https://arxiv.org/abs/2401.15196",
        "title": "Regularized Q-Learning with Linear Function Approximation",
        "authors": [
            "Jiachen Xi",
            "Alfredo Garcia",
            "Petar Momcilovic"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Several successful reinforcement learning algorithms make use ofregularization to promote multi-modal policies that exhibit enhancedexploration and robustness. With functional approximation, the convergenceproperties of some of these algorithms (e.g. soft Q-learning) are not wellunderstood. In this paper, we consider a single-loop algorithm for minimizingthe projected Bellman error with finite time convergence guarantees in the caseof linear function approximation. The algorithm operates on two scales: aslower scale for updating the target network of the state-action values, and afaster scale for approximating the Bellman backups in the subspace of the spanof basis vectors. We show that, under certain assumptions, the proposedalgorithm converges to a stationary point in the presence of Markovian noise.In addition, we provide a performance guarantee for the policies derived fromthe proposed algorithm."
    },
    {
        "link": "https://arxiv.org/abs/2401.15199",
        "title": "SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance",
        "authors": [
            "Zahra Kharazian",
            "Tony Lindgren",
            "Sindri Magn\u00fasson",
            "Olof Steinert",
            "Oskar Andersson Reyna"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a description of a real-world, multivariate time seriesdataset collected from an anonymized engine component (called Component X) of afleet of trucks from SCANIA, Sweden. This dataset includes diverse variablescapturing detailed operational data, repair records, and specifications oftrucks while maintaining confidentiality by anonymization. It is well-suitedfor a range of machine learning applications, such as classification,regression, survival analysis, and anomaly detection, particularly when appliedto predictive maintenance scenarios. The large population size and variety offeatures in the format of histograms and numerical counters, along with theinclusion of temporal information, make this real-world dataset unique in thefield. The objective of releasing this dataset is to give a broad range ofresearchers the possibility of working with real-world data from aninternationally well-known company and introduce a standard benchmark to thepredictive maintenance field, fostering reproducible research."
    },
    {
        "link": "https://arxiv.org/abs/2401.15201",
        "title": "Automatically Detecting Confusion and Conflict During Collaborative Learning Using Linguistic, Prosodic, and Facial Cues",
        "authors": [
            "Yingbo Ma",
            "Yukyeong Song",
            "Mehmet Celepkolu",
            "Kristy Elizabeth Boyer",
            "Eric Wiebe",
            "Collin F. Lynch",
            "Maya Israel"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "During collaborative learning, confusion and conflict emerge naturally.However, persistent confusion or conflict have the potential to generatefrustration and significantly impede learners' performance. Early automaticdetection of confusion and conflict would allow us to support earlyinterventions which can in turn improve students' experience with and outcomesfrom collaborative learning. Despite the extensive studies modeling confusionduring solo learning, there is a need for further work in collaborativelearning. This paper presents a multimodal machine-learning framework thatautomatically detects confusion and conflict during collaborative learning. Weused data from 38 elementary school learners who collaborated on a series ofprogramming tasks in classrooms. We trained deep multimodal learning models todetect confusion and conflict using features that were automatically extractedfrom learners' collaborative dialogues, including (1) language-derived featuresincluding TF-IDF, lexical semantics, and sentiment, (2) audio-derived featuresincluding acoustic-prosodic features, and (3) video-derived features includingeye gaze, head pose, and facial expressions. Our results show that multimodalmodels that combine semantics, pitch, and facial expressions detected confusionand conflict with the highest accuracy, outperforming all unimodal models. Wealso found that prosodic cues are more predictive of conflict, and facial cuesare more predictive of confusion. This study contributes to the automatedmodeling of collaborative learning processes and the development of real-timeadaptive support to enhance learners' collaborative learning experience inclassroom contexts."
    },
    {
        "link": "https://arxiv.org/abs/2401.15202",
        "title": "A Cross Entropy Interpretation of R{\u00e9}nyi Entropy for",
        "authors": [
            "Ni Ding",
            "Mohammad Amin Zarrabian",
            "Parastoo Sadeghi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This paper proposes an \u03b1-leakage measure for \u03b1\u2208[0,\u221e) bya cross entropy interpretation of R{\\'{e}}nyi entropy. While R\\'{e}nyi entropywas originally defined as an f-mean for f(t)=exp((1\u2212\u03b1)t), we revealthat it is also a f~-mean cross entropy measure for f~(t)=exp(1\u2212\u03b1\u03b1t). Minimizing this R\\'{e}nyi cross-entropy givesR\\'{e}nyi entropy, by which the prior and posterior uncertainty measures aredefined corresponding to the adversary's knowledge gain on sensitive attributebefore and after data release, respectively. The \u03b1-leakage is proposedas the difference between f~-mean prior and posterior uncertaintymeasures, which is exactly the Arimoto mutual information. This not onlyextends the existing \u03b1-leakage from \u03b1\u2208[1,\u221e) to theoverall R{\\'{e}}nyi order range \u03b1\u2208[0,\u221e) in a well-founded waywith \u03b1=0 referring to nonstochastic leakage, but also reveals that theexisting maximal leakage is a f~-mean of an elementary\u03b1-leakage for all \u03b1\u2208[0,\u221e), which generalizes theexisting pointwise maximal leakage."
    },
    {
        "link": "https://arxiv.org/abs/2401.15203",
        "title": "FedGT: Federated Node Classification with Scalable Graph Transformer",
        "authors": [
            "Zaixi Zhang",
            "Qingyong Hu",
            "Yang Yu",
            "Weibo Gao",
            "Qi Liu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Graphs are widely used to model relational data. As graphs are getting largerand larger in real-world scenarios, there is a trend to store and computesubgraphs in multiple local systems. For example, recently proposed\\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs)distributively on local subgraphs and aggregate GNN parameters with a centralserver. However, existing methods have the following limitations: (1) The linksbetween local subgraphs are missing in subgraph federated learning. This couldseverely damage the performance of GNNs that follow message-passing paradigmsto update node/edge features. (2) Most existing methods overlook the subgraphheterogeneity issue, brought by subgraphs being from different parts of thewhole graph. To address the aforementioned challenges, we propose a scalable\\textbf{Fed}erated \\textbf{G}raph \\textbf{T}ransformer (\\textbf{FedGT}) in thepaper. Firstly, we design a hybrid attention scheme to reduce the complexity ofthe Graph Transformer to linear while ensuring a global receptive field withtheoretical bounds. Specifically, each node attends to the sampled localneighbors and a set of curated global nodes to learn both local and globalinformation and be robust to missing links. The global nodes are dynamicallyupdated during training with an online clustering algorithm to capture the datadistribution of the corresponding local subgraph. Secondly, FedGT computesclients' similarity based on the aligned global nodes with optimal transport.The similarity is then used to perform weighted averaging for personalizedaggregation, which well addresses the data heterogeneity problem. Moreover,local differential privacy is applied to further protect the privacy ofclients. Finally, extensive experimental results on 6 datasets and 2 subgraphsettings demonstrate the superiority of FedGT."
    },
    {
        "link": "https://arxiv.org/abs/2401.15204",
        "title": "LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image Enhancement",
        "authors": [
            "A. Brateanu",
            "R. Balmez",
            "A. Avram",
            "C. C. Orhei"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In recent years, deep learning-based solutions have proven successful in thedomains of image enhancement. This paper introduces LYT-Net, or Lightweight YUVTransformer-based Network, as a novel approach for low-light image enhancement.The proposed architecture, distinct from conventional Retinex-based models,leverages the YUV color space's natural separation of luminance (Y) andchrominance (U and V) to simplify the intricate task of disentangling light andcolor information in images. By utilizing the strengths of transformers, knownfor their capability to capture long-range dependencies, LYT-Net ensures acomprehensive contextual understanding of the image while maintaining reducedmodel complexity. By employing a novel hybrid loss function, our proposedmethod achieves state-of-the-art results on low-light image enhancementdatasets, all while being considerably more compact than its counterparts. Thesource code and pre-trained models are available athttps://github.com/albrateanu/LYT-Net"
    },
    {
        "link": "https://arxiv.org/abs/2401.15206",
        "title": "Backscatter Measurements and Models for RF Sensing Applications in Cluttered Environments",
        "authors": [
            "Dmitry Chizhik",
            "Jinfeng Du",
            "Jakub Sapis",
            "Reinaldo A. Valenzuela",
            "Abhishek Adhikari",
            "Gil Zussman",
            "Manuel A. Almendra",
            "Mauricio Rodriguez",
            "Rodolfo Feick"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "A statistical backscatter channel model for indoor clutter is developed forindoor RF sensing applications based on measurements. A narrowband 28 GHzsounder used a quazi-monostatic radar arrangement with an omnidirectionaltransmit antenna illuminating an indoor scene and a spinning horn receiveantenna less than 1 m away collecting backscattered power as a function ofazimuth. Median average backscatter power was found to vary over a 12 dB range,with average power generally decreasing with increasing room size. Adeterministic model of average backscattered power dependent on distance tonearest wall and wall reflection coefficient reproduces observations with 4.0dB RMS error. Distribution of power variation in azimuth around this average isreproduced within 1 dB by a random azimuth spectrum with a lognormal amplitudedistribution and uniformly random phase. The model is extended to provide powerdistribution over both azimuth and delay (conveying range to scatterer) bycombining azimuthal distribution with published results on power delay profilesin reverberant environments. The statistical model does not require a detailedroom layout description, aiming to reproduce backscatter clutter statistics, asopposed to a deterministic response."
    },
    {
        "link": "https://arxiv.org/abs/2401.15207",
        "title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy",
        "authors": [
            "Yongkang Liu",
            "Yiqun Zhang",
            "Qian Li",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang",
            "Hinrich Sch\u00fctze"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Full-parameter fine-tuning has become the go-to choice for adapting languagemodels (LMs) to downstream tasks due to its excellent performance. As LMs growin size, fine-tuning the full parameters of LMs requires a prohibitively largeamount of GPU memory. Existing approaches utilize zeroth-order optimizer toconserve GPU memory, which can potentially compromise the performance of LMs asnon-zero order optimizers tend to converge more readily on most downstreamtasks. In this paper, we propose a novel optimizer-independent end-to-endhierarchical fine-tuning strategy, HiFT, which only updates a subset ofparameters at each training step. HiFT can significantly reduce the amount ofgradients and optimizer state parameters residing in GPU memory at the sametime, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFTachieves comparable performance to parameter-efficient fine-tuning and standardfull parameter fine-tuning. (2) HiFT supports various optimizers includingAdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\\% GPU memory comparedwith standard full-parameter fine-tuning for 7B model. (4) HiFT enablesfull-parameter fine-tuning of a 7B model on single 48G A6000 with a precisionof 32 using the AdamW optimizer, without using any memory saving techniques."
    },
    {
        "link": "https://arxiv.org/abs/2401.15210",
        "title": "Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model",
        "authors": [
            "Amin Kamali",
            "Verena Kantere",
            "Calisto Zuzarte",
            "Vincent Corvinelli"
        ],
        "primary_subject": "Databases (cs.DB)",
        "abstract": "Query optimizers in relational database management systems (RDBMSs) searchfor execution plans expected to be optimal for a given queries. They useparameter estimates, often inaccurate, and make assumptions that may not holdin practice. Consequently, they may select execution plans that are suboptimalat runtime, when these estimates and assumptions are not valid, which mayresult in poor query performance. Therefore, query optimizers do notsufficiently support robust query optimization. Recent years have seen a surgeof interest in using machine learning (ML) to improve efficiency of datasystems and reduce their maintenance overheads, with promising results obtainedin the area of query optimization in particular. In this paper, inspired bythese advancements, and based on several years of experience of IBM Db2 in thisjourney, we propose Robust Optimization of Queries, (Roq), a holistic frameworkthat enables robust query optimization based on a risk-aware learning approach.Roq includes a novel formalization of the notion of robustness in the contextof query optimization and a principled approach for its quantification andmeasurement based on approximate probabilistic ML. It also includes novelstrategies and algorithms for query plan evaluation and selection. Roq alsoincludes a novel learned cost model that is designed to predict query executioncost and the associated risks and performs query optimization accordingly. Wedemonstrate experimentally that Roq provides significant improvements to robustquery optimization compared to the state-of-the-art."
    },
    {
        "link": "https://arxiv.org/abs/2401.15212",
        "title": "Speed-based Filtration and DBSCAN of Event-based Camera Data with Neuromorphic Computing",
        "authors": [
            "Charles P. Rizzo",
            "Catherine D. Schuman",
            "James S. Plank"
        ],
        "primary_subject": "Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Spiking neural networks are powerful computational elements that pair wellwith event-based cameras (EBCs). In this work, we present two spiking neuralnetwork architectures that process events from EBCs: one that isolates andfilters out events based on their speeds, and another that clusters eventsbased on the DBSCAN algorithm."
    },
    {
        "link": "https://arxiv.org/abs/2401.15213",
        "title": "On inertial iterated Tikhonov methods for solving ill-posed problems",
        "authors": [
            "Joel C. Rabelo",
            "Antonio Leit\u00e3o",
            "Alexandre L. Madureira"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this manuscript we propose and analyze an implicit two-point type method(or inertial method) for obtaining stable approximate solutions to linearill-posed operator equations. The method is based on the iterated Tikhonov (iT)scheme. We establish convergence for exact data, and stability andsemi-convergence for noisy data. Regarding numerical experiments we consider:i) a 2D Inverse Potential Problem, ii) an Image Deblurring Problem; thecomputational efficiency of the method is compared with standardimplementations of the iT method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15219",
        "title": "Harnessing Deep Learning of Point Clouds for Inverse Control of 3D Shape Morphing",
        "authors": [
            "Jue Wang",
            "Dhirodaatto Sarkar",
            "Jiaqi Suo",
            "Alex Chortos"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Shape-morphing devices, a crucial branch in soft robotics, hold significantapplication value in areas like human-machine interfaces, biomimetic robotics,and tools for interacting with biological systems. To achieve three-dimensional(3D) programmable shape morphing (PSM), the deployment of array-based actuatorsis essential. However, a critical knowledge gap impeding the development of 3DPSM is the challenge of controlling the complex systems formed by these softactuator arrays. This study introduces a novel approach, for the first time,representing the configuration of shape morphing devices using point cloud dataand employing deep learning to map these configurations to control inputs. Wepropose Shape Morphing Net (SMNet), a method that realizes the regression frompoint cloud data to high-dimensional continuous vectors. Applied to previous 2DPSM actuator arrays, SMNet significantly enhances control precision from 82.23%to 97.68%. Further, we extend its application to 3D PSM devices with threedifferent actuator mechanisms, demonstrating the universal applicability ofSMNet to the control of 3D shape morphing technologies. In our demonstrations,we confirm the efficacy of inverse control, where 3D PSM devices successfullyreplicate target shapes. These shapes are obtained either through 3D scanningof physical objects or via 3D modeling software. The results show that withinthe deformable range of 3D PSM devices, accurate reproduction of the desiredshapes is achievable. The findings of this research represent a substantialadvancement in soft robotics, particularly for applications demanding intricate3D shape transformations, and establish a foundational framework for futuredevelopments in the field."
    },
    {
        "link": "https://arxiv.org/abs/2401.15221",
        "title": "Designing and Testing a Mobile Application for Collecting WhatsApp Chat Data While Preserving Privacy",
        "authors": [
            "Brennan Schaffner",
            "Archie Brohn",
            "Jason Chee",
            "K.J. Feng",
            "Marshini Chetty"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "It is common practice for researchers to join public WhatsApp chats andscrape their contents for analysis. However, research shows collecting datathis way contradicts user expectations and preferences, even if the data iseffectively public. To overcome these issues, we outline design considerationsfor collecting WhatsApp chat data with improved user privacy by heighteninguser control and oversight of data collection and taking care to minimize thedata researchers collect and process off a user's device. We refer to thesedesign principles as User-Centered Data Sharing (UCDS). To evaluate our UCDSprinciples, we implemented a mobile application representing one possibleinstance of these improved data collection techniques and evaluated theviability of using the app to collect WhatsApp chat data. Second, we surveyedWhatsApp users to gather user perceptions on common existing WhatsApp datacollection methods as well as UCDS methods. Our results show that we were ableto glean similar informative insights into WhatsApp chats using UCDS principlesin our prototype app to common, less privacy-preserving methods. Our surveyshowed that methods following the UCDS principles are preferred by usersbecause they offered users more control over the data collection process.Future user studies could further expand upon UCDS principles to overcomecomplications of researcher-to-group communication in research on WhatsAppchats and evaluate these principles in other data sharing contexts."
    },
    {
        "link": "https://arxiv.org/abs/2401.15222",
        "title": "Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection",
        "authors": [
            "Abdullateef I. Almudaifer",
            "Tobias O`Leary",
            "Whitney Covington",
            "JaMor Hairston",
            "Zachary Deitch",
            "Ankit Anand",
            "Caleb M. Carroll",
            "Estera Crisan",
            "William Bradford",
            "Lauren Walter",
            "Eaton Ellen",
            "Sue S. Feldman",
            "John D. Osborne"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Background: The semantics of entities extracted from a clinical text can bedramatically altered by modifiers, including entity negation, uncertainty,conditionality, severity, and subject. Existing models for determiningmodifiers of clinical entities involve regular expression or features weightsthat are trained independently for each modifier.Methods: We develop and evaluate a multi-task transformer architecture designwhere modifiers are learned and predicted jointly using the publicly availableSemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set thatcontains modifiers shared with SemEval as well as novel modifiers specific forOUD. We evaluate the effectiveness of our multi-task learning approach versuspreviously published systems and assess the feasibility of transfer learningfor clinical entity modifiers when only a portion of clinical modifiers areshared.Results: Our approach achieved state-of-the-art results on the ShARe corpusfrom SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,1.7% on unweighted accuracy, and 10% on micro F1 scores.Conclusions: We show that learned weights from our shared model can beeffectively transferred to a new partially matched data set, validating the useof transfer learning for clinical text modifiers"
    },
    {
        "link": "https://arxiv.org/abs/2401.15223",
        "title": "Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis",
        "authors": [
            "Mingshi Li",
            "Dusan Grujicic",
            "Steven De Saeger",
            "Stien Heremans",
            "Ben Somers",
            "Matthew B. Blaschko"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In recent years, machine learning has become crucial in remote sensinganalysis, particularly in the domain of Land-use/Land-cover (LULC). The synergyof machine learning and satellite imagery analysis has demonstrated significantproductivity in this field, as evidenced by several studies. A notablechallenge within this area is the semantic segmentation mapping of land usageover extensive territories, where the accessibility of accurate land-use dataand the reliability of ground truth land-use labels pose significantdifficulties. For example, providing a detailed and accurate pixel-wise labeleddataset of the Flanders region, a first-level administrative division ofBelgium, can be particularly insightful. Yet there is a notable lack ofregulated, formalized datasets and workflows for such studies in many regionsglobally. This paper introduces a comprehensive approach to addressing thesegaps. We present a densely labeled ground truth map of Flanders paired withSentinel-2 satellite imagery. Our methodology includes a formalized datasetdivision and sampling method, utilizing the topographic map layout'Kaartbladversnijdingen,' and a detailed semantic segmentation model trainingpipeline. Preliminary benchmarking results are also provided to demonstrate theefficacy of our approach."
    },
    {
        "link": "https://arxiv.org/abs/2401.15229",
        "title": "Evolving AI Risk Management: A Maturity Model based on the NIST AI Risk Management Framework",
        "authors": [
            "Ravit Dotan",
            "Borhane Blili-Hamelin",
            "Ravi Madhavan",
            "Jeanna Matthews",
            "Joshua Scarpino"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "Researchers, government bodies, and organizations have been repeatedlycalling for a shift in the responsible AI community from general principles totangible and operationalizable practices in mitigating the potentialsociotechnical harms of AI. Frameworks like the NIST AI RMF embody an emergingconsensus on recommended practices in operationalizing sociotechnical harmmitigation. However, private sector organizations currently lag far behind thisemerging consensus. Implementation is sporadic and selective at best. At worst,it is ineffective and can risk serving as a misleading veneer of trustworthyprocesses, providing an appearance of legitimacy to substantively harmfulpractices. In this paper, we provide a foundation for a framework forevaluating where organizations sit relative to the emerging consensus onsociotechnical harm mitigation best practices: a flexible maturity model basedon the NIST AI RMF."
    },
    {
        "link": "https://arxiv.org/abs/2401.15232",
        "title": "How Beginning Programmers and Code LLMs (Mis)read Each Other",
        "authors": [
            "Sydney Nguyen",
            "Hannah McLean Babe",
            "Yangtian Zi",
            "Arjun Guha",
            "Carolyn Jane Anderson",
            "Molly Q Feldman"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Generative AI models, specifically large language models (LLMs), have madestrides towards the long-standing goal of text-to-code generation. Thisprogress has invited numerous studies of user interaction. However, less isknown about the struggles and strategies of non-experts, for whom each step ofthe text-to-code problem presents challenges: describing their intent innatural language, evaluating the correctness of generated code, and editingprompts when the generated code is incorrect. This paper presents a large-scalecontrolled study of how 120 beginning coders across three academic institutionsapproach writing and editing prompts. A novel experimental design allows us totarget specific steps in the text-to-code process and reveals that beginnersstruggle with writing and editing prompts, even for problems at their skilllevel and when correctness is automatically determined. Our mixed-methodsevaluation provides insight into student processes and perceptions with keyimplications for non-expert Code LLM use within and outside of education."
    },
    {
        "link": "https://arxiv.org/abs/2401.15234",
        "title": "Moving beyond Deletions: Program Simplification via Diverse Program Transformations",
        "authors": [
            "Haibo Wang",
            "Zezhong Xing",
            "Zheng Wang",
            "Chengnian Sun",
            "Shin Hwei Tan"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "To reduce the complexity of software, Developers manually simplify program(known as developer-induced program simplification in this paper) to reduce itscode size yet preserving its functionality but manual simplification istime-consuming and error-prone. To reduce manual effort, rule-based approaches(e.g., refactoring) and deletion-based approaches (e.g., delta debugging) canbe potentially applied to automate developer-induced program simplification.However, as there is little study on how developers simplify programs inOpen-source Software (OSS) projects, it is unclear whether these approaches canbe effectively used for developer-induced program simplification. Hence, wepresent the first study of developer-induced program simplification in OSSprojects, focusing on the types of program transformations used, themotivations behind simplifications, and the set of program transformationscovered by existing refactoring types. Our study of 382 pull requests from 296projects reveals that there exist gaps in applying existing approaches forautomating developer-induced program simplification. and outlines the criteriafor designing automatic program simplification techniques. Inspired by ourstudy and to reduce the manual effort in developer-induced programsimplification, we propose SimpT5, a tool that can automatically producesimplified programs (semantically-equivalent programs with reduced source linesof code). SimpT5 is trained based on our collected dataset of 92,485 simplifiedprograms with two heuristics: (1) simplified line localization that encodeslines changed in simplified programs, and (2)checkers that measure the qualityof generated programs. Our evaluation shows that SimpT5 are more effective thanprior approaches in automating developer-induced program simplification."
    },
    {
        "link": "https://arxiv.org/abs/2401.15236",
        "title": "Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones",
        "authors": [
            "Beatrice Alessandra Motetti",
            "Luca Crupi",
            "Mustafa Omer Mohammed Elamin Elshaigi",
            "Matteo Risso",
            "Daniele Jahier Pagliari",
            "Daniele Palossi",
            "Alessio Burrello"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Sub-10cm diameter nano-drones are gaining momentum thanks to theirapplicability in scenarios prevented to bigger flying drones, such as in narrowenvironments and close to humans. However, their tiny form factor also bringstheir major drawback: ultra-constrained memory and processors for the onboardexecution of their perception pipelines. Therefore, lightweight deeplearning-based approaches are becoming increasingly popular, stressing howcomputational efficiency and energy-saving are paramount as they can make thedifference between a fully working closed-loop system and a failing one. Inthis work, to maximize the exploitation of the ultra-limited resources aboardnano-drones, we present a novel adaptive deep learning-based mechanism for theefficient execution of a vision-based human pose estimation task. We leveragetwo State-of-the-Art (SoA) convolutional neural networks (CNNs) with differentregression performance vs. computational costs trade-offs. By combining theseCNNs with three novel adaptation strategies based on the output's temporalconsistency and on auxiliary tasks to swap the CNN being executed proactively,we present six different systems. On a real-world dataset and the actualnano-drone hardware, our best-performing system, compared to executing only thebigger and most accurate SoA model, shows 28% latency reduction while keepingthe same mean absolute error (MAE), 3% MAE reduction while being iso-latency,and the absolute peak performance, i.e., 6% better than SoA model."
    },
    {
        "link": "https://arxiv.org/abs/2401.15238",
        "title": "Deep Learning with Tabular Data: A Self-supervised Approach",
        "authors": [
            "Tirth Kiranbhai Vyas"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We have described a novel approach for training tabular data using theTabTransformer model with self-supervised learning. Traditional machinelearning models for tabular data, such as GBDT are being widely used though ourpaper examines the effectiveness of the TabTransformer which is a Transformerbased model optimised specifically for tabular data. The TabTransformercaptures intricate relationships and dependencies among features in tabulardata by leveraging the self-attention mechanism of Transformers. We have used aself-supervised learning approach in this study, where the TabTransformerlearns from unlabelled data by creating surrogate supervised tasks, eliminatingthe need for the labelled data. The aim is to find the most effectiveTabTransformer model representation of categorical and numerical features. Toaddress the challenges faced during the construction of various input settingsinto the Transformers. Furthermore, a comparative analysis is also beenconducted to examine performance of the TabTransformer model against baselinemodels such as MLP and supervised TabTransformer.The research has presented with a novel approach by creating various variantsof TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT whichhas helped to increase the effective capturing of the underlying relationshipbetween various features of the tabular dataset by constructing optimal inputs.And further we have employed a self-supervised learning approach in the form ofa masking-based unsupervised setting for tabular data. The findings shed lighton the best way to represent categorical and numerical features, emphasizingthe TabTransormer performance when compared to established machine learningmodels and other self-supervised learning methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15239",
        "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
        "authors": [
            "Peizhuo Lv",
            "Hualong Ma",
            "Kai Chen",
            "Jiachen Zhou",
            "Shengzhi Zhang",
            "Ruigang Liang",
            "Shenchen Zhu",
            "Pan Li",
            "Yingjun Zhang"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have beentrained using deep learning algorithms. To protect the Intellectual Property(IP) of the original owners over such DNN models, backdoor-based watermarkshave been extensively studied. However, most of such watermarks fail upon modelextraction attack, which utilizes input samples to query the target model andobtains the corresponding outputs, thus training a substitute model using suchinput-output pairs. In this paper, we propose a novel watermark to protect IPof DNN models against model extraction, named MEA-Defender. In particular, weobtain the watermark by combining two samples from two source classes in theinput domain and design a watermark loss function that makes the output domainof the watermark within that of the main task samples. Since both the inputdomain and the output domain of our watermark are indispensable parts of thoseof the main task samples, the watermark will be extracted into the stolen modelalong with the main task during model extraction. We conduct extensiveexperiments on four model extraction attacks, using five datasets and sixmodels trained based on supervised learning and self-supervised learningalgorithms. The experimental results demonstrate that MEA-Defender is highlyrobust against different model extraction attacks, and various watermarkremoval/detection approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15240",
        "title": "Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games",
        "authors": [
            "Yang Cai",
            "Haipeng Luo",
            "Chen-Yu Wei",
            "Weiqiang Zheng"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We study policy optimization algorithms for computing correlated equilibriain multi-player general-sum Markov Games. Previous results achieveO(T\u22121/2) convergence rate to a correlated equilibrium and an acceleratedO(T\u22123/4) convergence rate to the weaker notion of coarse correlatedequilibrium. In this paper, we improve both results significantly by providingan uncoupled policy optimization algorithm that attains a near-optimalO~(T\u22121) convergence rate for computing a correlated equilibrium.Our algorithm is constructed by combining two main elements (i) smooth valueupdates and (ii) the optimistic-follow-the-regularized-leader algorithm withthe log barrier regularizer."
    },
    {
        "link": "https://arxiv.org/abs/2401.15241",
        "title": "Unlearning Reveals the Influential Training Data of Language Models",
        "authors": [
            "Masaru Isonuma",
            "Ivan Titov"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "In order to enhance the performance of language models while mitigating therisks of generating harmful content, it is crucial to identify which trainingdataset affects the model's outputs. Ideally, we can measure the influence ofeach dataset by removing it from training; however, it is prohibitivelyexpensive to retrain a model multiple times. This paper presents UnTrac, whichestimates the influence of a training dataset by unlearning it from the trainedmodel. UnTrac is extremely simple; each training dataset is unlearned bygradient ascent, and we evaluate how much the model's predictions change afterunlearning. We empirically examine if our methods can assess the influence ofpretraining datasets on generating toxic, biased, and untruthful content.Experimental results demonstrate that our method estimates their influence muchmore accurately than existing methods while requiring neither excessive memoryspace nor multiple model checkpoints."
    },
    {
        "link": "https://arxiv.org/abs/2401.15245",
        "title": "GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation",
        "authors": [
            "Bar\u0131\u015f Y\u0131ld\u0131r\u0131m",
            "Murat Kurt"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "This paper presents a plugin that adds a representation of homogeneous andheterogeneous, optically thick, translucent materials on the Blender 3Dmodeling tool. The working principle of this plugin is based on a combinationof Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-basedsubsurface scattering method (GenSSS). The proposed plugin has been implementedusing Mitsuba renderer, which is an open source rendering software. Theproposed plugin has been validated on measured subsurface scattering data. It'sshown that the proposed plugin visualizes homogeneous and heterogeneoussubsurface scattering effects, accurately, compactly and computationallyefficiently."
    },
    {
        "link": "https://arxiv.org/abs/2401.15246",
        "title": "Training Differentially Private Ad Prediction Models with Semi-Sensitive Features",
        "authors": [
            "Lynn Chua",
            "Qiliang Cui",
            "Badih Ghazi",
            "Charlie Harrison",
            "Pritish Kamath",
            "Walid Krichene",
            "Ravi Kumar",
            "Pasin Manurangsi",
            "Krishna Giri Narra",
            "Amer Sinha",
            "Avinash Varadarajan",
            "Chiyuan Zhang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Motivated by problems arising in digital advertising, we introduce the taskof training differentially private (DP) machine learning models withsemi-sensitive features. In this setting, a subset of the features is known tothe attacker (and thus need not be protected) while the remaining features aswell as the label are unknown to the attacker and should be protected by the DPguarantee. This task interpolates between training the model with full DP(where the label and all features should be protected) or with label DP (whereall the features are considered known, and only the label should be protected).We present a new algorithm for training DP models with semi-sensitive features.Through an empirical evaluation on real ads datasets, we demonstrate that ouralgorithm surpasses in utility the baselines of (i) DP stochastic gradientdescent (DP-SGD) run on all features (known and unknown), and (ii) a label DPalgorithm run only on the known features (while discarding the unknown ones)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15248",
        "title": "Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective",
        "authors": [
            "Yue Xing",
            "Xiaofeng Lin",
            "Qifan Song",
            "Yi Xu",
            "Belinda Zeng",
            "Guang Cheng"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Pre-training is known to generate universal representations for downstreamtasks in large-scale deep learning such as large language models. Existingliterature, e.g., \\cite{kim2020adversarial}, empirically observe that thedownstream tasks can inherit the adversarial robustness of the pre-trainedmodel. We provide theoretical justifications for this robustness inheritancephenomenon. Our theoretical results reveal that feature purification plays animportant role in connecting the adversarial robustness of the pre-trainedmodel and the downstream tasks in two-layer neural networks. Specifically, weshow that (i) with adversarial training, each hidden node tends to pick onlyone (or a few) feature; (ii) without adversarial training, the hidden nodes canbe vulnerable to attacks. This observation is valid for both supervisedpre-training and contrastive learning. With purified nodes, it turns out thatclean training is enough to achieve adversarial robustness in downstream tasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15258",
        "title": "Foundations of Substructural Dependent Type Theory",
        "authors": [
            "C.B. Aberl\u00e9"
        ],
        "primary_subject": "Logic in Computer Science (cs.LO)",
        "abstract": "This paper presents preliminary work on a general system for integratingdependent types into substructural type systems such as linear logic and lineartype theory. Prior work on this front has generally managed to deliver typesystems possessing either syntax or semantics inclusive of certain practicalapplications, but has struggled to combine these all in one and the samesystem. Toward resolving this difficulty, I propose a novel categoricalinterpretation of substructural dependent types, analogous to the use ofmonoidal categories as models of linear and ordered logic, that encompasses awide class of mathematical and computational examples. On this basis, I developa general framework for substructural dependent type theories, and proceed toprove some essential metatheoretic properties thereof. As an application ofthis framework, I show how it can be used to construct a type theory thatsatisfactorily addresses the problem of effectively representing cutadmissibility for linear sequent calculus in a logical framework."
    },
    {
        "link": "https://arxiv.org/abs/2401.15261",
        "title": "Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes",
        "authors": [
            "Diandian Guo",
            "Deng-Ping Fan",
            "Tongyu Lu",
            "Christos Sakaridis",
            "Luc Van Gool"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The estimation of implicit cross-frame correspondences and the highcomputational cost have long been major challenges in video semanticsegmentation (VSS) for driving scenes. Prior works utilize keyframes, featurepropagation, or cross-frame attention to address these issues. By contrast, weare the first to harness vanishing point (VP) priors for more effectivesegmentation. Intuitively, objects near VPs (i.e., away from the vehicle) areless discernible. Moreover, they tend to move radially away from the VP overtime in the usual case of a forward-facing camera, a straight road, and linearforward motion of the vehicle. Our novel, efficient network for VSS, namedVPSeg, incorporates two modules that utilize exactly this pair of static anddynamic VP priors: sparse-to-dense feature mining (DenseVP) and VP-guidedmotion fusion (MotionVP). MotionVP employs VP-guided motion estimation toestablish explicit correspondences across frames and help attend to the mostrelevant features from neighboring frames, while DenseVP enhances weak dynamicfeatures in distant regions around VPs. These modules operate within acontext-detail framework, which separates contextual features fromhigh-resolution local features at different input resolutions to reducecomputational costs. Contextual and local features are integrated throughcontextualized motion attention (CMA) for the final prediction. Extensiveexperiments on two popular driving segmentation benchmarks, Cityscapes andACDC, demonstrate that VPSeg outperforms previous SOTA methods, with onlymodest computational overhead."
    },
    {
        "link": "https://arxiv.org/abs/2401.15265",
        "title": "A method for constructing quaternary Hermitian self-dual codes and an application to quantum codes",
        "authors": [
            "Masaaki Harada"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We introduce quaternary modified four \u03bc-circulant codes as a modificationof four circulant codes. We give basic properties of quaternary modified four\u03bc-circulant Hermitian self-dual codes. We also construct quaternarymodified four \u03bc-circulant Hermitian self-dual codes having large minimumweights. Two quaternary Hermitian self-dual [56,28,16] codes are constructedfor the first time. These codes improve the previously known lower bound on thelargest minimum weight among all quaternary (linear) [56,28] codes. Inaddition, these codes imply the existence of a quantum [[56,0,16]] code."
    },
    {
        "link": "https://arxiv.org/abs/2401.15266",
        "title": "SAM-based instance segmentation models for the automation of masonry crack detection",
        "authors": [
            "Zehao Ye",
            "Lucy Lovell",
            "Asaad Faramarzi",
            "Jelena Ninic"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Automating visual inspection for capturing defects based on civil structuresappearance is crucial due to its currently labour-intensive and time-consumingnature. An important aspect of automated inspection is image acquisition, whichis rapid and cost-effective considering the pervasive developments in bothsoftware and hardware computing in recent years. Previous studies largelyfocused on concrete and asphalt, with less attention to masonry cracks. Thelatter also lacks publicly available datasets. In this paper, we first presenta corresponding data set for instance segmentation with 1,300 annotated images(640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks,and cracks. We then test several leading algorithms for benchmarking, includingthe latest large-scale model, the prompt-based Segment Anything Model (SAM). Wefine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novelmethods for automation of SAM execution. The first method involves abandoningthe prompt encoder and connecting the SAM encoder to other decoders, while thesecond method introduces a learnable self-generating prompter. In order toensure the seamless integration of the two proposed methods with SAM encodersection, we redesign the feature extractor. Both proposed methods exceedstate-of-the-art performance, surpassing the best benchmark by approximately 3%for all classes and around 6% for cracks specifically. Based on successfuldetection, we propose a method based on a monocular camera and the Hough LineTransform to automatically transform images into orthographic projection maps.By incorporating known real sizes of brick units, we accurately estimate crackdimensions, with the results differing by less than 10% from those obtained bylaser scanning. Overall, we address important research gaps in automatedmasonry crack detection and size estimation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15268",
        "title": "Towards Stable Preferences for Stakeholder-aligned Machine Learning",
        "authors": [
            "Haleema Sheraz",
            "Stefan C. Kremer",
            "Joshua August Skorburg",
            "Graham Taylor",
            "Walter Sinnott-Armstrong",
            "Kyle Boerstler"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In response to the pressing challenge of kidney allocation, characterized bygrowing demands for organs, this research sets out to develop a data-drivensolution to this problem, which also incorporates stakeholder values. Theprimary objective of this study is to create a method for learning bothindividual and group-level preferences pertaining to kidney allocations.Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveragingtwo distinct datasets and evaluating across three levels - Individual, Groupand Stability - we employ machine learning classifiers assessed through severalmetrics. The Individual level model predicts individual participantpreferences, the Group level model aggregates preferences across participants,and the Stability level model, an extension of the Group level, evaluates thestability of these preferences over time. By incorporating stakeholderpreferences into the kidney allocation process, we aspire to advance theethical dimensions of organ transplantation, contributing to more transparentand equitable practices while promoting the integration of moral values intoalgorithmic decision-making."
    },
    {
        "link": "https://arxiv.org/abs/2401.15269",
        "title": "Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models",
        "authors": [
            "Minbyul Jeong",
            "Jiwoong Sohn",
            "Mujeen Sung",
            "Jaewoo Kang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Recent proprietary large language models (LLMs), such as GPT-4, have achieveda milestone in tackling diverse challenges in the biomedical domain, rangingfrom multiple-choice questions to long-form generations. To address challengesthat still cannot be handled with the encoded knowledge of LLMs, variousretrieval-augmented generation (RAG) methods have been developed by searchingdocuments from the knowledge corpus and appending them unconditionally orselectively to the input of LLMs for generation. However, when applyingexisting methods to different domain-specific problems, poor generalizationbecomes apparent, leading to fetching incorrect documents or making inaccuratejudgments. In this paper, we introduce Self-BioRAG, a framework reliable forbiomedical text that specializes in generating explanations, retrievingdomain-specific documents, and self-reflecting generated responses. We utilize84k filtered biomedical instruction sets to train Self-BioRAG that can assessits generated explanations with customized reflective tokens. Our work provesthat domain-specific components, such as a retriever, domain-related documentcorpus, and instruction sets are necessary for adhering to domain-relatedinstructions. Using three major medical question-answering benchmark datasets,experimental results of Self-BioRAG demonstrate significant performance gainsby achieving a 7.2% absolute improvement on average over the state-of-the-artopen-foundation model with a parameter size of 7B or less. Overall, we analyzethat Self-BioRAG finds the clues in the question, retrieves relevant documentsif needed, and understands how to answer with information from retrieveddocuments and encoded knowledge as a medical expert does. We release our dataand code for training our framework components and model weights (7B and 13B)to enhance capabilities in biomedical and clinical domains."
    },
    {
        "link": "https://arxiv.org/abs/2401.15270",
        "title": "SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models",
        "authors": [
            "Zhihao Wang",
            "Yiqun Xie",
            "Zhili Li",
            "Xiaowei Jia",
            "Zhe Jiang",
            "Aolin Jia",
            "Shuo Xu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Fairness-awareness has emerged as an essential building block for theresponsible use of artificial intelligence in real applications. In many cases,inequity in performance is due to the change in distribution over differentregions. While techniques have been developed to improve the transferability offairness, a solution to the problem is not always feasible with no samples fromthe new regions, which is a bottleneck for pure data-driven attempts.Fortunately, physics-based mechanistic models have been studied for manyproblems with major social impacts. We propose SimFair, a physics-guidedfairness-aware learning framework, which bridges the data limitation byintegrating physical-rule-based simulation and inverse modeling into thetraining design. Using temperature prediction as an example, we demonstrate theeffectiveness of the proposed SimFair in fairness preservation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15273",
        "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
        "authors": [
            "Chenyu Zhang",
            "Han Wang",
            "Aritra Mitra",
            "James Anderson"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigmfor reducing the sample complexity of reinforcement learning tasks byexploiting information from different agents. However, when each agentinteracts with a potentially different environment, little to nothing is knowntheoretically about the non-asymptotic performance of FRL algorithms. The lackof such results can be attributed to various technical challenges and theirintricate interplay: Markovian sampling, linear function approximation,multiple local updates to save communication, heterogeneity in the rewardfunctions and transition kernels of the agents' MDPs, and continuousstate-action spaces. Moreover, in the on-policy setting, the behavior policiesvary with time, further complicating the analysis. In response, we introduceFedSARSA, a novel federated on-policy reinforcement learning scheme, equippedwith linear function approximation, to address these challenges and provide acomprehensive finite-time error analysis. Notably, we establish that FedSARSAconverges to a policy that is near-optimal for all agents, with the extent ofnear-optimality proportional to the level of heterogeneity. Furthermore, weprove that FedSARSA leverages agent collaboration to enable linear speedups asthe number of agents increases, which holds for both fixed and adaptivestep-size configurations."
    },
    {
        "link": "https://arxiv.org/abs/2401.15275",
        "title": "Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks",
        "authors": [
            "Yuliang Cai",
            "Mohammad Rostami"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformer neural networks are increasingly replacing prior architectures ina wide range of applications in different data modalities. The increasing sizeand computational demands of fine-tuning large pre-trained transformer neuralnetworks pose significant challenges for the widespread adoption of thesemodels for applications that demand on-edge computing. To tackle thischallenge, continual learning (CL) emerges as a solution by facilitating thetransfer of knowledge across tasks that arrive sequentially for an autonomouslylearning agent. However, current CL methods mainly focus on learning tasks thatare exclusively vision-based or language-based. We propose a transformer-basedCL framework focusing on learning tasks that involve both vision and language,known as Vision-and-Language (VaL) tasks. Due to the success of transformers inother modalities, our architecture has the potential to be used in multimodallearning settings. In our framework, we benefit from introducing extraparameters to a base transformer to specialize the network for each task. As aresult, we enable dynamic model expansion to learn several tasks in a sequence.We also use knowledge distillation to benefit from relevant past experiences tolearn the current task more efficiently. Our proposed method, Task AttentiveMultimodal Continual Learning (TAM-CL), allows for the exchange of informationbetween tasks while mitigating the problem of catastrophic forgetting. Notably,our approach is scalable, incurring minimal memory and time overhead. TAM-CLachieves state-of-the-art (SOTA) performance on challenging multimodal tasks"
    },
    {
        "link": "https://arxiv.org/abs/2401.15278",
        "title": "Online Data-Driven Adaptive Control for Unknown Linear Time-Varying Systems",
        "authors": [
            "Shenyu Liu",
            "Kaiwen Chen",
            "Jaap Eising"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "This paper proposes a novel online data-driven adaptive control for unknownlinear time-varying systems. Initialized with an empirical feedback gain, thealgorithm periodically updates this gain based on the data collected over ashort time window before each update. Meanwhile, the stability of theclosed-loop system is analyzed in detail, which shows that under some mildassumptions, the proposed online data-driven adaptive control scheme canguarantee practical global exponential stability. Finally, the proposedalgorithm is demonstrated by numerical simulations and its performance iscompared with other control algorithms for unknown linear time-varying systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15279",
        "title": "FabHacks: Transform Everyday Objects into Functional Fixtures",
        "authors": [
            "Yuxuan Mei",
            "Benjamin Jones",
            "Dan Cascaval",
            "Jennifer Mankoff",
            "Etienne Vouga",
            "Adriana Schulz"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "Storage, organizing, and decorating are an important part of home design.While one can buy commercial items for many of these tasks, this can be costly,and re-use is more sustainable. An alternative is a \"home hack\", a functionalassembly that can be constructed from existing household items. However, comingup with such hacks requires combining objects to make a physically validdesign, which might be difficult to test if they are large, require nailing orscrewing something to the wall, or the designer has mobility limitations. Inthis work, we present a design and visualization system for creating workablefunctional assemblies, FabHacks, which is based on a solver-aideddomain-specific language (S-DSL) FabHaL. By analyzing existing home hacksshared online, we create a design abstraction for connecting household itemsusing predefined types of connections. We provide a UI for FabHaL that can beused to design assemblies that fulfill a given specification. Our systemleverages a physics-based solver that takes an assembly design and finds itsexpected physical configuration. Our validation includes a user study showingthat users can create assemblies successfully using our UI and explore a rangeof designs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15280",
        "title": "Analytical Framework for Effective Degrees of Freedom in Near-Field XL-MIMO",
        "authors": [
            "Zhe Wang",
            "Jiayi Zhang",
            "Wenhui Yi",
            "Hongyang Du",
            "Dusit Niyato",
            "Bo Ai",
            "Derrick Wing Kwan Ng"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In this paper, we develop an effective degrees of freedom (EDoF) performanceanalysis framework specifically tailored for near-field XL-MIMO systems. Weexplore five representative distinct XL-MIMO hardware designs, includinguniform planar array (UPA)-based with point antennas, two-dimensional (2D)continuous aperture (CAP) plane-based, UPA-based with patch antennas, uniformlinear array (ULA)-based, and one-dimensional (1D) CAP line segment-basedXL-MIMO systems. Our analysis encompasses two near-field channel models: thescalar and dyadic Green's function-based channel models. More importantly, whenapplying the scalar Green's function-based channel, we derive EDoF expressionsin the closed-form, characterizing the impacts of the physical size of thetransceiver, the transmitting distance, and the carrier frequency. In ournumerical results, we evaluate and compare the EDoF performance across allexamined XL-MIMO designs, confirming the accuracy of our proposed closed-formexpressions. Furthermore, we observe that with an increasing number ofantennas, the EDoF performance for both UPA-based and ULA-based systemsapproaches that of 2D CAP plane and 1D CAP line segment-based systems,respectively. Moreover, we unveil that the EDoF performance for near-fieldXL-MIMO systems is predominantly determined by the array aperture size ratherthan the sheer number of antennas."
    },
    {
        "link": "https://arxiv.org/abs/2401.15282",
        "title": "GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis",
        "authors": [
            "Jing Hao",
            "Moyun Liu",
            "Kuo Feng Hung"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Detecting glass regions is a challenging task due to the ambiguity of theirtransparency and reflection properties. These transparent glasses share thevisual appearance of both transmitted arbitrary background scenes and reflectedobjects, thus having no fixed patterns.Recent visual foundation models, whichare trained on vast amounts of data, have manifested stunning performance interms of image perception and image generation. To segment glass surfaces withhigher accuracy, we make full use of two visual foundation models: SegmentAnything (SAM) and Stable Diffusion.Specifically, we devise a simple glasssurface segmentor named GEM, which only consists of a SAM backbone, a simplefeature pyramid, a discerning query selection module, and a mask decoder. Thediscerning query selection can adaptively identify glass surface features,assigning them as initialized queries in the mask decoder. We also propose aSynthetic but photorealistic large-scale Glass Surface Detection dataset dubbedS-GSD via diffusion model with four different scales, which contain 1x, 5x,10x, and 20x of the original real data size. This dataset is a feasible sourcefor transfer learning. The scale of synthetic data has positive impacts ontransfer learning, while the improvement will gradually saturate as the amountof data increases. Extensive experiments demonstrate that GEM achieves a newstate-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasetsare available at: https://github.com/isbrycee/GEM-Glass-Segmentor."
    },
    {
        "link": "https://arxiv.org/abs/2401.15284",
        "title": "Building ethical guidelines for generative AI in scientific research",
        "authors": [
            "Zhicheng Lin"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity."
    },
    {
        "link": "https://arxiv.org/abs/2401.15285",
        "title": "Ransomware threat mitigation through network traffic analysis and machine learning techniques",
        "authors": [
            "Ali Mehrban",
            "Shirin Karimi Geransayeh"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "In recent years, there has been a noticeable increase in cyberattacks usingransomware. Attackers use this malicious software to break into networks andharm computer systems. This has caused significant and lasting damage tovarious organizations, including government, private companies, and regularusers. These attacks often lead to the loss or exposure of sensitiveinformation, disruptions in normal operations, and persistent vulnerabilities.This paper focuses on a method for recognizing and identifying ransomware incomputer networks. The approach relies on using machine learning algorithms andanalyzing the patterns of network traffic. By collecting and studying thistraffic, and then applying machine learning models, we can accurately identifyand detect ransomware. The results of implementing this method show thatmachine learning algorithms can effectively pinpoint ransomware based onnetwork traffic, achieving high levels of precision and accuracy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15287",
        "title": "Applications of Tao General Difference in Discrete Domain",
        "authors": [
            "Linmi Tao",
            "Ruiyang Liu",
            "Donglai Tao",
            "Wu Xia",
            "Feilong Ma",
            "Yu Cheng",
            "Jingmao Cui"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Numerical difference computation is one of the cores and indispensable in themodern digital era. Tao general difference (TGD) is a novel theory and approachto difference computation for discrete sequences and arrays in multidimensionalspace. Built on the solid theoretical foundation of the general difference in afinite interval, the TGD operators demonstrate exceptional signal processingcapabilities in real-world applications. A novel smoothness property of asequence is defined on the first- and second TGD. This property is used todenoise one-dimensional signals, where the noise is the non-smooth points inthe sequence. Meanwhile, the center of the gradient in a finite interval can beaccurately location via TGD calculation. This solves a traditional challenge incomputer vision, which is the precise localization of image edges with noiserobustness. Furthermore, the power of TGD operators extends to spatio-temporaledge detection in three-dimensional arrays, enabling the identification ofkinetic edges in video data. These diverse applications highlight theproperties of TGD in discrete domain and the significant promise of TGD for thecomputation across signal processing, image analysis, and video analytic."
    },
    {
        "link": "https://arxiv.org/abs/2401.15288",
        "title": "STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics",
        "authors": [
            "Volodymyr Vakhniuk",
            "Ayush Sarkar",
            "Ragini Gupta"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose an efficient cross-cameras surveillance system called,STAC, thatleverages spatio-temporal associations between multiple cameras to providereal-time analytics and inference under constrained network environments. STACis built using the proposed omni-scale feature learning people reidentification(reid) algorithm that allows accurate detection, tracking and re-identificationof people across cameras using the spatio-temporal characteristics of videoframes. We integrate STAC with frame filtering and state-of-the-art compressionfor streaming technique (that is, ffmpeg libx264 codec) to remove redundantinformation from cross-camera frames. This helps in optimizing the cost ofvideo transmission as well as compute/processing, while maintaining highaccuracy for real-time query inference. The introduction of AICity Challenge2023 Data [1] by NVIDIA has allowed exploration of systems utilizingmulti-camera people tracking algorithms. We evaluate the performance of STACusing this dataset to measure the accuracy metrics and inference rate for reid.Additionally, we quantify the reduction in video streams achieved through framefiltering and compression using FFmpeg compared to the raw camera streams. Forcompleteness, we make available our repository to reproduce the results,available at https://github.com/VolodymyrVakhniuk/CS444_Final_Project."
    },
    {
        "link": "https://arxiv.org/abs/2401.15289",
        "title": "Where's the \"up\"?! A Comprehensive (bottom-up) Study on the Security of Arm Cortex-M Systems",
        "authors": [
            "Xi Tan",
            "Zheyuan Ma",
            "Sandro Pinto",
            "Le Guan",
            "Ning Zhang",
            "Jun Xu",
            "Zhiqiang Lin",
            "Hongxing Hu",
            "Ziming Zhao"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Arm Cortex-M processors are the most widely used 32-bit microcontrollersamong embedded and Internetof-Things devices. Despite the widespread usage,there has been little effort in summarizing their hardware security features,characterizing the limitations and vulnerabilities of their hardware andsoftware stack, and systematizing the research on securing these systems. Thegoals and contributions of this paper are multi-fold. First, we analyze thehardware security limitations and issues of Cortex-M systems. Second, weconducted a deep study of the software stack designed for Cortex-M and revealedits limitations, which is accompanied by an empirical analysis of 1,797real-world firmware from seven hardware vendors. Third, we categorize thereported bugs in Cortex-M software systems. Finally, we systematize the effortsthat aim at securing Cortex-M systems and evaluate them in terms of theprotections they offer, run-time performance, required hardware features, etc.Based on the insights, we develop a set of recommendations for the researchcommunity and MCU software developers."
    },
    {
        "link": "https://arxiv.org/abs/2401.15290",
        "title": "Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset",
        "authors": [
            "Hung Bui",
            "Harikrishna Warrier",
            "Yogesh Gupta"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Electronic health record (EHR) is more and more popular, and it comes withapplying machine learning solutions to resolve various problems in the domain.This growing research area also raises the need for EHRs accessibility. MedicalInformation Mart for Intensive Care (MIMIC) dataset is a popular, public, andfree EHR dataset in a raw format that has been used in numerous studies.However, despite of its popularity, it is lacking benchmarking work, especiallywith recent state of the art works in the field of deep learning withtime-series tabular data. The aim of this work is to fill this lack byproviding a benchmark for latest version of MIMIC dataset, MIMIC-IV. We alsogive a detailed literature survey about studies that has been already done forMIIMIC-III."
    },
    {
        "link": "https://arxiv.org/abs/2401.15291",
        "title": "Improved Construction of Robust Gray Code",
        "authors": [
            "Dorsa Fathollahi",
            "Mary Wootters"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "A robust Gray code, formally introduced by (Lolck and Pagh, SODA 2024), is aGray code that additionally has the property that, given a noisy version of theencoding of an integer j, it is possible to reconstruct j^ so that |j\u2212j^| is small with high probability. That work presented atransformation that transforms a binary code C of rate R to a robust Graycode with rate \u03a9(R), where the constant in the \u03a9(\u22c5) can be atmost 1/4. We improve upon their construction by presenting a transformationfrom a (linear) binary code C to a robust Gray code with similar robustnessguarantees, but with rate that can approach R/2."
    },
    {
        "link": "https://arxiv.org/abs/2401.15292",
        "title": "Adaptive Block sparse regularization under arbitrary linear transform",
        "authors": [
            "Takanobu Furuhashi",
            "Hidekata Hontani",
            "Tatsuya Yokota"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We propose a convex signal reconstruction method for block sparsity underarbitrary linear transform with unknown block structure. The proposed method isa generalization of the existing method LOP-\u21132/\u21131 and canreconstruct signals with block sparsity under non-invertible transforms, unlikeLOP-\u21132/\u21131. Our work broadens the scope of block sparseregularization, enabling more versatile and powerful applications acrossvarious signal processing domains. We derive an iterative algorithm for solvingproposed method and provide conditions for its convergence to the optimalsolution. Numerical experiments demonstrate the effectiveness of the proposedmethod."
    },
    {
        "link": "https://arxiv.org/abs/2401.15293",
        "title": "SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection",
        "authors": [
            "Foozhan Ataiefard",
            "Walid Ahmed",
            "Habib Hajimolahoseini",
            "Saina Asani",
            "Farnoosh Javadi",
            "Mohammad Hassanpour",
            "Omar Mohamed Awad",
            "Austin Wen",
            "Kangling Liu",
            "Yang Liu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision transformers are known to be more computationally and data-intensivethan CNN models. These transformer models such as ViT, require all the inputimage tokens to learn the relationship among them. However, many of thesetokens are not informative and may contain irrelevant information such asunrelated background or unimportant scenery. These tokens are overlooked by themulti-head self-attention (MHSA), resulting in many redundant and unnecessarycomputations in MHSA and the feed-forward network (FFN). In this work, wepropose a method to optimize the amount of unnecessary interactions betweenunimportant tokens by separating and sending them through a different low-costcomputational path. Our method does not add any parameters to the ViT model andaims to find the best trade-off between training throughput and achieving a 0%loss in the Top-1 accuracy of the final model. Our experimental results ontraining ViT-small from scratch show that SkipViT is capable of effectivelydropping 55% of the tokens while gaining more than 13% training throughput andmaintaining classification accuracy at the level of the baseline model onHuawei Ascend910A."
    },
    {
        "link": "https://arxiv.org/abs/2401.15294",
        "title": "Integral Operator Approaches for Scattered Data Fitting on Spheres",
        "authors": [
            "Shao-Bo Lin"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This paper focuses on scattered data fitting problems on spheres. We studythe approximation performance of a class of weighted spectral filteralgorithms, including Tikhonov regularization, Landaweber iteration, spectralcut-off, and iterated Tikhonov, in fitting noisy data with possibly unboundedrandom noise. For the analysis, we develop an integral operator approach thatcan be regarded as an extension of the widely used sampling inequality approachand norming set method in the community of scattered data fitting. Afterproviding an equivalence between the operator differences and quadrature rules,we succeed in deriving optimal Sobolev-type error estimates of weightedspectral filter algorithms. Our derived error estimates do not suffer from thesaturation phenomenon for Tikhonov regularization in the literature,native-space-barrier for existing error analysis and adapts to differentembedding spaces. We also propose a divide-and-conquer scheme to equip weightedspectral filter algorithms to reduce their computational burden and present theoptimal approximation error bounds."
    },
    {
        "link": "https://arxiv.org/abs/2401.15295",
        "title": "Multi-Trigger Backdoor Attacks: More Triggers, More Threats",
        "authors": [
            "Yige Li",
            "Xingjun Ma",
            "Jiabo He",
            "Hanxun Huang",
            "Yu-Gang Jiang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Backdoor attacks have emerged as a primary threat to (pre-)training anddeployment of deep neural networks (DNNs). While backdoor attacks have beenextensively studied in a body of works, most of them were focused onsingle-trigger attacks that poison a dataset using a single type of trigger.Arguably, real-world backdoor attacks can be much more complex, e.g., theexistence of multiple adversaries for the same dataset if it is of high value.In this work, we investigate the practical threat of backdoor attacks under thesetting of \\textbf{multi-trigger attacks} where multiple adversaries leveragedifferent types of triggers to poison the same dataset. By proposing andinvestigating three types of multi-trigger attacks, including parallel,sequential, and hybrid attacks, we provide a set of important understandings ofthe coexisting, overwriting, and cross-activating effects between differenttriggers on the same dataset. Moreover, we show that single-trigger attackstend to cause overly optimistic views of the security of current defensetechniques, as all examined defense methods struggle to defend againstmulti-trigger attacks. Finally, we create a multi-trigger backdoor poisoningdataset to help future evaluation of backdoor attacks and defenses. Althoughour work is purely empirical, we hope it can help steer backdoor researchtoward more realistic settings."
    },
    {
        "link": "https://arxiv.org/abs/2401.15296",
        "title": "A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions",
        "authors": [
            "Haocong Rao",
            "Chunyan Miao"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Person re-identification via 3D skeletons is an important emerging researcharea that triggers great interest in the pattern recognition community. Withdistinctive advantages for many application scenarios, a great diversity of 3Dskeleton based person re-identification (SRID) methods have been proposed inrecent years, effectively addressing prominent problems in skeleton modelingand feature learning. Despite recent advances, to the best of our knowledge,little effort has been made to comprehensively summarize these studies andtheir challenges. In this paper, we attempt to fill this gap by providing asystematic survey on current SRID approaches, model designs, challenges, andfuture directions. Specifically, we first formulate the SRID problem, andpropose a taxonomy of SRID research with a summary of benchmark datasets,commonly-used model architectures, and an analytical review of differentmethods' characteristics. Then, we elaborate on the design principles of SRIDmodels from multiple aspects to offer key insights for model improvement.Finally, we identify critical challenges confronting current studies anddiscuss several promising directions for future research of SRID."
    },
    {
        "link": "https://arxiv.org/abs/2401.15298",
        "title": "Together We Go Further: LLMs and IDE Static Analysis for Extract Method Refactoring",
        "authors": [
            "Dorin Pomian",
            "Abhiram Bellur",
            "Malinda Dilhara",
            "Zarina Kurbatova",
            "Egor Bogomolov",
            "Timofey Bryksin",
            "Danny Dig"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Excessively long methods that encapsulate multiple responsibilities within asingle method are challenging to comprehend, debug, reuse, and maintain. Thesolution to this problem, a hallmark refactoring called Extract Method,consists of two phases: (i) choosing the statements to extract and (ii)applying the mechanics to perform this refactoring. While the application parthas been a staple feature of all modern IDEs, they leave it up to developers tochoose the statements to extract. Choosing which statements are profitable toextract has been the subject of many research tools that employ hard-codedrules to optimize software quality metrics. Despite steady improvements, thesetools often fail to generate refactorings that align with developers'preferences and acceptance criteria. In this paper, we introduce EM-Assist, atool that augments the refactoring capabilities of IDEs with the power of LLMsto perform Extract Method refactoring. We empirically evaluated EM-Assist on adiverse, publicly available corpus that other researchers used in the past. Theresults show that EM-Assist outperforms previous state-of-the-art tools: at 1%tolerance, EM-Assist suggests the correct refactoring among its top-5suggestions 60.6% of the time, compared to 54.2% reported by existing MLmodels, and 52.2% reported by existing static analysis tools. When wereplicated 2,849 actual Extract Method instances from open-source projects,EM-Assist's recall rate was 42.1% compared to 6.5% for its peers. Furthermore,we conducted warehouse surveys with 20 industrial developers and suggestedrefactorings on their recent commits. 81.3% of the respondents agreed with therecommendations provided by EM-Assist. This shows the usefulness of ourapproach and ushers us into a new era of refactoring when LLMs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15299",
        "title": "SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks",
        "authors": [
            "Azmine Toushik Wasi",
            "MD Shafikul Islam",
            "Adipto Raihan Akib"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) have gained traction across different domainssuch as transportation, bio-informatics, language processing, and computervision. However, there is a noticeable absence of research on applying GNNs tosupply chain networks. Supply chain networks are inherently graph-like instructure, making them prime candidates for applying GNN methodologies. Thisopens up a world of possibilities for optimizing, predicting, and solving eventhe most complex supply chain problems. A major setback in this approach liesin the absence of real-world benchmark datasets to facilitate the research andresolution of supply chain problems using GNNs. To address the issue, wepresent a real-world benchmark dataset for temporal tasks, obtained from one ofthe leading FMCG companies in Bangladesh, focusing on supply chain planning forproduction purposes. The dataset includes temporal data as node features toenable sales predictions, production planning, and the identification offactory issues. By utilizing this dataset, researchers can employ GNNs toaddress numerous supply chain problems, thereby advancing the field of supplychain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph"
    },
    {
        "link": "https://arxiv.org/abs/2401.15304",
        "title": "Adaptive Least Mean Squares Graph Neural Networks and Online Graph Signal Estimation",
        "authors": [
            "Yi Yan",
            "Changran Peng",
            "Ercan Engin Kuruoglu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The online prediction of multivariate signals, existing simultaneously inspace and time, from noisy partial observations is a fundamental task innumerous applications. We propose an efficient Neural Network architecture forthe online estimation of time-varying graph signals named the Adaptive LeastMean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the timevariation and bridge the cross-space-time interactions under the condition thatsignals are corrupted by noise and missing values. The LMS-GNN is a combinationof adaptive graph filters and Graph Neural Networks (GNN). At each time step,the forward propagation of LMS-GNN is similar to adaptive graph filters wherethe output is based on the error between the observation and the predictionsimilar to GNN. The filter coefficients are updated via backpropagation as inGNN. Experimenting on real-world temperature data reveals that our LMS-GNNachieves more accurate online predictions compared to graph-based methods likeadaptive graph filters and graph convolutional neural networks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15308",
        "title": "Construction of Locally Repairable Array Codes with Optimal Repair Bandwidth under the Rack-Aware Storage Model",
        "authors": [
            "Yumeng Yang",
            "Han Cai",
            "Xiaohu Tang"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In this paper, we discuss codes for distributed storage systems withhierarchical repair properties. Specifically, we devote attention to the repairproblem of the rack-aware storage model with locality, aiming to enhance thesystem's ability to repair a small number of erasures within each rack bylocality and efficiently handling a rack erasure with a small repair bandwidth.By employing the regenerating coding technique, we construct a family of arraycodes with (r,u\u2212r+1)-locality, where the u nodes of each repair set aresystematically organized into a rack. When the number of failures is less thanu\u2212r+1, these failures can be repaired without counting the systembandwidth. In cases where the number of failures exceeds the locality, thefailed nodes within a single rack can be recovered with optimal cross-rackbandwidth."
    },
    {
        "link": "https://arxiv.org/abs/2401.15312",
        "title": "How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation",
        "authors": [
            "Wei-Yu Kao",
            "An-Zi Yen"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Automated fact-checking is a crucial task in the governance of internetcontent. Although various studies utilize advanced models to tackle this issue,a significant gap persists in addressing complex real-world rumors anddeceptive claims. To address this challenge, this paper explores the novel taskof flaw-oriented fact-checking, including aspect generation and flawidentification. We also introduce RefuteClaim, a new framework designedspecifically for this task. Given the absence of an existing dataset, wepresent FlawCheck, a dataset created by extracting and transforming insightsfrom expert reviews into relevant aspects and identified flaws. Theexperimental results underscore the efficacy of RefuteClaim, particularly inclassifying and elucidating false claims."
    },
    {
        "link": "https://arxiv.org/abs/2401.15313",
        "title": "Multi-Robot Relative Pose Estimation in SE(2) with Observability Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph Optimization",
        "authors": [
            "Kihoon Shin",
            "Hyunjae Sim",
            "Seungwon Nam",
            "Yonghee Kim",
            "Jae Hu",
            "Kwang-Ki K. Kim"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "In this paper, we consider multi-robot localization problems with focus oncooperative localization and observability analysis of relative poseestimation. For cooperative localization, there is extra information availableto each robot via communication network and message passing. If odometry dataof a target robot can be transmitted to the ego-robot then the observability oftheir relative pose estimation can be achieved by range-only or bearing-onlymeasurements provided both of their linear velocities are non-zero. If odometrydata of a target robot is not directly transmitted but estimated by theego-robot then there must be both range and bearing measurements to guaranteethe observability of relative pose estimation. For ROS/Gazebo simulations, weconsider four different sensing and communication structures in which extendedKalman filtering (EKF) and pose graph optimization (PGO) estimation withdifferent robust loss functions (filtering and smoothing with different batchsizes of sliding window) are compared in terms of estimation accuracy. Forhardware experiments, two Turtlebot3 equipped with UWB modules are used forreal-world inter-robot relative pose estimation, in which both EKF and PGO areapplied and compared."
    },
    {
        "link": "https://arxiv.org/abs/2401.15315",
        "title": "Learning Online Belief Prediction for Efficient POMDP Planning in Autonomous Driving",
        "authors": [
            "Zhiyu Huang",
            "Chen Tang",
            "Chen Lv",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Effective decision-making in autonomous driving relies on accurate inferenceof other traffic agents' future behaviors. To achieve this, we propose anonline learning-based behavior prediction model and an efficient planner forPartially Observable Markov Decision Processes (POMDPs). We develop alearning-based prediction model, enhanced with a recurrent neural memorynetwork, to dynamically update latent belief states and infer the intentions ofother agents. The model can also integrate the ego vehicle's intentions toreflect closed-loop interactions among agents, and it learns from both offlinedata and online interactions. For planning, we employ an option-basedMonte-Carlo Tree Search (MCTS) planner, which reduces computational complexityby searching over action sequences. Inside the MCTS planner, we use predictedlong-term multi-modal trajectories to approximate future updates, whicheliminates iterative belief updating and improves the running efficiency. Ourapproach also incorporates deep Q-learning (DQN) as a search prior, whichsignificantly improves the performance of the MCTS planner. Experimentalresults from simulated environments validate the effectiveness of our proposedmethod. The online belief update model can significantly enhance the accuracyand temporal consistency of predictions, leading to improved decision-makingperformance. Employing DQN as a search prior in the MCTS planner considerablyboosts its performance and outperforms an imitation learning-based prior.Additionally, we show that the option-based MCTS substantially outperforms thevanilla method in terms of performance and efficiency."
    },
    {
        "link": "https://arxiv.org/abs/2401.15316",
        "title": "UNSEE: Unsupervised Non-contrastive Sentence Embeddings",
        "authors": [
            "\u00d6mer Veysel \u00c7a\u011fatan"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novelapproach that outperforms SimCSE in the Massive Text Embedding benchmark. Ourexploration begins by addressing the challenge of representation collapse, aphenomenon observed when contrastive objectives in SimCSE are replaced withnon-contrastive objectives. To counter this issue, we propose a straightforwardsolution known as the target network, effectively mitigating representationcollapse. The introduction of the target network allows us to leveragenon-contrastive objectives, maintaining training stability while achievingperformance improvements comparable to contrastive objectives. Our method hasachieved peak performance in non-contrastive sentence embeddings throughmeticulous fine-tuning and optimization. This comprehensive effort has yieldedsuperior sentence representation models, showcasing the effectiveness of ourapproach."
    },
    {
        "link": "https://arxiv.org/abs/2401.15317",
        "title": "Floorplanning of VLSI by Mixed-Variable Optimization",
        "authors": [
            "Jian Sun",
            "Huabin Cheng",
            "Jian Wu",
            "Zhanyang Zhu",
            "Yu Chen"
        ],
        "primary_subject": "Neural and Evolutionary Computing (cs.NE)",
        "abstract": "By formulating the floorplanning of VLSI as a mixed-variable optimizationproblem, this paper proposes to solve it by memetic algorithms, where thediscrete orientation variables are addressed by the distribution evolutionaryalgorithm based on a population of probability model (DEA-PPM), and thecontinuous coordination variables are optimized by the conjugate sub-gradientalgorithm (CSA). Accordingly, the fixed-outline floorplanning algorithm basedon CSA and DEA-PPM (FFA-CD) and the floorplanning algorithm with golden sectionstrategy (FA-GSS) are proposed for the floorplanning problems with and withoutfixed-outline constraint. %FF-CD is committed to optimizing wirelength targetswithin a fixed profile. FA-GSS uses the Golden Section strategy to optimizeboth wirelength and area targets. The CSA is used to solve the proposednon-smooth optimization model, and the DEA-PPM is used to explore the modulerotation scheme to enhance the flexibility of the algorithm. Numericalexperiments on GSRC test circuits show that the proposed algorithms aresuperior to some celebrated B*-tree based floorplanning algorithms, and areexpected to be applied to large-scale floorplanning problems due to their lowtime complexity."
    },
    {
        "link": "https://arxiv.org/abs/2401.15318",
        "title": "Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting",
        "authors": [
            "Yutao Feng",
            "Xiang Feng",
            "Yintong Shang",
            "Ying Jiang",
            "Chang Yu",
            "Zeshun Zong",
            "Tianjia Shao",
            "Hongzhi Wu",
            "Kun Zhou",
            "Chenfanfu Jiang",
            "Yin Yang"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "We demonstrate the feasibility of integrating physics-based animations ofsolids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects invirtual scenes reconstructed using 3DGS. Leveraging the coherence of theGaussian splatting and position-based dynamics (PBD) in the underlyingrepresentation, we manage rendering, view synthesis, and the dynamics of solidsand fluids in a cohesive manner. Similar to Gaussian shader, we enhance eachGaussian kernel with an added normal, aligning the kernel's orientation withthe surface normal to refine the PBD simulation. This approach effectivelyeliminates spiky noises that arise from rotational deformation in solids. Italso allows us to integrate physically based rendering to augment the dynamicsurface reflections on fluids. Consequently, our framework is capable ofrealistically reproducing surface highlights on dynamic fluids and facilitatinginteractions between scene objects and fluids from new views. For moreinformation, please visit our project page at\\url{https://amysteriouscat.github.io/GaussianSplashing/}."
    },
    {
        "link": "https://arxiv.org/abs/2401.15319",
        "title": "You Only Look Bottom-Up for Monocular 3D Object Detection",
        "authors": [
            "Kaixin Xiong",
            "Dingyuan Zhang",
            "Dingkang Liang",
            "Zhe Liu",
            "Hongcheng Yang",
            "Wondimu Dikubab",
            "Jianwei Cheng",
            "Xiang Bai"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Monocular 3D Object Detection is an essential task for autonomous driving.Meanwhile, accurate 3D object detection from pure images is very challengingdue to the loss of depth information. Most existing image-based methods inferobjects' location in 3D space based on their 2D sizes on the image plane, whichusually ignores the intrinsic position clues from images, leading tounsatisfactory performances. Motivated by the fact that humans could leveragethe bottom-up positional clues to locate objects in 3D space from a singleimage, in this paper, we explore the position modeling from the image featurecolumn and propose a new method named You Only Look Bottum-Up (YOLOBU).Specifically, our YOLOBU leverages Column-based Cross Attention to determinehow much a pixel contributes to pixels above it. Next, the Row-based ReverseCumulative Sum (RRCS) is introduced to build the connections of pixels in thebottom-up direction. Our YOLOBU fully explores the position clues for monocular3D detection via building the relationship of pixels from the bottom-up way.Extensive experiments on the KITTI dataset demonstrate the effectiveness andsuperiority of our method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15321",
        "title": "Localization of Dummy Data Injection Attacks in Power Systems Considering Incomplete Topological Information: A Spatio-Temporal Graph Wavelet Convolutional Neural Network Approach",
        "authors": [
            "Zhaoyang Qu",
            "Yunchang Dong",
            "Yang Li",
            "Siqi Song",
            "Tao Jiang",
            "Min Li",
            "Qiming Wang",
            "Lei Wang",
            "Xiaoyong Bo",
            "Jiye Zang",
            "Qi Xu"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "The emergence of novel the dummy data injection attack (DDIA) poses a severethreat to the secure and stable operation of power systems. These attacks areparticularly perilous due to the minimal Euclidean spatial separation betweenthe injected malicious data and legitimate data, rendering their precisedetection challenging using conventional distance-based methods. Furthermore,existing research predominantly focuses on various machine learning techniques,often analyzing the temporal data sequences post-attack or relying solely onEuclidean spatial characteristics. Unfortunately, this approach tends tooverlook the inherent topological correlations within the non-Euclidean spatialattributes of power grid data, consequently leading to diminished accuracy inattack localization. To address this issue, this study takes a comprehensiveapproach. Initially, it examines the underlying principles of these new DDIAson power systems. Here, an intricate mathematical model of the DDIA isdesigned, accounting for incomplete topological knowledge and alternatingcurrent (AC) state estimation from an attacker's perspective. Subsequently, byintegrating a priori knowledge of grid topology and considering the temporalcorrelations within measurement data and the topology-dependent attributes ofthe power grid, this study introduces temporal and spatial attention matrices.These matrices adaptively capture the spatio-temporal correlations within theattacks. Leveraging gated stacked causal convolution and graph wavelet sparseconvolution, the study jointly extracts spatio-temporal DDIA features. Finally,the research proposes a DDIA localization method based on spatio-temporal graphneural networks. The accuracy and effectiveness of the DDIA model arerigorously demonstrated through comprehensive analytical cases."
    },
    {
        "link": "https://arxiv.org/abs/2401.15323",
        "title": "Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training",
        "authors": [
            "Haesun Joung",
            "Kyogu Lee"
        ],
        "primary_subject": "Sound (cs.SD)",
        "abstract": "Music auto-tagging is crucial for enhancing music discovery andrecommendation. Existing models in Music Information Retrieval (MIR) strugglewith real-world noise such as environmental and speech sounds in multimediacontent. This study proposes a method inspired by speech-related tasks toenhance music auto-tagging performance in noisy settings. The approachintegrates Domain Adversarial Training (DAT) into the music domain, enablingrobust music representations that withstand noise. Unlike previous research,this approach involves an additional pretraining phase for the domainclassifier, to avoid performance degradation in the subsequent phase. Addingvarious synthesized noisy music data improves the model's generalization acrossdifferent noise levels. The proposed architecture demonstrates enhancedperformance in music auto-tagging by effectively utilizing unlabeled noisymusic data. Additional experiments with supplementary unlabeled data furtherimproves the model's performance, underscoring its robust generalizationcapabilities and broad applicability."
    },
    {
        "link": "https://arxiv.org/abs/2401.15328",
        "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
        "authors": [
            "Adrian Theuma",
            "Ehsan Shareghi"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) have exhibited an array of reasoningcapabilities but face challenges like error propagation and hallucination,particularly in specialised areas like finance, where data is heterogeneous,and precision is paramount. We explore the potential of language modelaugmentation with external tools to mitigate these limitations and offloadcertain reasoning steps to external tools that are more suited for the task,instead of solely depending on the LLM's inherent abilities. More concretely,using financial domain question-answering datasets, we apply supervisedfine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and'task solver'. The 'task router' dynamically directs a question to either beanswered internally by the LLM or externally via the right tool from the toolset. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%and 5.06% over the base model and SFT-only baselines, respectively, and ishighly competitive with strong GPT-3.5 results. To the best of our knowledge,our work is the first that investigates tool augmentation of language modelsfor the finance domain."
    },
    {
        "link": "https://arxiv.org/abs/2401.15330",
        "title": "Optimal Sparse Survival Trees",
        "authors": [
            "Rui Zhang",
            "Rui Xin",
            "Margo Seltzer",
            "Cynthia Rudin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Interpretability is crucial for doctors, hospitals, pharmaceutical companiesand biotechnology corporations to analyze and make decisions for high stakesproblems that involve human health. Tree-based methods have been widely adoptedfor \\textit{survival analysis} due to their appealing interpretablility andtheir ability to capture complex relationships. However, most existing methodsto produce survival trees rely on heuristic (or greedy) algorithms, which riskproducing sub-optimal models. We present a dynamic-programming-with-boundsapproach that finds provably-optimal sparse survival tree models, frequently inonly a few seconds."
    },
    {
        "link": "https://arxiv.org/abs/2401.15332",
        "title": "Efficient yet Accurate End-to-End SC Accelerator Design",
        "authors": [
            "Meng Li",
            "Yixuan Hu",
            "Tengyu Zhang",
            "Renjie Wei",
            "Yawen Zhang",
            "Ru Huang",
            "Runsheng Wang"
        ],
        "primary_subject": "Hardware Architecture (cs.AR)",
        "abstract": "Providing end-to-end stochastic computing (SC) neural network accelerationfor state-of-the-art (SOTA) models has become an increasingly challenging task,requiring the pursuit of accuracy while maintaining efficiency. It alsonecessitates flexible support for different types and sizes of operations inmodels by end-to-end SC circuits. In this paper, we summarize our recentresearch on end-to-end SC neural network acceleration. We introduce an accurateend-to-end SC accelerator based on a deterministic coding and sorting network.In addition, we propose an SC-friendly model that combines low-precision datapaths with high-precision residuals. We introduce approximate computingtechniques to optimize SC nonlinear adders and provide some new SC designs forarithmetic operations required by SOTA models. Overall, our approach allows forfurther significant improvements in circuit efficiency, flexibility, andcompatibility through circuit design and model co-optimization. The resultsdemonstrate that the proposed end-to-end SC architecture achieves accurate andefficient neural network acceleration while flexibly accommodating modelrequirements, showcasing the potential of SC in neural network acceleration."
    },
    {
        "link": "https://arxiv.org/abs/2401.15335",
        "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks",
        "authors": [
            "Ping Guo",
            "Fei Liu",
            "Xi Lin",
            "Qingchuan Zhao",
            "Qingfu Zhang"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "In the rapidly evolving field of machine learning, adversarial attackspresent a significant challenge to model robustness and security.Decision-based attacks, which only require feedback on the decision of a modelrather than detailed probabilities or scores, are particularly insidious anddifficult to defend against. This work introduces L-AutoDA (Large LanguageModel-based Automated Decision-based Adversarial Attacks), a novel approachleveraging the generative capabilities of Large Language Models (LLMs) toautomate the design of these attacks. By iteratively interacting with LLMs inan evolutionary framework, L-AutoDA automatically designs competitive attackalgorithms efficiently without much human effort. We demonstrate the efficacyof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baselinemethods in both success rate and computational efficiency. Our findingsunderscore the potential of language models as tools for adversarial attackgeneration and highlight new avenues for the development of robust AI systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15337",
        "title": "Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data",
        "authors": [
            "Zenghui Lin",
            "Xintong Liu",
            "Nan Wang",
            "Ruichen Li",
            "Qingao Liu",
            "Jingying Ma",
            "Liwei Wang",
            "Yan Wang",
            "Shenda Hong"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Long-term fetal heart rate (FHR) monitoring during the antepartum period,increasingly popularized by electronic FHR monitoring, represents a growingapproach in FHR monitoring. This kind of continuous monitoring, in contrast tothe short-term one, collects an extended period of fetal heart data. Thisoffers a more comprehensive understanding of fetus's conditions. However, theinterpretation of long-term antenatal fetal heart monitoring is still in itsearly stages, lacking corresponding clinical standards. Furthermore, thesubstantial amount of data generated by continuous monitoring imposes asignificant burden on clinical work when analyzed manually. To address abovechallenges, this study develops an automatic analysis system named LARA(Long-term Antepartum Risk Analysis system) for continuous FHR monitoring,combining deep learning and information fusion methods. LARA's core is awell-established convolutional neural network (CNN) model. It processeslong-term FHR data as input and generates a Risk Distribution Map (RDM) andRisk Index (RI) as the analysis results. We evaluate LARA on inner testdataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816,specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. Inour study, we observe that long-term FHR monitoring data with higher RI is morelikely to result in adverse outcomes (p=0.0021). In conclusion, this studyintroduces LARA, the first automated analysis system for long-term FHRmonitoring, initiating the further explorations into its clinical value in thefuture."
    },
    {
        "link": "https://arxiv.org/abs/2401.15343",
        "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware Low latency Encoding",
        "authors": [
            "Vignesh V Menon",
            "Jingwen Zhu",
            "Prajit T Rajendran",
            "Samira Afzal",
            "Klaus Schoeffmann",
            "Patrick Le Callet",
            "Christian Timmerer"
        ],
        "primary_subject": "Multimedia (cs.MM)",
        "abstract": "In HTTP adaptive live streaming applications, video segments are encoded at afixed set of bitrate-resolution pairs known as bitrate ladder. Live encodersuse the fastest available encoding configuration, referred to as preset, toensure the minimum possible latency in video encoding. However, an optimizedpreset and optimized number of CPU threads for each encoding instance mayresult in (i) increased quality and (ii) efficient CPU utilization whileencoding. For low latency live encoders, the encoding speed is expected to bemore than or equal to the video framerate. To this light, this paper introducesa Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE),which uses random forest-based models to jointly determine the optimizedencoder preset and thread count for each representation, based on videocomplexity features, the target encoding speed, the total number of availableCPU threads, and the target encoder. Experimental results show that, onaverage, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF pointswith the same bitrate, compared to the fastest preset encoding of the HTTP LiveStreaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eightCPU threads used for each representation. These enhancements are achieved whilemaintaining the desired encoding speed. Furthermore, on average, JALE resultsin an overall storage reduction of 72.70 %, a reduction in the total number ofCPU threads used by 63.83 %, and a 37.87 % reduction in the overall encodingtime, considering a JND of six VMAF points."
    },
    {
        "link": "https://arxiv.org/abs/2401.15344",
        "title": "IRS Aided Millimeter-Wave Sensing and Communication: Beam Scanning, Beam Splitting, and Performance Analysis",
        "authors": [
            "Renwang Li",
            "Xiaodan Shao",
            "Shu Sun",
            "Meixia Tao",
            "Rui Zhang"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Integrated sensing and communication (ISAC) has attracted growing interestsfor enabling the future 6G wireless networks, due to its capability of sharingspectrum and hardware resources between communication and sensing systems.However, existing works on ISAC usually need to modify the communicationprotocol to cater for the new sensing performance requirement, which may bedifficult to implement in practice. In this paper, we study a new intelligentreflecting surface (IRS) aided millimeter-wave (mmWave) ISAC system byexploiting the distinct beam scanning operation in mmWave communications toachieve efficient sensing at the same time. First, we propose a two-phase ISACprotocol aided by a semi-passive IRS, consisting of beam scanning and datatransmission. Specifically, in the beam scanning phase, the IRS finds theoptimal beam for reflecting signals from the base station to a communicationuser via its passive elements. Meanwhile, the IRS directly estimates the angleof a nearby target based on echo signals from the target using its equippedactive sensing element. Then, in the data transmission phase, the sensingaccuracy is further improved by leveraging the data signals via possible IRSbeam splitting. Next, we derive the achievable rate of the communication useras well as the Cram\\'er-Rao bound and the approximate mean square error of thetarget angle estimation Finally, extensive simulation results are provided toverify our analysis as well as the effectiveness of the proposed scheme."
    },
    {
        "link": "https://arxiv.org/abs/2401.15346",
        "title": "Energy-efficient Adaptive Video Streaming with Latency-Aware Dynamic Resolution Encoding",
        "authors": [
            "Vignesh V Menon",
            "Amritha Premkumar",
            "Prajit T Rajendran",
            "Adam Wieckowski",
            "Benjamin Bross",
            "Christian Timmerer",
            "Detlev Marpe"
        ],
        "primary_subject": "Multimedia (cs.MM)",
        "abstract": "Traditional per-title encoding schemes aim to optimize encoding resolutionsto deliver the highest perceptual quality for each representation. However,keeping the encoding time within an acceptable threshold for a smooth userexperience is important to reduce the carbon footprint and energy consumptionon encoding servers in video streaming applications. Toward this realization,we introduce an encoding latency-a ware dynamic resolution encoding scheme(LADRE) for adaptive video streaming applications. LADRE determines theencoding resolution for each target bitrate by utilizing a random forest-basedprediction model for every video segment based on spatiotemporal features andthe acceptable target latency. Experimental results show that LADRE achieves anoverall average quality improvement of 0.58 dB PSNR and 0.43 dB XPSNR whilemaintaining the same bitrate, compared to the HTTP Live Streaming (HLS) bitrateladder encoding of 200 s segments using the VVenC encoder, when the encodinglatency for each representation is set to remain below the 200 s threshold.This is accompanied by an 84.17 % reduction in overall encoding energyconsumption."
    },
    {
        "link": "https://arxiv.org/abs/2401.15347",
        "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
        "authors": [
            "Seungcheol Park",
            "Jaehyeon Choi",
            "Sojin Lee",
            "U Kang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "How can we compress language models without sacrificing accuracy? The numberof compression algorithms for language models is rapidly growing to benefitfrom remarkable advances of recent language models without side effects due tothe gigantic size of language models, such as increased carbon emissions andexpensive maintenance fees. While numerous compression algorithms have shownremarkable progress in compressing language models, it ironically becomeschallenging to capture emerging trends and identify the fundamental conceptsunderlying them due to the excessive number of algorithms. In this paper, wesurvey and summarize diverse compression algorithms including pruning,quantization, knowledge distillation, low-rank approximation, parametersharing, and efficient architecture design. We not only summarize the overalltrend of diverse compression algorithms but also select representativealgorithms and provide in-depth analyses of them. We discuss the value of eachcategory of compression algorithms, and the desired properties of low-costcompression algorithms which have a significant impact due to the emergence oflarge language models. Finally, we introduce promising future research topicsbased on our survey results."
    },
    {
        "link": "https://arxiv.org/abs/2401.15348",
        "title": "AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model",
        "authors": [
            "Beijia Chen",
            "Yuefan Shen",
            "Qing Shuai",
            "Xiaowei Zhou",
            "Kun Zhou",
            "Youyi Zheng"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent communities have seen significant progress in building photo-realisticanimatable avatars from sparse multi-view videos. However, current workflowsstruggle to render realistic garment dynamics for loose-fitting characters asthey predominantly rely on naked body models for human modeling while leavingthe garment part un-modeled. This is mainly due to that the deformationsyielded by loose garments are highly non-rigid, and capturing such deformationsoften requires dense views as supervision. In this paper, we introduceAniDress, a novel method for generating animatable human avatars in looseclothes using very sparse multi-view videos (4-8 in our setting). To allow thecapturing and appearance learning of loose garments in such a situation, weemploy a virtual bone-based garment rigging model obtained from physics-basedsimulation data. Such a model allows us to capture and render complex garmentdynamics through a set of low-dimensional bone transformations. Technically, wedevelop a novel method for estimating temporal coherent garment dynamics from asparse multi-view video. To build a realistic rendering for unseen garmentstatus using coarse estimations, a pose-driven deformable neural radiance fieldconditioned on both body and garment motions is introduced, providing explicitcontrol of both parts. At test time, the new garment poses can be captured fromunseen situations, derived from a physics-based or neural network-basedsimulator to drive unseen garment dynamics. To evaluate our approach, we createa multi-view dataset that captures loose-dressed performers with diversemotions. Experiments show that our method is able to render natural garmentdynamics that deviate highly from the body and generalize well to both unseenviews and poses, surpassing the performance of existing methods. The code anddata will be publicly available."
    },
    {
        "link": "https://arxiv.org/abs/2401.15351",
        "title": "A Survey on Neural Topic Models: Methods, Applications, and Challenges",
        "authors": [
            "Xiaobao Wu",
            "Thong Nguyen",
            "Anh Tuan Luu"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Topic models have been prevalent for decades to discover latent topics andinfer topic proportions of documents in an unsupervised fashion. They have beenwidely used in various applications like text analysis and contextrecommendation. Recently, the rise of neural networks has facilitated theemergence of a new research field -- Neural Topic Models (NTMs). Different fromconventional topic models, NTMs directly optimize parameters without requiringmodel-specific derivations. This endows NTMs with better scalability andflexibility, resulting in significant research attention and plentiful newmethods and applications. In this paper, we present a comprehensive survey onneural topic models concerning methods, applications, and challenges.Specifically, we systematically organize current NTM methods according to theirnetwork structures and introduce the NTMs for various scenarios like shorttexts and cross-lingual documents. We also discuss a wide range of popularapplications built on NTMs. Finally, we highlight the challenges confronted byNTMs to inspire future research."
    },
    {
        "link": "https://arxiv.org/abs/2401.15352",
        "title": "Randomized query composition and product distributions",
        "authors": [
            "Swagato Sanyal"
        ],
        "primary_subject": "Computational Complexity (cs.CC)",
        "abstract": "Let R_eps denote randomized query complexity for error probability eps, andR:=R_{1/3}. In this work we investigate whether a perfect composition theoremR(f o g^n)=Omega(R(f).R(g)) holds for a relation f in {0,1}^n * S and a totalinner function g:{0,1}^m \\to {0, 1}.Let D^(prod) denote the maximum distributional query complexity with respectto any product (over variables) distribution. In this work we show thecomposition theorem R(f o g^n)=Omega(R(f).D^{prod}(g)) up to logarithmicfactors. In light of the minimax theorem which states that R(g) is the maximumdistributional complexity of g over any distribution, our result makes progresstowards answering the composition question. We prove our result by means of acomplexity measure R^(prod)_(eps) that we define for total Boolean functions.We show it to be equivalent (up to logarithmic factors) to the sabotagecomplexity measure RS() defined by Ben-David and Kothari (ICALP 2019): RS(g) =Theta(R^(prod)_(1/3)(g)) (up to log factors).We ask if our bound RS(g) = Omega(D^(prod)(g)) (up to log factors) is tight.We answer this question in the negative, by showing that for the NAND treefunction, sabotage complexity is polynomially larger than D^(prod). Our proofyields an alternative and different derivation of the tight lower bound on thebounded error randomized query complexity of the NAND tree function (originallyproved by Santha in 1985), which may be of independent interest. Our resultgives an explicit polynomial separation between R and D^(prod) which, to ourknowledge, was not known prior to our work."
    },
    {
        "link": "https://arxiv.org/abs/2401.15355",
        "title": "Improved bounds on the interactive capacity via error pattern analysis",
        "authors": [
            "Mudit Aggarwal",
            "Manuj Mukherjee"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Any interactive protocol between a pair of parties can be reliably simulatedin the presence of noise with a multiplicative overhead on the number of rounds(Schulman 1996). The reciprocal of the best (least) overhead is called theinteractive capacity of the noisy channel.In this work, we present lower bounds on the interactive capacity of thebinary erasure channel. Our lower bound improves the best known bound due toBen-Yishai et al. 2021 by roughly a factor of 1.75. The improvement is due to atighter analysis of the correctness of the simulation protocol using errorpattern analysis. More precisely, instead of using the well-known technique ofbounding the least number of erasures needed to make the simulation fail, weidentify and bound the probability of specific erasure patterns causingsimulation failure.We remark that error pattern analysis can be useful in solving other problemsinvolving stochastic noise, such as bounding the interactive capacity ofdifferent channels."
    },
    {
        "link": "https://arxiv.org/abs/2401.15356",
        "title": "A Statistical Framework for Measuring AI Reliance",
        "authors": [
            "Ziyang Guo",
            "Yifan Wu",
            "Jason Hartline",
            "Jessica Hullman"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Humans frequently make decisions with the aid of artificially intelligent(AI) systems. A common pattern is for the AI to recommend an action to thehuman who retains control over the final decision. Researchers have identifiedensuring that a human has appropriate reliance on an AI as a critical componentof achieving complementary performance. We argue that the current definition ofappropriate reliance used in such research lacks formal statistical groundingand can lead to contradictions. We propose a formal definition of reliance,based on statistical decision theory, which separates the concepts of relianceas the probability the decision-maker follows the AI's prediction fromchallenges a human may face in differentiating the signals and forming accuratebeliefs about the situation. Our definition gives rise to a framework that canbe used to guide the design and interpretation of studies on human-AIcomplementarity and reliance. Using recent AI-advised decision making studiesfrom literature, we demonstrate how our framework can be used to separate theloss due to mis-reliance from the loss due to not accurately differentiatingthe signals. We evaluate these losses by comparing to a baseline and abenchmark for complementary performance defined by the expected payoff achievedby a rational agent facing the same decision task as the behavioral agents."
    },
    {
        "link": "https://arxiv.org/abs/2401.15360",
        "title": "Importance-Aware Data Augmentation for Document-Level Neural Machine Translation",
        "authors": [
            "Minghao Wu",
            "Yufei Wang",
            "George Foster",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Document-level neural machine translation (DocNMT) aims to generatetranslations that are both coherent and cohesive, in contrast to itssentence-level counterpart. However, due to its longer input length and limitedavailability of training data, DocNMT often faces the challenge of datasparsity. To overcome this issue, we propose a novel Importance-Aware DataAugmentation (IADA) algorithm for DocNMT that augments the training data basedon token importance information estimated by the norm of hidden states andtraining gradients. We conduct comprehensive experiments on three widely-usedDocNMT benchmarks. Our empirical results show that our proposed IADAoutperforms strong DocNMT baselines as well as several data augmentationapproaches, with statistical significance on both sentence-level anddocument-level BLEU."
    },
    {
        "link": "https://arxiv.org/abs/2401.15362",
        "title": "Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval",
        "authors": [
            "Ayush Dubey",
            "Shiv Ram Dubey",
            "Satish Kumar Singh",
            "Wei-Ta Chu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Unsupervised image retrieval aims to learn the important visualcharacteristics without any given level to retrieve the similar images for agiven query image. The Convolutional Neural Network (CNN)-based approaches havebeen extensively exploited with self-supervised contrastive learning for imagehashing. However, the existing approaches suffer due to lack of effectiveutilization of global features by CNNs and biased-ness created by falsenegative pairs in the contrastive learning. In this paper, we propose aTransClippedCLR model by encoding the global context of an image usingTransformer having local context through patch based processing, by generatingthe hash codes through product quantization and by avoiding the potential falsenegative pairs through clipped contrastive learning. The proposed model istested with superior performance for unsupervised image retrieval on benchmarkdatasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recentstate-of-the-art deep models. The results using the proposed clippedcontrastive learning are greatly improved on all datasets as compared to samebackbone network with vanilla contrastive learning."
    },
    {
        "link": "https://arxiv.org/abs/2401.15363",
        "title": "Fair and Efficient Ridesharing: A Dynamic Programming-based Relocation Approach",
        "authors": [
            "Aqsa Ashraf Makhdomi",
            "Iqra Altaf Gillani"
        ],
        "primary_subject": "Data Structures and Algorithms (cs.DS)",
        "abstract": "Recommending routes by their probability of having a rider has long been thegoal of conventional route recommendation systems. While this maximizes theplatform-specific criteria of efficiency, it results in sub-optimal outcomeswith the disparity among the income of drivers who work for similar timeframes. Pioneer studies on fairness in ridesharing platforms have focused onalgorithms that match drivers and riders. However, these studies do notconsider the time schedules of different riders sharing a ride in theridesharing mode. To overcome this shortcoming, we present the first routerecommendation system for ridesharing networks that explicitly considersfairness as an evaluation criterion. In particular, we design a routingmechanism that reduces the inequality among drivers and provides them withroutes that have a similar probability of finding riders over a period of time.However, while optimizing fairness the efficiency of the platform should not beaffected as both of these goals are important for the long-term sustainabilityof the system. In order to jointly optimize fairness and efficiency we considerrepositioning drivers with low income to the areas that have a higherprobability of finding riders in future. While applying driver repositioning,we design a future-aware policy and allocate the areas to the driversconsidering the destination of requests in the corresponding area. Extensivesimulations on real-world datasets of Washington DC and New York demonstratesuperior performance by our proposed system in comparison to the existingbaselines."
    },
    {
        "link": "https://arxiv.org/abs/2401.15365",
        "title": "An open dataset for oracle bone script recognition and decipherment",
        "authors": [
            "Pengjie Wang",
            "Kaile Zhang",
            "Yuliang Liu",
            "Jinpeng Wan",
            "Haisu Guan",
            "Zhebin Kuang",
            "Xinyu Wang",
            "Lianwen Jin",
            "Xiang Bai"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Oracle Bone Script (OBS), one of the earliest known forms of ancient Chinesewriting, holds invaluable insights into the humanities and geography of theShang Dynasty, dating back 3,000 years. The immense historical and culturalsignificance of these writings cannot be overstated. However, the passage oftime has obscured much of their meaning, presenting a significant challenge indeciphering these ancient texts. With the advent of Artificial Intelligence(AI), employing AI to assist in interpreting OBS has become a feasible option.Yet, progress in this area has been hindered by a lack of high-qualitydatasets. To address this issue, this paper details the creation of theHUST-OBS dataset. This dataset encompasses 77,064 images of 1,588 individualdeciphered scripts and 62,989 images of 9,411 undeciphered characters, with atotal of 140,053 images, compiled from diverse sources. Additionally, allimages and labels have been reviewed and corrected by experts in oracle bonestudies. The hope is that this dataset could inspire and assist future researchin deciphering those unknown OBS."
    },
    {
        "link": "https://arxiv.org/abs/2401.15366",
        "title": "Face to Cartoon Incremental Super-Resolution using Knowledge Distillation",
        "authors": [
            "Trinetra Devkatte",
            "Shiv Ram Dubey",
            "Satish Kumar Singh",
            "Abdenour Hadid"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Facial super-resolution/hallucination is an important area of research thatseeks to enhance low-resolution facial images for a variety of applications.While Generative Adversarial Networks (GANs) have shown promise in this area,their ability to adapt to new, unseen data remains a challenge. This paperaddresses this problem by proposing an incremental super-resolution using GANswith knowledge distillation (ISR-KD) for face to cartoon. Previous research inthis area has not investigated incremental learning, which is critical forreal-world applications where new data is continually being generated. Theproposed ISR-KD aims to develop a novel unified framework for facialsuper-resolution that can handle different settings, including different typesof faces such as cartoon face and various levels of detail. To achieve this, aGAN-based super-resolution network was pre-trained on the CelebA dataset andthen incrementally trained on the iCartoonFace dataset, using knowledgedistillation to retain performance on the CelebA test set while improving theperformance on iCartoonFace test set. Our experiments demonstrate theeffectiveness of knowledge distillation in incrementally adding capability tothe model for cartoon face super-resolution while retaining the learnedknowledge for facial hallucination tasks in GANs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15368",
        "title": "The Capacity of the Weighted Read Channel",
        "authors": [
            "Omer Yerushalmi",
            "Tuvi Etzion",
            "Eitan Yaakobi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "One of the primary sequencing methods gaining prominence in DNA storage isnanopore sequencing, attributed to various factors. In this work, we consider asimplified model of the sequencer, characterized as a channel. This channeltakes a sequence and processes it using a sliding window of length \u2113,shifting the window by \u03b4 characters each time. The output of thischannel, which we refer to as the read vector, is a vector containing the sumsof the entries in each of the windows. The capacity of the channel is definedas the maximal information rate of the channel. Previous works have alreadyrevealed capacity values for certain parameters \u2113 and \u03b4. In thiswork, we show that when \u03b4<\u2113<2\u03b4, the capacity value is givenby 1\u03b4log212(\u2113+1+(\u2113+1)2\u22124(\u2113\u2212\u03b4)(\u2113\u2212\u03b4+1)\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u221a). Additionally, we construct an upper bound when2\u03b4<\u2113. Finally, we extend the model to the two-dimensional case andpresent several results on its capacity."
    },
    {
        "link": "https://arxiv.org/abs/2401.15369",
        "title": "Privacy-Preserving Cross-Domain Sequential Recommendation",
        "authors": [
            "Zhaohao Lin",
            "Weike Pan",
            "Zhong Ming"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "Cross-domain sequential recommendation is an important development directionof recommender systems. It combines the characteristics of sequentialrecommender systems and cross-domain recommender systems, which can capture thedynamic preferences of users and alleviate the problem of cold-start users.However, in recent years, people pay more and more attention to their privacy.They do not want other people to know what they just bought, what videos theyjust watched, and where they just came from. How to protect the users' privacyhas become an urgent problem to be solved. In this paper, we propose a novelprivacy-preserving cross-domain sequential recommender system (PriCDSR), whichcan provide users with recommendation services while preserving their privacyat the same time. Specifically, we define a new differential privacy on thedata, taking into account both the ID information and the order information.Then, we design a random mechanism that satisfies this differential privacy andprovide its theoretical proof. Our PriCDSR is a non-invasive method that canadopt any cross-domain sequential recommender system as a base model withoutany modification to it. To the best of our knowledge, our PriCDSR is the firstwork to investigate privacy issues in cross-domain sequential recommendersystems. We conduct experiments on three domains, and the results demonstratethat our PriCDSR, despite introducing noise, still outperforms recommendersystems that only use data from a single domain."
    },
    {
        "link": "https://arxiv.org/abs/2401.15371",
        "title": "LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning",
        "authors": [
            "Pengjie Liu",
            "Zhenghao Liu",
            "Xiaoyuan Yi",
            "Liner Yang",
            "Shuo Wang",
            "Yu Gu",
            "Ge Yu",
            "Xing Xie",
            "Shuang-hua Yang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Most existing Legal Judgment Prediction (LJP) models focus on discovering thelegal triggers in the criminal fact description. However, in real-worldscenarios, a professional judge not only needs to assimilate the law caseexperience that thrives on past sentenced legal judgments but also depends onthe professional legal grounded reasoning that learned from professional legalknowledge. In this paper, we propose a LegalDuet model, which pretrainslanguage models to learn a tailored embedding space for making legal judgments.It proposes a dual-view legal clue reasoning mechanism, which derives from tworeasoning chains of judges: 1) Law Case Reasoning, which makes legal judgmentsaccording to the judgment experiences learned from analogy/confusing legalcases; 2) Legal Ground Reasoning, which lies in matching the legal cluesbetween criminal cases and legal decisions. Our experiments show that LegalDuetachieves state-of-the-art performance on the CAIL2018 dataset and outperformsbaselines with about 4% improvements on average. Our dual-view reasoning basedpretraining can capture critical legal clues to learn a tailored embeddingspace to distinguish criminal cases. It reduces LegalDuet's uncertainty duringprediction and brings pretraining advances to the confusing/low frequentcharges. All codes are available at https://github.com/NEUIR/LegalDuet."
    },
    {
        "link": "https://arxiv.org/abs/2401.15377",
        "title": "Validation of artificial neural networks to model the acoustic behaviour of induction motors",
        "authors": [
            "F.J. Jimenez-Romero",
            "D. Guijo-Rubio",
            "F.R. Lara-Raya",
            "A. Ruiz-Gonzalez",
            "C. Hervas-Martinez"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In the last decade, the sound quality of electric induction motors is a hottopic in the research field. Specially, due to its high number of applications,the population is exposed to physical and psychological discomfort caused bythe noise emission. Therefore, it is necessary to minimise its psychologicalimpact on the population. In this way, the main goal of this work is toevaluate the use of multitask artificial neural networks as a modellingtechnique for simultaneously predicting psychoacoustic parameters of inductionmotors. Several inputs are used, such as, the electrical magnitudes of themotor power signal and the number of poles, instead of separating the noise ofthe electric motor from the environmental noise. Two different kind ofartificial neural networks are proposed to evaluate the acoustic quality ofinduction motors, by using the equivalent sound pressure, the loudness, theroughness and the sharpness as outputs. Concretely, two different topologieshave been considered: simple models and more complex models. The former aremore interpretable, while the later lead to higher accuracy at the cost ofhiding the cause-effect relationship. Focusing on the simple interpretablemodels, product unit neural networks achieved the best results: for MSE and forSEP. The main benefit of this product unit model is its simplicity, since only10 inputs variables are used, outlining the effective transfer mechanism ofmultitask artificial neural networks to extract common features of multipletasks. Finally, a deep analysis of the acoustic quality of induction motors indone using the best product unit neural networks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15378",
        "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
        "authors": [
            "Ahmet Yusuf Alan",
            "Enis Karaarslan",
            "Omer Aydin"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "There exist challenges in learning and understanding religions as thepresence of complexity and depth of religious doctrines and teachings. Chatbotsas question-answering systems can help in solving these challenges. LLMchatbots use NLP techniques to establish connections between topics andaccurately respond to complex questions. These capabilities make it perfect tobe used in enlightenment on religion as a question answering chatbot. However,LLMs also have a tendency to generate false information, known ashallucination. The responses of the chatbots can include content that insultspersonal religious beliefs, interfaith conflicts, and controversial orsensitive topics. It needs to avoid such cases without promoting hate speech oroffending certain groups of people or their beliefs. This study uses a vectordatabase-based Retrieval Augmented Generation (RAG) approach to enhance theaccuracy and transparency of LLMs. Our question-answering system is called as\"MufassirQAS\". We created a vector database with several open-access books thatinclude Turkish context. These are Turkish translations, and interpretations onIslam. We worked on creating system prompts with care, ensuring they provideinstructions that prevent harmful, offensive, or disrespectful responses. Wealso tested the MufassirQAS and ChatGPT with sensitive questions. We got betterperformance with our system. Study and enhancements are still in progress.Results and future works are given."
    },
    {
        "link": "https://arxiv.org/abs/2401.15380",
        "title": "Open-RadVLAD: Fast and Robust Radar Place Recognition",
        "authors": [
            "Matthew Gadd",
            "Paul Newman"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Radar place recognition often involves encoding a live scan as a vector andmatching this vector to a database in order to recognise that the vehicle is ina location that it has visited before. Radar is inherently robust to lightingor weather conditions, but place recognition with this sensor is still affectedby: (1) viewpoint variation, i.e. translation and rotation, (2) sensorartefacts or \"noises\". For 360-degree scanning radar, rotation is readily dealtwith by in some way aggregating across azimuths. Also, we argue in this workthat it is more critical to deal with the richness of representation and sensornoises than it is to deal with translational invariance - particularly in urbandriving where vehicles predominantly follow the same lane when repeating aroute. In our method, for computational efficiency, we use only the polarrepresentation. For partial translation invariance and robustness to signalnoise, we use only a one-dimensional Fourier Transform along radial returns. Wealso achieve rotational invariance and a very discriminative descriptor spaceby building a vector of locally aggregated descriptors. Our method is morecomprehensively tested than all prior radar place recognition work - over anexhaustive combination of all 870 pairs of trajectories from 30 Oxford RadarRobotCar Dataset sequences (each approximately 10 km). Code and detailedresults are provided at github.com/mttgdd/open-radvlad, as an openimplementation and benchmark for future work in this area. We achieve a medianof 91.52% in Recall@1, outstripping the 69.55% for the only other openimplementation, RaPlace, and at a fraction of its computational cost (relyingon fewer integral transforms e.g. Radon, Fourier, and inverse Fourier)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15381",
        "title": "Golay Complementary Sequences of Arbitrary Length and Asymptotic Existence of Hadamard Matrices",
        "authors": [
            "Cheng Du",
            "Yi Jiang"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In this work, we construct 4-phase Golay complementary sequence (GCS) setof cardinality 23+\u2308log2r\u2309 with arbitrary sequence lengthn, where the 1013-base expansion of n has r nonzero digits.Specifically, the GCS octets (eight sequences) cover all the lengths no greaterthan 1013. Besides, based on the representation theory of signed symmetricgroup, we construct Hadamard matrices from some special GCS to improve theirasymptotic existence: there exist Hadamard matrices of order 2tm for anyodd number m, where t=6\u230a140log2m\u230b+10."
    },
    {
        "link": "https://arxiv.org/abs/2401.15384",
        "title": "Half-positional",
        "authors": [
            "Antonio Casares",
            "Pierre Ohlmann"
        ],
        "primary_subject": "Formal Languages and Automata Theory (cs.FL)",
        "abstract": "In the context of two-player games over graphs, a language L is calledhalf-positional if, in all games using L as winning objective, theprotagonist can play optimally using positional strategies, that is, strategiesthat do not depend on the history of the play. In this work, we describe theclass of parity automata recognising half-positional languages, providing acomplete characterisation of half-positionality for \u03c9-regular languages.As corollaries, we establish decidability of half-positionality in polynomialtime, finite-to-infinite and 1-to-2-players lifts, and show the closure underunion of prefix-independent half-positional objectives, answering a conjectureby Kopczy\\'nski."
    },
    {
        "link": "https://arxiv.org/abs/2401.15385",
        "title": "Towards Event Extraction from Speech with Contextual Clues",
        "authors": [
            "Jingqi Kang",
            "Tongtong Wu",
            "Jinming Zhao",
            "Guitao Wang",
            "Guilin Qi",
            "Yuan-Fang Li",
            "Gholamreza Haffari"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "While text-based event extraction has been an active research area and hasseen successful application in many domains, extracting semantic events fromspeech directly is an under-explored problem. In this paper, we introduce theSpeech Event Extraction (SpeechEE) task and construct three synthetic trainingsets and one human-spoken test set. Compared to event extraction from text,SpeechEE poses greater challenges mainly due to complex speech signals that arecontinuous and have no word boundaries. Additionally, unlike perceptible soundevents, semantic events are more subtle and require a deeper understanding. Totackle these challenges, we introduce a sequence-to-structure generationparadigm that can produce events from speech signals in an end-to-end manner,together with a conditioned generation method that utilizes speech recognitiontranscripts as the contextual clue. We further propose to represent events witha flat format to make outputs more natural language-like. Our experimentalresults show that our method brings significant improvements on all datasets,achieving a maximum F1 gain of 10.7%. The code and datasets are released onhttps://github.com/jodie-kang/SpeechEE."
    },
    {
        "link": "https://arxiv.org/abs/2401.15390",
        "title": "A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports",
        "authors": [
            "Guadalupe Ortiz",
            "Juan Boubeta-Puig",
            "Javier Criado",
            "David Corral-Plaza",
            "Alfonso Garcia-de-Prado",
            "Inmaculada Medina-Bulo",
            "Luis Iribarne"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Major advances in telecommunications and the Internet of Things have givenrise to numerous smart city scenarios in which smart services are provided.What was once a dream for the future has now become reality. However, the needto provide these smart services quickly, efficiently, in an interoperablemanner and in real time is a cutting-edge technological challenge. Althoughsome software architectures offer solutions in this area, these are oftenlimited in terms of reusability and maintenance by independent modules,involving the need for system downtime when maintaining or evolving, as well asby a lack of standards in terms of the interoperability of their interface. Inthis paper, we propose a fully reusable microservice architecture, standardizedthrough the use of the Web of things paradigm, and with high efficiency inreal-time data processing, supported by complex event processing techniques. Toillustrate the proposal, we present a fully reusable implementation of themicroservices necessary for the deployment of the architecture in the field ofair quality monitoring and alerting in smart ports. The performance evaluationof this architecture shows excellent results."
    },
    {
        "link": "https://arxiv.org/abs/2401.15391",
        "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) byretrieving relevant knowledge, showing promising potential in mitigating LLMhallucinations and enhancing response quality, thereby facilitating the greatadoption of LLMs in practice. However, we find that existing RAG systems areinadequate in answering multi-hop queries, which require retrieving andreasoning over multiple pieces of supporting evidence. Furthermore, to ourknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.In this paper, we develop a novel dataset, MultiHop-RAG, which consists of aknowledge base, a large collection of multi-hop queries, their ground-truthanswers, and the associated supporting evidence. We detail the procedure ofbuilding the dataset, utilizing an English news article dataset as theunderlying RAG knowledge base. We demonstrate the benchmarking utility ofMultiHop-RAG in two experiments. The first experiment compares differentembedding models for retrieving evidence for multi-hop queries. In the secondexperiment, we examine the capabilities of various state-of-the-art LLMs,including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hopqueries given the evidence. Both experiments reveal that existing RAG methodsperform unsatisfactorily in retrieving and answering multi-hop queries. We hopeMultiHop-RAG will be a valuable resource for the community in developingeffective RAG systems, thereby facilitating greater adoption of LLMs inpractice. The MultiHop-RAG and implemented RAG system is publicly available athttps://github.com/yixuantt/MultiHop-RAG/."
    },
    {
        "link": "https://arxiv.org/abs/2401.15393",
        "title": "Semantics of Multiword Expressions in Transformer-Based Models: A Survey",
        "authors": [
            "Filip Mileti\u0107",
            "Sabine Schulte im Walde"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Multiword expressions (MWEs) are composed of multiple words and exhibitvariable degrees of compositionality. As such, their meanings are notoriouslydifficult to model, and it is unclear to what extent this issue affectstransformer architectures. Addressing this gap, we provide the first in-depthsurvey of MWE processing with transformer models. We overall find that theycapture MWE semantics inconsistently, as shown by reliance on surface patternsand memorized information. MWE meaning is also strongly localized,predominantly in early layers of the architecture. Representations benefit fromspecific linguistic properties, such as lower semantic idiosyncrasy andambiguity of target expressions. Our findings overall question the ability oftransformer models to robustly capture fine-grained semantics. Furthermore, wehighlight the need for more directly comparable evaluation setups."
    },
    {
        "link": "https://arxiv.org/abs/2401.15399",
        "title": "Parallel Self-assembly for Modular USVs with Diverse Docking Mechanism Layouts",
        "authors": [
            "Lianxin Zhang",
            "Yang Jiao",
            "Yihan Huang",
            "Ziyou Wang",
            "Huihuan Qian"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Self-assembly enables multi-robot systems to merge diverse capabilities andaccomplish tasks beyond the reach of individual robots. Incorporating varieddocking mechanisms layouts (DMLs) can enhance robot versatility or reducecosts. However, assembling multiple heterogeneous robots with diverse DMLs isstill a research gap. This paper addresses this problem by introducing CuBoat,an omnidirectional unmanned surface vehicle (USV). CuBoat can be equipped withor without docking systems on its four sides to emulate heterogeneous robots.We implement a multi-robot system based on multiple CuBoats. To enhancemaneuverability, a linear active disturbance rejection control (LADRC) schemeis proposed. Additionally, we present a generalized parallel self-assemblyplanning algorithm for efficient assembly among CuBoats with different DMLs.Validation is conducted through simulation within 2 scenarios across 4 distinctmaps, demonstrating the performance of the self-assembly planning algorithm.Moreover, trajectory tracking tests confirm the effectiveness of the LADRCcontroller. Self-assembly experiments on 5 maps with different targetstructures affirm the algorithm's feasibility and generality. This studyadvances robotic self-assembly, enabling multi-robot systems to collaborativelytackle complex tasks beyond the capabilities of individual robots."
    },
    {
        "link": "https://arxiv.org/abs/2401.15400",
        "title": "Indexing Portuguese NLP Resources with PT-Pump-Up",
        "authors": [
            "R\u00faben Almeida",
            "Ricardo Campos",
            "Al\u00edpio Jorge",
            "S\u00e9rgio Nunes"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "The recent advances in natural language processing (NLP) are linked totraining processes that require vast amounts of corpora. Access to this data iscommonly not a trivial process due to resource dispersion and the need tomaintain these infrastructures online and up-to-date. New developments in NLPare often compromised due to the scarcity of data or lack of a sharedrepository that works as an entry point to the community. This is especiallytrue in low and mid-resource languages, such as Portuguese, which lack data andproper resource management infrastructures. In this work, we proposePT-Pump-Up, a set of tools that aim to reduce resource dispersion and improvethe accessibility to Portuguese NLP resources. Our proposal is divided intofour software components: a) a web platform to list the available resources; b)a client-side Python package to simplify the loading of Portuguese NLPresources; c) an administrative Python package to manage the platform and d) apublic GitHub repository to foster future collaboration and contributions. Allfour components are accessible using: https://linktr.ee/pt_pump_up"
    },
    {
        "link": "https://arxiv.org/abs/2401.15407",
        "title": "Well-posedness and Euler-Maruyama approximation for stochastic fractional neutral integro-differential equations with weakly singular kernel and generalized Gronwall inequality with a multi weakly singularity",
        "authors": [
            "Javad A. Asadzade",
            "Nazim I. Mahmudov"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This manuscript examines the problem of nonlinear stochastic fractionalneutral integro-differential equations with weakly singular kernels. Our focusis on obtaining precise estimates to cover all possible cases of Abel-typesingular kernels. Initially, we establish the existence, uniqueness, andcontinuous dependence on the initial value of the true solution, assuming alocal Lipschitz condition and linear growth condition. Additionally, we developthe Euler-Maruyama method for the numerical solution of the equation and proveits strong convergence under the same conditions as the well-posedness.Moreover, we determine the accurate convergence rate of this method underglobal Lipschitz conditions and linear growth conditions. And also we haveproven generalized Gronwall inequality with a multi-weakly singularity."
    },
    {
        "link": "https://arxiv.org/abs/2401.15414",
        "title": "An Implicit Physical Face Model Driven by Expression and Style",
        "authors": [
            "Lingchen Yang",
            "Gaspard Zoss",
            "Prashanth Chandran",
            "Paulo Gotardo",
            "Markus Gross",
            "Barbara Solenthaler",
            "Eftychios Sifakis",
            "Derek Bradley"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D facial animation is often produced by manipulating facial deformationmodels (or rigs), that are traditionally parameterized by expression controls.A key component that is usually overlooked is expression 'style', as in, how aparticular expression is performed. Although it is common to define a semanticbasis of expressions that characters can perform, most characters perform eachexpression in their own style. To date, style is usually entangled with theexpression, and it is not possible to transfer the style of one character toanother when considering facial animation. We present a new face model, basedon a data-driven implicit neural physics model, that can be driven by bothexpression and style separately. At the core, we present a framework forlearning implicit physics-based actuations for multiple subjectssimultaneously, trained on a few arbitrary performance capture sequences from asmall set of identities. Once trained, our method allows generalizedphysics-based facial animation for any of the trained identities, extending tounseen performances. Furthermore, it grants control over the animation style,enabling style transfer from one character to another or blending styles ofdifferent characters. Lastly, as a physics-based model, it is capable ofsynthesizing physical effects, such as collision handling, setting our methodapart from conventional approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15417",
        "title": "Fault Diagnosis on Induction Motor using Machine Learning and Signal Processing",
        "authors": [
            "Muhammad Samiullah",
            "Hasan Ali",
            "Shehryar Zahoor",
            "Anas Ali"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The detection and identification of induction motor faults using machinelearning and signal processing is a valuable approach to avoiding plantdisturbances and shutdowns in the context of Industry 4.0. In this work, wepresent a study on the detection and identification of induction motor faultsusing machine learning and signal processing with MATLAB Simulink. We developeda model of a three-phase induction motor in MATLAB Simulink to generate healthyand faulty motor data. The data collected included stator currents, rotorcurrents, input power, slip, rotor speed, and efficiency. We generated fourfaults in the induction motor: open circuit fault, short circuit fault,overload, and broken rotor bars. We collected a total of 150,000 data pointswith a 60-40% ratio of healthy to faulty motor data. We applied Fast FourierTransform (FFT) to detect and identify healthy and unhealthy conditions andadded a distinctive feature in our data. The generated dataset was traineddifferent machine learning models. On comparing the accuracy of the models onthe test set, we concluded that the Decision Tree algorithm performed the bestwith an accuracy of about 92%. Our study contributes to the literature byproviding a valuable approach to fault detection and classification withmachine learning models for industrial applications."
    },
    {
        "link": "https://arxiv.org/abs/2401.15422",
        "title": "A Survey on Data Augmentation in Large Model Era",
        "authors": [
            "Yue Zhou",
            "Chenlu Guo",
            "Xu Wang",
            "Yi Chang",
            "Yuan Wu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Large models, encompassing large language and diffusion models, have shownexceptional promise in approximating human-level intelligence, garneringsignificant interest from both academic and industrial spheres. However, thetraining of these large models necessitates vast quantities of high-qualitydata, and with continuous updates to these models, the existing reservoir ofhigh-quality data may soon be depleted. This challenge has catalyzed a surge inresearch focused on data augmentation methods. Leveraging large models, thesedata augmentation techniques have outperformed traditional approaches. Thispaper offers an exhaustive review of large model-driven data augmentationmethods, adopting a comprehensive perspective. We begin by establishing aclassification of relevant studies into three main categories: imageaugmentation, text augmentation, and paired data augmentation. Following this,we delve into various data post-processing techniques pertinent to largemodel-based data augmentation. Our discussion then expands to encompass thearray of applications for these data augmentation methods within naturallanguage processing, computer vision, and audio signal processing. We proceedto evaluate the successes and limitations of large model-based dataaugmentation across different scenarios. Concluding our review, we highlightprospective challenges and avenues for future exploration in the field of dataaugmentation. Our objective is to furnish researchers with critical insights,ultimately contributing to the advancement of more sophisticated large models.We consistently maintain the related open-source materials at:https://github.com/MLGroup-JLU/LLM-data-aug-survey."
    },
    {
        "link": "https://arxiv.org/abs/2401.15436",
        "title": "A simple and complete discrete exterior calculus on general polygonal meshes",
        "authors": [
            "Lenka Ptackova",
            "Luiz Velho"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Discrete exterior calculus (DEC) offers a coordinate-free discretization ofexterior calculus especially suited for computations on curved spaces. In thiswork, we present an extended version of DEC on surface meshes formed by generalpolygons that bypasses the need for combinatorial subdivision and does notinvolve any dual mesh. At its core, our approach introduces a new polygonalwedge product that is compatible with the discrete exterior derivative in thesense that it satisfies the Leibniz product rule. Based on the discrete wedgeproduct, we then derive a novel primal-to-primal Hodge star operator. Combiningthese three `basic operators' we then define new discrete versions of thecontraction operator and Lie derivative, codifferential and Laplace operator.We discuss the numerical convergence of each one of these proposed operatorsand compare them to existing DEC methods. Finally, we show simple applicationsof our operators on Helmholtz-Hodge decomposition, Laplacian surface fairing,and Lie advection of functions and vector fields on meshes formed by generalpolygons."
    },
    {
        "link": "https://arxiv.org/abs/2401.15439",
        "title": "Pre-training and Diagnosing Knowledge Base Completion Models",
        "authors": [
            "Vid Kocijan",
            "Myeongjun Erik Jang",
            "Thomas Lukasiewicz"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "In this work, we introduce and analyze an approach to knowledge transfer fromone collection of facts to another without the need for entity or relationmatching. The method works for both canonicalized knowledge bases anduncanonicalized or open knowledge bases, i.e., knowledge bases where more thanone copy of a real-world entity or relation may exist. The main contribution isa method that can make use of large-scale pre-training on facts, which werecollected from unstructured text, to improve predictions on structured datafrom a specific domain. The introduced method is most impactful on smalldatasets such as ReVerb20k, where a 6% absolute increase of mean reciprocalrank and 65% relative decrease of mean rank over the previously best method wasachieved, despite not relying on large pre-trained models like Bert. Tounderstand the obtained pre-trained models better, we then introduce a noveldataset for the analysis of pre-trained models for Open Knowledge BaseCompletion, called Doge (Diagnostics of Open knowledge Graph Embeddings). Itconsists of 6 subsets and is designed to measure multiple properties of apre-trained model: robustness against synonyms, ability to perform deductivereasoning, presence of gender stereotypes, consistency with reverse relations,and coverage of different areas of general knowledge. Using the introduceddataset, we show that the existing OKBC models lack consistency in the presenceof synonyms and inverse relations and are unable to perform deductivereasoning. Moreover, their predictions often align with gender stereotypes,which persist even when presented with counterevidence. We additionallyinvestigate the role of pre-trained word embeddings and demonstrate thatavoiding biased word embeddings is not a sufficient measure to prevent biasedbehavior of OKBC models."
    },
    {
        "link": "https://arxiv.org/abs/2401.15441",
        "title": "An overview of IoT architectures, technologies, and existing open-source projects",
        "authors": [
            "Tom\u00e1s Dom\u00ednguez-Bola\u00f1o",
            "Omar Campos",
            "Valent\u00edn Barral",
            "Carlos J. Escudero",
            "Jos\u00e9 A. Garc\u00eda-Naya"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Today's needs for monitoring and control of different devices inorganizations require an Internet of Things (IoT) platform that can integrateheterogeneous elements provided by multiple vendors and using differentprotocols, data formats and communication technologies. This article provides acomprehensive review of all the architectures, technologies, protocols and dataformats most commonly used by existing IoT platforms. On this basis, acomparative analysis of the most widely used open source IoT platforms ispresented. This exhaustive comparison is based on multiple characteristics thatwill be essential to select the platform that best suits the needs of eachorganization."
    },
    {
        "link": "https://arxiv.org/abs/2401.15443",
        "title": "DiffuserLite: Towards Real-time Diffusion Planning",
        "authors": [
            "Zibin Dong",
            "Jianye Hao",
            "Yifu Yuan",
            "Fei Ni",
            "Yitian Wang",
            "Pengyi Li",
            "Yan Zheng"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion planning has been recognized as an effective decision-makingparadigm in various domains. The high-quality conditional generation capabilityof long-horizon trajectories makes it a promising research direction. However,existing diffusion planning methods suffer from low decision-making frequenciesbecause of the expensive iterative sampling cost. To address this issue, weintroduce DiffuserLite, a fast and lightweight diffusion planning framework.DiffuserLite employs a planning refinement process (PRP) to generatecoarse-to-fine-grained trajectories, which significantly reduces the modelingof redundant information and leads to notable increases in decision-makingfrequency. Our experimental results demonstrate that DiffuserLite incurs only0.88% of the runtime cost compared to previous frameworks, achieves anaverage decision-making frequency of 122Hz, and reaches state-of-the-artperformance on D4RL benchmarks. In addition, our clean DiffuserLite frameworkcan serve as a flexible plugin to enhance decision frequency in other diffusionplanning algorithms, providing a structural design reference for future works.More details and visualizations are available at [projectwebsite](https://diffuserlite.github.io/)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15444",
        "title": "Towards Causal Classification: A Comprehensive Study on Graph Neural Networks",
        "authors": [
            "Simi Job",
            "Xiaohui Tao",
            "Taotao Cai",
            "Lin Li",
            "Haoran Xie",
            "Jianming Yong"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The exploration of Graph Neural Networks (GNNs) for processinggraph-structured data has expanded, particularly their potential for causalanalysis due to their universal approximation capabilities. Anticipated tosignificantly enhance common graph-based tasks such as classification andprediction, the development of a causally enhanced GNN framework is yet to bethoroughly investigated. Addressing this shortfall, our study delves into ninebenchmark graph classification models, testing their strength and versatilityacross seven datasets spanning three varied domains to discern the impact ofcausality on the predictive prowess of GNNs. This research offers a detailedassessment of these models, shedding light on their efficiency, and flexibilityin different data environments, and highlighting areas needing advancement. Ourfindings are instrumental in furthering the understanding and practicalapplication of GNNs in diverse datacentric fields"
    },
    {
        "link": "https://arxiv.org/abs/2401.15447",
        "title": "Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing",
        "authors": [
            "Lokesh Nagalapatti",
            "Akshay Iyer",
            "Abir De",
            "Sunita Sarawagi"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We address the Individualized continuous treatment effect (ICTE) estimationproblem where we predict the effect of any continuous-valued treatment on anindividual using observational data. The main challenge in this estimation taskis the potential confounding of treatment assignment with an individual'scovariates in the training data, whereas during inference ICTE requiresprediction on independently sampled treatments. In contrast to prior work thatrelied on regularizers or unstable GAN training, we advocate the directapproach of augmenting training individuals with independently sampledtreatments and inferred counterfactual outcomes. We infer counterfactualoutcomes using a two-pronged strategy: a Gradient Interpolation forclose-to-observed treatments, and a Gaussian Process based Kernel Smoothingwhich allows us to downweigh high variance inferences. We evaluate our methodon five benchmarks and show that our method outperforms six state-of-the-artmethods on the counterfactual estimation error. We analyze the superiorperformance of our method by showing that (1) our inferred counterfactualresponses are more accurate, and (2) adding them to the training data reducesthe distributional distance between the confounded training distribution andtest distribution where treatment is independent of covariates. Our proposedmethod is model-agnostic and we show that it improves ICTE accuracy of severalexisting models."
    },
    {
        "link": "https://arxiv.org/abs/2401.15448",
        "title": "A Systematic Review of Available Datasets in Additive Manufacturing",
        "authors": [
            "Xiao Liu",
            "Alessandra Mileo",
            "Alan F. Smeaton"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In-situ monitoring incorporating data from visual and other sensortechnologies, allows the collection of extensive datasets during the AdditiveManufacturing (AM) process. These datasets have potential for determining thequality of the manufactured output and the detection of defects through the useof Machine Learning during the manufacturing process. Open and annotateddatasets derived from AM processes are necessary for the machine learningcommunity to address this opportunity, which creates difficulties in theapplication of computer vision-related machine learning in AM. This systematicreview investigates the availability of open image-based datasets originatingfrom AM processes that align with a number of pre-defined selection criteria.The review identifies existing gaps among the current image-based datasets inthe domain of AM, and points to the need for greater availability of opendatasets in order to allow quality assessment and defect detection duringadditive manufacturing, to develop."
    },
    {
        "link": "https://arxiv.org/abs/2401.15449",
        "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation",
        "authors": [
            "Yuxin Liang",
            "Zhuoyang Song",
            "Hao Wang",
            "Jiaxing Zhang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern andexpress their internal knowledge state, a key factor in countering factualhallucination and ensuring reliable application of LLMs. We observe a robustself-awareness of internal knowledge state in LLMs, evidenced by over 85%accuracy in knowledge probing. However, LLMs often fail to express theirinternal knowledge during generation, leading to factual hallucinations. Wedevelop an automated hallucination annotation tool, Dreamcatcher, which mergesknowledge probing and consistency checking methods to rank factual preferencedata. Using knowledge preference as reward, We propose a Reinforcement Learningfrom Knowledge Feedback (RLKF) training framework, leveraging reinforcementlearning to enhance the factuality and honesty of LLMs. Our experiments acrossmultiple models show that RLKF training effectively enhances the ability ofmodels to utilize their internal knowledge state, boosting performance in avariety of knowledge-based and honesty-related tasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15453",
        "title": "Bayesian Inference Accelerator for Spiking Neural Networks",
        "authors": [
            "Prabodh Katti",
            "Anagha Nimbekar",
            "Chen Li",
            "Amit Acharyya",
            "Bashir M. Al-Hashimi",
            "Bipin Rajendran"
        ],
        "primary_subject": "Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Bayesian neural networks offer better estimates of model uncertainty comparedto frequentist networks. However, inference involving Bayesian models requiresmultiple instantiations or sampling of the network parameters, requiringsignificant computational resources. Compared to traditional deep learningnetworks, spiking neural networks (SNNs) have the potential to reducecomputational area and power, thanks to their event-driven and spike-basedcomputational framework. Most works in literature either address frequentistSNN models or non-spiking Bayesian neural networks. In this work, wedemonstrate an optimization framework for developing and implementing efficientBayesian SNNs in hardware by additionally restricting network weights to bebinary-valued to further decrease power and area consumption. We demonstrateaccuracies comparable to Bayesian binary networks with full-precision Bernoulliparameters, while requiring up to 25\u00d7 less spikes than equivalent binarySNN implementations. We show the feasibility of the design by mapping it ontoZynq-7000, a lightweight SoC, and achieve a 6.5\u00d7 improvement inGOPS/DSP while utilizing up to 30 times less power compared to thestate-of-the-art."
    },
    {
        "link": "https://arxiv.org/abs/2401.15455",
        "title": "New Foggy Object Detecting Model",
        "authors": [
            "Rahul Banavathu",
            "Modem Veda Sree",
            "Bollina Kavya Sri",
            "Suddhasil De"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Object detection in reduced visibility has become a prominent research area.The existing techniques are not accurate enough in recognizing objects undersuch circumstances. This paper introduces a new foggy object detection methodthrough a two-staged architecture of region identification from input imagesand detecting objects in such regions. The paper confirms notable improvementsof the proposed method's accuracy and detection time over existing techniques."
    },
    {
        "link": "https://arxiv.org/abs/2401.15458",
        "title": "A New Method for Vehicle Logo Recognition Based on Swin Transformer",
        "authors": [
            "Yang Li",
            "Doudou Zhang",
            "Jianli Xiao"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and bigdata analysis to monitor real-time traffic conditions, aiming to improvetraffic efficiency and safety. Accurate vehicle recognition is crucial in thisprocess, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enableseffective management and monitoring by distinguishing vehicles on the road.Convolutional Neural Networks (CNNs) have made impressive strides in VLRresearch. However, achieving higher performance demands significant time andcomputational resources for training. Recently, the rise of Transformer modelshas brought new opportunities to VLR. Swin Transformer, with its efficientcomputation and global feature modeling capabilities, outperforms CNNs underchallenging conditions. In this paper, we implement real-time VLR using SwinTransformer and fine-tune it for optimal performance. Extensive experimentsconducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD)demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%,respectively. Additionally, the use of a transfer learning strategy enables ourmethod to be on par with state-of-the-art VLR methods. These findings affirmthe superiority of our approach over existing methods. Future research canexplore and optimize the application of the Swin Transformer in other vehiclevision recognition tasks to drive advancements in ITS."
    },
    {
        "link": "https://arxiv.org/abs/2401.15459",
        "title": "Large Language Model as Synthesizer: Fusing Diverse Inputs for Better Automatic Vulnerability Repair",
        "authors": [
            "Xin Zhou",
            "Kisub Kim",
            "Bowen Xu",
            "DongGyun Han",
            "David Lo"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "The advances of deep learning (DL) have paved the way for automatic softwarevulnerability repair approaches, which effectively learn the mapping from thevulnerable code to the fixed code. Nevertheless, existing DL-basedvulnerability repair methods face notable limitations: 1) they struggle tohandle lengthy vulnerable code, 2) they treat code as natural language texts,neglecting its inherent structure, and 3) they do not tap into the valuableexpert knowledge present in the expert system. To address this, we proposeVulMaster, a Transformer-based neural network model that excels at generatingvulnerability repairs by comprehensively understanding the entire vulnerablecode, irrespective of its length. This model also integrates diverseinformation, encompassing vulnerable code structures and expert knowledge fromthe CWE system. We evaluated VulMaster on a real-world C/C++ vulnerabilityrepair dataset comprising 1,754 projects with 5,800 vulnerable functions. Theexperimental results demonstrated that VulMaster exhibits substantialimprovements compared to the learning-based state-of-the-art vulnerabilityrepair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEUscores from 10.2\\% to 20.0\\%, 21.3\\% to 29.3\\%, and 32.5\\% to 40.9\\%,respectively."
    },
    {
        "link": "https://arxiv.org/abs/2401.15463",
        "title": "DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure",
        "authors": [
            "Junyi Ye",
            "Mengnan Du",
            "Guiling Wang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This paper introduces DataFrame question answering (QA), a novel task thatutilizes large language models (LLMs) to generate Pandas queries forinformation retrieval and data analysis on dataframes, emphasizing safe andnon-revealing data handling. Our method, which solely relies on dataframecolumn names, not only ensures data privacy but also significantly reduces thecontext window in the prompt, streamlining information processing andaddressing major challenges in LLM-based data analysis. We propose DataFrame QAas a comprehensive framework that includes safe Pandas query generation andcode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1metric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA',tailored for complex data analysis queries. Our findings indicate that GPT-4achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,underscoring its capability in securely retrieving and aggregating dataframevalues and conducting sophisticated data analyses. This approach, deployable ina zero-shot manner without prior training or adjustments, proves to be highlyadaptable and secure for diverse applications."
    },
    {
        "link": "https://arxiv.org/abs/2401.15468",
        "title": "Large Language Model for Vulnerability Detection: Emerging Results and Future Directions",
        "authors": [
            "Xin Zhou",
            "Ting Zhang",
            "David Lo"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Previous learning-based vulnerability detection methods relied on eithermedium-sized pre-trained models or smaller neural networks from scratch. Recentadvancements in Large Pre-Trained Language Models (LLMs) have showcasedremarkable few-shot learning capabilities in various tasks. However, theeffectiveness of LLMs in detecting software vulnerabilities is largelyunexplored. This paper aims to bridge this gap by exploring how LLMs performwith various prompts, particularly focusing on two state-of-the-art LLMs:GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achievescompetitive performance with the prior state-of-the-art vulnerability detectionapproach and GPT-4 consistently outperformed the state-of-the-art."
    },
    {
        "link": "https://arxiv.org/abs/2401.15469",
        "title": "Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models",
        "authors": [
            "Fabio Merizzi",
            "Andrea Asperti",
            "Stefano Colamonaco"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolutionregional reanalysis dataset for the European domain. In recent years it hasshown significant utility across various climate-related tasks, ranging fromforecasting and climate change research to renewable energy prediction,resource management, air quality risk assessment, and the forecasting of rareevents, among others. Unfortunately, the availability of CERRA is lagging twoyears behind the current date, due to constraints in acquiring the requisiteexternal data and the intensive computational demands inherent in itsgeneration. As a solution, this paper introduces a novel method using diffusionmodels to approximate CERRA downscaling in a data-driven manner, withoutadditional informations. By leveraging the lower resolution ERA5 dataset, whichprovides boundary conditions for CERRA, we approach this as a super-resolutiontask. Focusing on wind speed around Italy, our model, trained on existing CERRAdata, shows promising results, closely mirroring original CERRA data.Validation with in-situ observations further confirms the model's accuracy inapproximating ground measurements."
    },
    {
        "link": "https://arxiv.org/abs/2401.15470",
        "title": "Robust globally divergence-free weak Galerkin methods for stationary incompressible convective Brinkman-Forchheimer equations",
        "authors": [
            "X.J. Wang",
            "X.P. Xie"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This paper develops a class of robust weak Galerkin methods for thestationary incompressible convective Brinkman-Forchheimer equations. Themethods adopt piecewise polynomials of degrees m\u00a0(m\u22651) and m\u22121respectively for the approximations of velocity and pressure variables insidethe elements and piecewise polynomials of degrees k\u00a0(k=m\u22121,m) and mrespectively for their numerical traces on the interfaces of elements, and areshown to yield globally divergence-free velocity approximation. Existence anduniqueness results for the discrete schemes, as well as optimal a priori errorestimates, are established. A convergent linearized iterative algorithm is alsopresented. Numerical experiments are provided to verify the performance of theproposed methods"
    },
    {
        "link": "https://arxiv.org/abs/2401.15471",
        "title": "ConvoSense: Overcoming Monotonous Commonsense Inferences for Conversational AI",
        "authors": [
            "Sarah E. Finch",
            "Jinho D. Choi"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Mastering commonsense understanding and reasoning is a pivotal skillessential for conducting engaging conversations. While there have been severalattempts to create datasets that facilitate commonsense inferences in dialoguecontexts, existing datasets tend to lack in-depth details, restate informationalready present in the conversation, and often fail to capture the multifacetednature of commonsense reasoning. In response to these limitations, we compile anew synthetic dataset for commonsense reasoning in dialogue contexts using GPT,ConvoSense, that boasts greater contextual novelty, offers a higher volume ofinferences per example, and substantially enriches the detail conveyed by theinferences. Our dataset contains over 500,000 inferences across 12,000dialogues with 10 popular inference types, which empowers the training ofgenerative commonsense models for dialogue that are superior in producingplausible inferences with high novelty when compared to models trained on theprevious datasets. To the best of our knowledge, ConvoSense is the first of itskind to provide such a multitude of novel inferences at such a large scale."
    },
    {
        "link": "https://arxiv.org/abs/2401.15472",
        "title": "Temporal evolution in synthetic handwriting",
        "authors": [
            "Cristina Carmona-Duarte",
            "Miguel A. Ferrer",
            "Antonio Parziale",
            "Angelo Marcelli"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "New methods for generating synthetic handwriting images for biometricapplications have recently been developed. The temporal evolution ofhandwriting from childhood to adulthood is usually left unexplored in theseworks. This paper proposes a novel methodology for including temporal evolutionin a handwriting synthesizer by means of simplifying the text trajectory planand handwriting dynamics. This is achieved through a tailored version of thekinematic theory of rapid human movements and the neuromotor inspiredhandwriting synthesizer. The realism of the proposed method has been evaluatedby comparing the temporal evolution of real and synthetic samples bothquantitatively and subjectively. The quantitative test is based on a visualperception algorithm that compares the letter variability and the number ofstrokes in the real and synthetic handwriting produced at different ages. Inthe subjective test, 30 people are asked to evaluate the perceived realism ofthe evolution of the synthetic handwriting."
    },
    {
        "link": "https://arxiv.org/abs/2401.15473",
        "title": "iDeLog: Iterative Dual Spatial and Kinematic Extraction of Sigma-Lognormal Parameters",
        "authors": [
            "Miguel A. Ferrer",
            "Moises Diaz",
            "Cristina Carmona-Duarte",
            "Rejean Plamondon"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Kinematic Theory of rapid movements and its associated Sigma-Lognormalmodel have been extensively used in a large variety of applications. While thephysical and biological meaning of the model have been widely tested andvalidated for rapid movements, some shortcomings have been detected when it isused with continuous long and complex movements. To alleviate such drawbacks,and inspired by the motor equivalence theory and a conceivable visual feedback,this paper proposes a novel framework to extract the Sigma-Lognormalparameters, namely iDeLog. Specifically, iDeLog consists of two steps. Thefirst one, influenced by the motor equivalence model, separately derives aninitial action plan defined by a set of virtual points and angles from thetrajectory and a sequence of lognormals from the velocity. In the second step,based on a hypothetical visual feedback compatible with an open-loop motorcontrol, the virtual target points of the action plan are iteratively moved toimprove the matching between the observed and reconstructed trajectory andvelocity. During experiments conducted with handwritten signatures, iDeLogobtained promising results as compared to the previous development of theSigma-Lognormal."
    },
    {
        "link": "https://arxiv.org/abs/2401.15475",
        "title": "Epidemic Population Games And Perturbed Best Response Dynamics",
        "authors": [
            "Shinkyu Park",
            "Jair Certorio",
            "Nuno C. Martins",
            "Richard J. La"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "This paper proposes an approach to mitigate epidemic spread in a populationof strategic agents by encouraging safer behaviors through carefully designedrewards. These rewards, which vary according to the state of the epidemic, areascribed by a dynamic payoff mechanism we seek to design. We use a modifiedSIRS model to track how the epidemic progresses in response to the population'sagents strategic choices. By employing perturbed best response evolutionarydynamics to model the population's strategic behavior, we extend previousrelated work so as to allow for noise in the agents' perceptions of the rewardsand intrinsic costs of the available strategies. Central to our approach is theuse of system-theoretic methods and passivity concepts to obtain a Lyapunovfunction, ensuring the global asymptotic stability of an endemic equilibriumwith minimized infection prevalence, under budget constraints. We use theLyapunov function to construct anytime upper bounds for the size of thepopulation's infectious fraction. For a class of one-parameter perturbed bestresponse models, we propose a method to learn the model's parameter from data."
    },
    {
        "link": "https://arxiv.org/abs/2401.15476",
        "title": "To Burst or Not to Burst: Generating and Quantifying Improbable Text",
        "authors": [
            "Kuleen Sasse",
            "Samuel Barham",
            "Efsun Sarioglu Kayi",
            "Edward W. Staley"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "While large language models (LLMs) are extremely capable at text generation,their outputs are still distinguishable from human-authored text. We explorethis separation across many metrics over text, many sampling techniques, manytypes of text data, and across two popular LLMs, LLaMA and Vicuna. Along theway, we introduce a new metric, recoverability, to highlight differencesbetween human and machine text; and we propose a new sampling technique, burstsampling, designed to close this gap. We find that LLaMA and Vicuna havedistinct distributions under many of the metrics, and that this influences ourresults: Recoverability separates real from fake text better than any othermetric when using LLaMA. When using Vicuna, burst sampling produces text whichis distributionally closer to real text compared to other sampling techniques."
    },
    {
        "link": "https://arxiv.org/abs/2401.15479",
        "title": "Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data",
        "authors": [
            "Amrit Poudel",
            "Tim Weninger"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "Recent decisions to discontinue access to social media APIs are havingdetrimental effects on Internet research and the field of computational socialscience as a whole. This lack of access to data has been dubbed the Post-APIera of Internet research. Fortunately, popular search engines have the means tocrawl, capture, and surface social media data on their Search Engine ResultsPages (SERP) if provided the proper search query, and may provide a solution tothis dilemma. In the present work we ask: does SERP provide a complete andunbiased sample of social media data? Is SERP a viable alternative to directAPI-access? To answer these questions, we perform a comparative analysisbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. Wefind that SERP results are highly biased in favor of popular posts; againstpolitical, pornographic, and vulgar posts; are more positive in theirsentiment; and have large topical gaps. Overall, we conclude that SERP is not aviable alternative to social media API access."
    },
    {
        "link": "https://arxiv.org/abs/2401.15480",
        "title": "Social Interpretable Reinforcement Learning",
        "authors": [
            "Leonardo Lucio Custode",
            "Giovanni Iacca"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning (RL) bears the promise of being an enabling technologyfor many applications. However, since most of the literature in the field iscurrently focused on opaque models, the use of RL in high-stakes scenarios,where interpretability is crucial, is still limited. Recently, some approachesto interpretable RL, e.g., based on Decision Trees, have been proposed, but oneof the main limitations of these techniques is their training cost. To overcomethis limitation, we propose a new population-based method, called SocialInterpretable RL (SIRL), inspired by social learning principles, to improvelearning efficiency. Our method mimics a social learning process, where eachagent in a group learns to solve a given task based both on its own individualexperience as well as the experience acquired together with its peers. Ourapproach is divided into two phases. In the \\emph{collaborative phase}, all theagents in the population interact with a shared instance of the environment,where each agent observes the state and independently proposes an action. Then,voting is performed to choose the action that will actually be performed in theenvironment. In the \\emph{individual phase}, each agent refines its individualperformance by interacting with its own instance of the environment. Thismechanism makes the agents experience a larger number of episodes whilesimultaneously reducing the computational cost of the process. Our results onsix well-known benchmarks show that SIRL reaches state-of-the-art performancew.r.t. the alternative interpretable methods from the literature."
    },
    {
        "link": "https://arxiv.org/abs/2401.15481",
        "title": "BugsInPy: A Database of Existing Bugs in Python Programs to Enable Controlled Testing and Debugging Studies",
        "authors": [
            "Ratnadira Widyasari",
            "Sheng Qin Sim",
            "Camellia Lok",
            "Haodi Qi",
            "Jack Phan",
            "Qijin Tay",
            "Constance Tan",
            "Fiona Wee",
            "Jodie Ethelda Tan",
            "Yuheng Yieh",
            "Brian Goh",
            "Ferdian Thung",
            "Hong Jin Kang",
            "Thong Hoang",
            "David Lo",
            "Eng Lieh Ouh"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "The 2019 edition of Stack Overflow developer survey highlights that, for thefirst time, Python outperformed Java in terms of popularity. The gap betweenPython and Java further widened in the 2020 edition of the survey.Unfortunately, despite the rapid increase in Python's popularity, there are notmany testing and debugging tools that are designed for Python. This is in starkcontrast with the abundance of testing and debugging tools for Java. Thus,there is a need to push research on tools that can help Python developers. Onefactor that contributed to the rapid growth of Java testing and debugging toolsis the availability of benchmarks. A popular benchmark is the Defects4Jbenchmark; its initial version contained 357 real bugs from 5 real-world Javaprograms. Each bug comes with a test suite that can expose the bug. Defects4Jhas been used by hundreds of testing and debugging studies and has helped topush the frontier of research in these directions. In this project, inspired byDefects4J, we create another benchmark database and tool that contain 493 realbugs from 17 real-world Python programs. We hope our benchmark can helpcatalyze future work on testing and debugging tools that work on Pythonprograms."
    },
    {
        "link": "https://arxiv.org/abs/2401.15482",
        "title": "Unsupervised Solution Operator Learning for Mean-Field Games via Sampling-Invariant Parametrizations",
        "authors": [
            "Han Huang",
            "Rongjie Lai"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Recent advances in deep learning has witnessed many innovative frameworksthat solve high dimensional mean-field games (MFG) accurately and efficiently.These methods, however, are restricted to solving single-instance MFG anddemands extensive computational time per instance, limiting practicality. Toovercome this, we develop a novel framework to learn the MFG solution operator.Our model takes a MFG instances as input and output their solutions with oneforward pass. To ensure the proposed parametrization is well-suited foroperator learning, we introduce and prove the notion of sampling invariance forour model, establishing its convergence to a continuous operator in thesampling limit. Our method features two key advantages. First, it isdiscretization-free, making it particularly suitable for learning operators ofhigh-dimensional MFGs. Secondly, it can be trained without the need for accessto supervised labels, significantly reducing the computational overheadassociated with creating training datasets in existing operator learningmethods. We test our framework on synthetic and realistic datasets with varyingcomplexity and dimensionality to substantiate its robustness."
    },
    {
        "link": "https://arxiv.org/abs/2401.15484",
        "title": "R",
        "authors": [
            "Gagan Khandate",
            "Tristan L. Saidi",
            "Siqi Shang",
            "Eric T. Chang",
            "Yang Liu",
            "Seth Dennis",
            "Johnson Adams",
            "Matei Ciocarlie"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "We present a method for enabling Reinforcement Learning of motor controlpolicies for complex skills such as dexterous manipulation. We posit that a keydifficulty for training such policies is the difficulty of exploring theproblem state space, as the accessible and useful regions of this space form acomplex structure along manifolds of the original high-dimensional state space.This work presents a method to enable and support exploration withSampling-based Planning. We use a generally applicable non-holonomicRapidly-exploring Random Trees algorithm and present multiple methods to usethe resulting structure to bootstrap model-free Reinforcement Learning. Ourmethod is effective at learning various challenging dexterous motor controlskills of higher difficulty than previously shown. In particular, we achievedexterous in-hand manipulation of complex objects while simultaneously securingthe object without the use of passive support surfaces. These policies alsotransfer effectively to real robots. A number of example videos can also befound on the project website: https://sbrl.cs.columbia.edu"
    },
    {
        "link": "https://arxiv.org/abs/2401.15486",
        "title": "Pulse-Width Modulation Technique With Harmonic Injection in the Modulating Wave and Discontinuous Frequency Modulation for the Carrier Wave for Multilevel Inverters: An Application to the Reduction of Acoustic Noise in Induction Motors",
        "authors": [
            "Antonio Ruiz-Gonzalez",
            "Juan-Ramon Heredia-Larrubia",
            "Mario J. Meco-Gutierrez",
            "Francisco-M. Perez-Hidalgo"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "An implementation of a harmonic injection pulse width modulationfrequency-modulated triangular carrier (HIPWM-FMTC) control strategy applied toa multilevel power inverter feeding an asynchronous motor is presented. The aimwas to justify the reduction in acoustic noise emitted by the machine comparedwith other strategies in the technical literature. In addition, we checked howthe THD at the inverter output was reduced compared to the other controltechniques used as a reference. The proposed strategy is based on frequencymodulation of the triangular carrier. The main advantage of the proposed methodis that only one control parameter is required for modifying the electricalspectrum. Therefore, the mechanical natural frequencies and spatial harmonicsof the machine can be avoided, and acoustic noise can be reduced. Theeffectiveness of the technique was demonstrated after laboratory validation bycomparing the acoustic results for a 1 kW motor. The results obtained from thelaboratory tests are presented and compared with those of other acousticmeasurements using different PWM strategies."
    },
    {
        "link": "https://arxiv.org/abs/2401.15487",
        "title": "Artificial Intelligence: Arguments for Catastrophic Risk",
        "authors": [
            "Adam Bales",
            "William D'Alessandro",
            "Cameron Domenico Kirk-Giannini"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "Recent progress in artificial intelligence (AI) has drawn attention to thetechnology's transformative potential, including what some see as its prospectsfor causing large-scale harm. We review two influential arguments purporting toshow how AI could pose catastrophic risks. The first argument -- the Problem ofPower-Seeking -- claims that, under certain assumptions, advanced AI systemsare likely to engage in dangerous power-seeking behavior in pursuit of theirgoals. We review reasons for thinking that AI systems might seek power, thatthey might obtain it, that this could lead to catastrophe, and that we mightbuild and deploy such systems anyway. The second argument claims that thedevelopment of human-level AI will unlock rapid further progress, culminatingin AI systems far more capable than any human -- this is the SingularityHypothesis. Power-seeking behavior on the part of such systems might beparticularly dangerous. We discuss a variety of objections to both argumentsand conclude by assessing the state of the debate."
    },
    {
        "link": "https://arxiv.org/abs/2401.15489",
        "title": "Distilling Privileged Multimodal Information for Expression Recognition using Optimal Transport",
        "authors": [
            "Muhammad Haseeb Aslam",
            "Muhammad Osama Zeeshan",
            "Soufiane Belharbi",
            "Marco Pedersoli",
            "Alessandro Koerich",
            "Simon Bacon",
            "Eric Granger"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal affect recognition models have reached remarkable performance inthe lab environment due to their ability to model complementary and redundantsemantic information. However, these models struggle in the wild, mainlybecause of the unavailability or quality of modalities used for training. Inpractice, only a subset of the training-time modalities may be available attest time. Learning with privileged information (PI) enables deep learningmodels (DL) to exploit data from additional modalities only available duringtraining. State-of-the-art knowledge distillation (KD) methods have beenproposed to distill multiple teacher models (each trained on a modality) to acommon student model. These privileged KD methods typically utilizepoint-to-point matching and have no explicit mechanism to capture thestructural information in the teacher representation space formed byintroducing the privileged modality. We argue that encoding this same structurein the student space may lead to enhanced student performance. This paperintroduces a new structural KD mechanism based on optimal transport (OT), whereentropy-regularized OT distills the structural dark knowledge. Privileged KDwith OT (PKDOT) method captures the local structures in the multimodal teacherrepresentation by calculating a cosine similarity matrix and selects the top-kanchors to allow for sparse OT solutions, resulting in a more stabledistillation process. Experiments were performed on two different problems:pain estimation on the Biovid dataset (ordinal classification) andarousal-valance prediction on the Affwild2 dataset (regression). Results showthat the proposed method can outperform state-of-the-art privileged KD methodson these problems. The diversity of different modalities and fusionarchitectures indicates that the proposed PKDOT method is modality andmodel-agnostic."
    },
    {
        "link": "https://arxiv.org/abs/2401.15492",
        "title": "Cholesky-like Preconditioner for Hodge Laplacians via Heavy Collapsible Subcomplex",
        "authors": [
            "Anton Savostianov",
            "Francesco Tudisco",
            "Nicola Guglielmi"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Techniques based on k-th order Hodge Laplacian operators Lk are widelyused to describe the topology as well as the governing dynamics of high-ordersystems modeled as simplicial complexes. In all of them, it is required tosolve a number of least square problems with Lk as coefficient matrix, forexample in order to compute some portions of the spectrum or integrate thedynamical system. In this work, we introduce the notion of optimal collapsiblesubcomplex and we present a fast combinatorial algorithm for the computation ofa sparse Cholesky-like preconditioner for Lk that exploits the topologicalstructure of the simplicial complex. The performance of the preconditioner istested for conjugate gradient method for least square problems (CGLS) on avariety of simplicial complexes with different dimensions and edge densities.We show that, for sparse simplicial complexes, the new preconditioner reducessignificantly the condition number of Lk and performs better than thestandard incomplete Cholesky factorization."
    },
    {
        "link": "https://arxiv.org/abs/2401.15495",
        "title": "Nobody Expects a Differential Equation: Minimum Energy-Per-Bit for the Gaussian Relay Channel with Rank-1 Linear Relaying",
        "authors": [
            "Oliver Kosut",
            "Michelle Effros",
            "Michael Langberg"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Motivated by the design of low-complexity low-power coding solutions for theGaussian relay channel, this work presents an upper bound on the minimumenergy-per-bit achievable on the Gaussian relay channel using rank-1 linearrelaying. Our study addresses high-dimensional relay codes and presents boundsthat outperform prior known bounds using 2-dimensional schemes. A novelty ofour analysis ties the optimization problem at hand to the solution of a certaindifferential equation which, in turn, leads to a low energy-per-bit achievablescheme."
    },
    {
        "link": "https://arxiv.org/abs/2401.15496",
        "title": "Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization",
        "authors": [
            "Jianfei Xiao",
            "Yancan Chen",
            "Yimin Ou",
            "Hanyi Yu",
            "Yiyong Xiao"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) like Llama, Baichuan and Bloom models showremarkable ability with instruction fine-tuning in many natural language tasks.Nevertheless, for the dialogue summarization task, which aims to generatesummaries for different roles in dialogue, most of the state-of-the-art methodsconduct on small models (e.g Bart and Bert). Existing methods try to add taskspecified optimization on small models like adding global-local centralityscore to models. In this paper, we propose an instruction fine-tuning model:Baichuan2-Sum, for role-oriented diaglouge summarization. By setting differentinstructions for different roles, the model can learn from the dialogueinteractions and output the expected summaries. Furthermore, we applied NEFTunetechnique to add suitable noise during training to improve the results. Theexperiments demonstrate that the proposed model achieves the newstate-of-the-art results on two public dialogue summarization datasets: CSDSand SAMSUM. We release our model and related codes to facilitate future studieson dialogue summarization task."
    },
    {
        "link": "https://arxiv.org/abs/2401.15497",
        "title": "Foregrounding Artist Opinions: A Survey Study on Transparency, Ownership, and Fairness in AI Generative Art",
        "authors": [
            "Juniper Lovato",
            "Julia Zimmerman",
            "Isabelle Smith",
            "Peter Dodds",
            "Jennifer Karson"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "Generative Artificial Intelligence (AI) tools are used to create art-likeoutputs and aid in the creative process. While these tools have potentialbenefits for artists, they also have the potential to harm the art workforceand infringe upon artistic and intellectual property rights. Without explicitconsent from artists, Generative AI creators scrape artists' digital work totrain Generative AI models and produce art-like model outputs at scale. Theseoutputs are now being used to compete with human artists in the marketplace aswell as being used by some artists in their generative processes to create art.We surveyed 459 artists to investigate the tension between artists' opinions onGenerative AI art's potential utility and harm. This study surveys artists'opinions on the utility and threat of Generative AI art models, fair practicesin the disclosure of artistic works in AI art training models, ownership andrights of AI art derivatives, and fair compensation. We find that artists, byand large, think that model creators should be required to disclose in detailwhat art and images they use to train their AI models. We also find thatartists' opinions vary by professional status and practice, demographics,whether they have purchased art, and familiarity with and use of Generative AI.We hope the results of this work will further more meaningful collaboration andalignment between the art community and Generative AI researchers anddevelopers."
    },
    {
        "link": "https://arxiv.org/abs/2401.15498",
        "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
        "authors": [
            "Caiqi Zhang",
            "Zhijiang Guo",
            "Andreas Vlachos"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This paper investigates the potential benefits of language-specificfact-checking models, focusing on the case of Chinese. We demonstrate thelimitations of methods such as translating Chinese claims and evidence intoEnglish or directly using multilingual large language models (e.g. GPT4),highlighting the need for language-specific systems. We further develop astate-of-the-art Chinese fact-checking system that, in contrast to previousapproaches which treat evidence selection as a pairwise sentence classificationtask, considers the context of sentences. We also create an adversarial datasetto identify biases in our model, and while they are present as in Englishlanguage datasets and models, they are often specific to the Chinese culture.Our study emphasizes the importance of language-specific fact-checking modelsto effectively combat misinformation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15499",
        "title": "Semantic Properties of cosine based bias scores for word embeddings",
        "authors": [
            "Sarah Schr\u00f6der",
            "Alexander Schulz",
            "Fabian Hinder",
            "Barbara Hammer"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Plenty of works have brought social biases in language models to attentionand proposed methods to detect such biases. As a result, the literaturecontains a great deal of different bias tests and scores, each introduced withthe premise to uncover yet more biases that other scores fail to detect. Whatseverely lacks in the literature, however, are comparative studies that analysesuch bias scores and help researchers to understand the benefits or limitationsof the existing methods. In this work, we aim to close this gap for cosinebased bias scores. By building on a geometric definition of bias, we proposerequirements for bias scores to be considered meaningful for quantifyingbiases. Furthermore, we formally analyze cosine based scores from theliterature with regard to these requirements. We underline these findings withexperiments to show that the bias scores' limitations have an impact in theapplication case."
    },
    {
        "link": "https://arxiv.org/abs/2401.15500",
        "title": "Data-Driven Estimation of the False Positive Rate of the Bayes Binary Classifier via Soft Labels",
        "authors": [
            "Minoh Jeong",
            "Martina Cardone",
            "Alex Dytso"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Classification is a fundamental task in many applications on whichdata-driven methods have shown outstanding performances. However, it ischallenging to determine whether such methods have achieved the optimalperformance. This is mainly because the best achievable performance istypically unknown and hence, effectively estimating it is of prime importance.In this paper, we consider binary classification problems and we propose anestimator for the false positive rate (FPR) of the Bayes classifier, that is,the optimal classifier with respect to accuracy, from a given dataset. Ourmethod utilizes soft labels, or real-valued labels, which are gainingsignificant traction thanks to their properties. We thoroughly examine varioustheoretical properties of our estimator, including its consistency,unbiasedness, rate of convergence, and variance. To enhance the versatility ofour estimator beyond soft labels, we also consider noisy labels, whichencompass binary labels. For noisy labels, we develop effective FPR estimatorsby leveraging a denoising technique and the Nadaraya-Watson estimator. Due tothe symmetry of the problem, our results can be readily applied to estimate thefalse negative rate of the Bayes classifier."
    },
    {
        "link": "https://arxiv.org/abs/2401.15501",
        "title": "FloodLense: A Framework for ChatGPT-based Real-time Flood Detection",
        "authors": [
            "Pranath Reddy Kumbam",
            "Kshitij Maruti Vejre"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This study addresses the vital issue of real-time flood detection andmanagement. It innovatively combines advanced deep learning models with Largelanguage models (LLM), enhancing flood monitoring and response capabilities.This approach addresses the limitations of current methods by offering a moreaccurate, versatile, user-friendly and accessible solution. The integration ofUNet, RDN, and ViT models with natural language processing significantlyimproves flood area detection in diverse environments, including using aerialand satellite imagery. The experimental evaluation demonstrates the models'efficacy in accurately identifying and mapping flood zones, showcasing theproject's potential in transforming environmental monitoring and disastermanagement fields."
    },
    {
        "link": "https://arxiv.org/abs/2401.15503",
        "title": "Dawn of the Dead(line Misses): Impact of Job Dismiss on the Deadline Miss Rate",
        "authors": [
            "Jian-Jia Chen",
            "Mario G\u00fcnzel",
            "Peter Bella",
            "Georg von der Br\u00fcggen",
            "Kuan-Hsun Chen"
        ],
        "primary_subject": "Performance (cs.PF)",
        "abstract": "Occasional deadline misses are acceptable for soft real-time systems.Quantifying probabilistic and deterministic characteristics of deadline missesis therefore essential to ensure that deadline misses indeed happen onlyoccasionally. This is supported by recent research activities on probabilisticworst-case execution time, worst-case deadline failure probability, the maximumnumber of deadline misses, upper bounds on the deadline miss probability, andthe deadline miss rate. This paper focuses on the deadline miss rate of aperiodic soft real-time task in the long run. Our model assumes that this softreal-time task has an arbitrary relative deadline and that a job can still beexecuted after a deadline-miss until a dismiss point. This model generalizesthe existing models that either dismiss a job immediately after its deadlinemiss or never dismiss a job. We provide mathematical notation on theconvergence of the deadline miss rate in the long run and essential propertiesto calculate the deadline miss rate. Specifically, we use a Markov chain tomodel the execution behavior of a periodic soft real-time task. We present therequired ergodicity property to ensure that the deadline miss rate in the longrun is described by a stationary distribution."
    },
    {
        "link": "https://arxiv.org/abs/2401.15507",
        "title": "\"May I Speak?\": Multi-modal Attention Guidance in Social VR Group Conversations",
        "authors": [
            "Geonsun Lee",
            "Dae Yeol Lee",
            "Guan-Ming Su",
            "Dinesh Manocha"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "In this paper, we present a novel multi-modal attention guidance methoddesigned to address the challenges of turn-taking dynamics in meetings andenhance group conversations within virtual reality (VR) environments.Recognizing the difficulties posed by a confined field of view and the absenceof detailed gesture tracking in VR, our proposed method aims to mitigate thechallenges of noticing new speakers attempting to join the conversation. Thisapproach tailors attention guidance, providing a nuanced experience for highlyengaged participants while offering subtler cues for those less engaged,thereby enriching the overall meeting dynamics. Through group interviewstudies, we gathered insights to guide our design, resulting in a prototypethat employs \"light\" as a diegetic guidance mechanism, complemented by spatialaudio. The combination creates an intuitive and immersive meeting environment,effectively directing users' attention to new speakers. An evaluation study,comparing our method to state-of-the-art attention guidance approaches,demonstrated significantly faster response times (p < 0.001), heightenedperceived conversation satisfaction (p < 0.001), and preference (p < 0.001) forour method. Our findings contribute to the understanding of design implicationsfor VR social attention guidance, opening avenues for future research anddevelopment."
    },
    {
        "link": "https://arxiv.org/abs/2401.15508",
        "title": "Proto-MPC: An Encoder-Prototype-Decoder Approach for Quadrotor Control in Challenging Winds",
        "authors": [
            "Yuliang Gu",
            "Sheng Cheng",
            "Naira Hovakimyan"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Quadrotors are increasingly used in the evolving field of aerial robotics fortheir agility and mechanical simplicity. However, inherent uncertainties, suchas aerodynamic effects coupled with quadrotors' operation in dynamicallychanging environments, pose significant challenges for traditional, nominalmodel-based control designs. We propose a multi-task meta-learning methodcalled Encoder-Prototype-Decoder (EPD), which has the advantage of effectivelybalancing shared and distinctive representations across diverse training tasks.Subsequently, we integrate the EPD model into a model predictive controlproblem (Proto-MPC) to enhance the quadrotor's ability to adapt and operateacross a spectrum of dynamically changing tasks with an efficient onlineimplementation. We validate the proposed method in simulations, whichdemonstrates Proto-MPC's robust performance in trajectory tracking of aquadrotor being subject to static and spatially varying side winds."
    },
    {
        "link": "https://arxiv.org/abs/2401.15509",
        "title": "Style-News: Incorporating Stylized News Generation and Adversarial Verification for Neural Fake News Detection",
        "authors": [
            "Wei-Yao Wang",
            "Yu-Chieh Chang",
            "Wen-Chih Peng"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "With the improvements in generative models, the issues of producinghallucinations in various domains (e.g., law, writing) have been brought topeople's attention due to concerns about misinformation. In this paper, wefocus on neural fake news, which refers to content generated by neural networksaiming to mimic the style of real news to deceive people. To prevent harmfuldisinformation spreading fallaciously from malicious social media (e.g.,content farms), we propose a novel verification framework, Style-News, usingpublisher metadata to imply a publisher's template with the corresponding texttypes, political stance, and credibility. Based on threat modeling aspects, astyle-aware neural news generator is introduced as an adversary for generatingnews content conditioning for a specific publisher, and style and sourcediscriminators are trained to defend against this attack by identifying whichpublisher the style corresponds with, and discriminating whether the source ofthe given news is human-written or machine-generated. To evaluate the qualityof the generated content, we integrate various dimensional metrics (languagefluency, content preservation, and style adherence) and demonstrate thatStyle-News significantly outperforms the previous approaches by a margin of0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, ourdiscriminative model outperforms state-of-the-art baselines in terms ofpublisher prediction (up to 4.64%) and neural fake news detection (+6.94%\u223c 31.72%)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15510",
        "title": "DocuBits: VR Document Decomposition for Procedural Task Completion",
        "authors": [
            "Geonsun Lee",
            "Jennifer Healey",
            "Dinesh Manocha"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Reading monolithic instructional documents in VR is often challenging,especially when tasks are collaborative. Here we present DocuBits, a novelmethod for transforming monolithic documents into small, interactiveinstructional elements. Our approach allows users to:(i) create instructionalelements (ii) position them within VR and (iii) use them to monitor and shareprogress in a multi-user VR learning environment. We describe our designmethodology as well as two user studies evaluating how both individual usersand pairs of users interact with DocuBits compared to monolithic documentswhile performing a chemistry lab task. Our analysis shows that, for bothstudies, DocuBits had substantially higher usability, while decreasingperceived workload (p < 0.001$. Our collaborative study showed thatparticipants perceived higher social presence, collaborator awareness as wellas immersion and presence (p < 0.001). We discuss our insights for usingtext-based instructions to support enhanced collaboration in VR environments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15511",
        "title": "Distributed Resilient Interval Observer Synthesis for Nonlinear Discrete-Time Systems",
        "authors": [
            "Mohammad Khajenejad",
            "Scott Brown",
            "Sonia Martinez"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "This paper introduces a novel recursive distributed estimation algorithmaimed at synthesizing input and state interval observers for nonlinearbounded-error discrete-time multi-agent systems. The considered systems havesensors and actuators that are susceptible to unknown or adversarial inputs. Tosolve this problem, we first identify conditions that allow agents to obtainnonlinear bounded-error equations characterizing the input. Then, we propose adistributed interval-valued observer that is guaranteed to contain thedisturbance and system states. To do this, we first detail a gain designprocedure that uses global problem data to minimize an upper bound on the\u21131 norm of the observer error. We then propose a gain design approachthat does not require global information, using only values that are local toeach agent. The second method improves on the computational tractability of thefirst, at the expense of some added conservatism. Further, we discuss somepossible ways of extending the results to a broader class of systems. Weconclude by demonstrating our observer on two examples. The first is a unicyclesystem, for which we apply the first gain design method. The second is a145-bus power system, which showcases the benefits of the second method, due tothe first approach being intractable for systems with high dimensional statespaces."
    },
    {
        "link": "https://arxiv.org/abs/2401.15517",
        "title": "Signal Recovery From Product of Two Vandermonde Matrices",
        "authors": [
            "Dzevdan Kapetanovic"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In this work, we present some new results for compressed sensing and phaseretrieval. For compressed sensing, it is shown that if the unknownn-dimensional vector can be expressed as a linear combination of s unknownVandermonde vectors (with Fourier vectors as a special case) and themeasurement matrix is a Vandermonde matrix, exact recovery of the vector with2s measurements and O(poly(s)) complexity is possible when n\u22652s. From these results, a measurement matrix is constructed from which it ispossible to recover s-sparse n-dimensional vectors for n\u22652s with asfew as 2s measurements and with a recovery algorithm of O(poly(s))complexity. In the second part of the work, these results are extended to thechallenging problem of phase retrieval. The most significant discovery in thisdirection is that if the unknown n-dimensional vector is composed of sfrequencies with at least one being non-harmonic, n\u22654s\u22121 and we takeat least 8s\u22123 Fourier measurements, there are, remarkably, only two possiblevectors producing the observed measurement values and they are easilyobtainable from each other. The two vectors can be found by an algorithm withonly O(poly(s)) complexity. An immediate application of the newresult is construction of a measurement matrix from which it is possible torecover all s-sparse n-dimensional signals (up to a global phase) fromO(s) magnitude-only measurements and O(poly(s)) recoverycomplexity when n\u22654s\u22121."
    },
    {
        "link": "https://arxiv.org/abs/2401.15520",
        "title": "Oracle-Efficient Hybrid Online Learning with Unknown Distribution",
        "authors": [
            "Changlong Wu",
            "Jin Sima",
            "Wojciech Szpankowski"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We study the problem of oracle-efficient hybrid online learning when thefeatures are generated by an unknown i.i.d. process and the labels aregenerated adversarially. Assuming access to an (offline) ERM oracle, we showthat there exists a computationally efficient online predictor that achieves aregret upper bounded by O~(T34) for a finite-VC class, andupper bounded by O~(Tp+1p+2) for a class with \u03b1fat-shattering dimension \u03b1\u2212p. This provides the first knownoracle-efficient sublinear regret bounds for hybrid online learning with anunknown feature generation process. In particular, it confirms a conjecture ofLazaric and Munos (JCSS 2012). We then extend our result to the scenario ofshifting distributions with K changes, yielding a regret of orderO~(T45K15). Finally, we establish a regret ofO~((K23(log|H|)13+K)\u22c5T45) for the contextual K-armed bandits with a finite policy setH, i.i.d. generated contexts from an unknown distribution, andadversarially generated costs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15522",
        "title": "New time domain decomposition methods for parabolic optimal control problems II: Neumann-Neumann algorithms",
        "authors": [
            "Martin Jakob Gander",
            "Liu-Di Lu"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We present new Neumann-Neumann algorithms based on a time domaindecomposition applied to unconstrained parabolic optimal control problems.After a spatial semi-discretization, the Lagrange multiplier approach providesa coupled forward-backward optimality system, which can be solved using a timedomain decomposition. Due to the forward-backward structure of the optimalitysystem, nine variants can be found for the Neumann-Neumann algorithms. Weanalyze their convergence behavior and determine the optimal relaxationparameter for each algorithm. Our analysis reveals that the most naturalalgorithms are actually only good smoothers, and there are better choices whichlead to efficient solvers. We illustrate our analysis with numericalexperiments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15523",
        "title": "Performance-Based Biped Control using a Consumer Depth Camera",
        "authors": [
            "Yoonsang Lee",
            "Taesoo Kwon"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "We present a technique for controlling physically simulated characters usinguser inputs from an off-the-shelf depth camera. Our controller takes areal-time stream of user poses as input, and simulates a stream of target posesof a biped based on it. The simulated biped mimics the user's actions whilemoving forward at a modest speed and maintaining balance. The controller isparameterized over a set of modulated reference motions that aims to cover therange of possible user actions. For real-time simulation, the best set ofcontrol parameters for the current input pose is chosen from the parameterizedsets of pre-computed control parameters via a regression method. By applyingthe chosen parameters at each moment, the simulated biped can imitate a rangeof user actions while walking in various interactive scenarios."
    },
    {
        "link": "https://arxiv.org/abs/2401.15525",
        "title": "Multi-Interval Energy-Reserve Co-Optimization with SoC-Dependent Bids from Battery Storage",
        "authors": [
            "Cong Chen",
            "Siying Li",
            "Lang Tong"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "We consider the problem of co-optimized energy-reserve market clearing withstate-of-charge (SoC) dependent bids from battery storage participants. WhileSoC-dependent bidding accurately captures storage's degradation and opportunitycosts, such bids result in a non-convex optimization in the market clearingprocess. More challenging is the regulation reserve capacity clearing, wherethe SoC-dependent cost is uncertain as it depends on the unknown regulationtrajectories ex-post of the market clearing. Addressing the nonconvexity anduncertainty in a multi-interval co-optimized real-time energy-reserve market,we introduce a simple restriction on the SoC-dependent bids along with a robustoptimization formulation, transforming the non-convex market clearing underuncertainty into a standard convex piece-wise linear program and making itpossible for large-scale storage integration. Under reasonable assumptions, weshow that SoC-dependent bids yield higher profit for storage participants thanthat from SoC-independent bids. Numerical simulations demonstrate a 28%-150%profit increase of the proposed SoC-dependent bids compared with theSoC-independent counterpart."
    },
    {
        "link": "https://arxiv.org/abs/2401.15526",
        "title": "Exploring the Transferability of a Foundation Model for Fundus Images: Application to Hypertensive Retinopathy",
        "authors": [
            "Julio Silva-Rodriguez",
            "Jihed Chelbi",
            "Waziha Kabir",
            "Hadi Chakor",
            "Jose Dolz",
            "Ismail Ben Ayed",
            "Riadh Kobbi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Using deep learning models pre-trained on Imagenet is the traditionalsolution for medical image classification to deal with data scarcity.Nevertheless, relevant literature supports that this strategy may offer limitedgains due to the high dissimilarity between domains. Currently, the paradigm ofadapting domain-specialized foundation models is proving to be a promisingalternative. However, how to perform such knowledge transfer, and the benefitsand limitations it presents, are under study. The CGI-HRDC challenge forHypertensive Retinopathy diagnosis on fundus images introduces an appealingopportunity to evaluate the transferability of a recently releasedvision-language foundation model of the retina, FLAIR. In this work, we explorethe potential of using FLAIR features as starting point for fundus imageclassification, and we compare its performance with regard to Imagenetinitialization on two popular transfer learning methods: Linear Probing (LP)and Fine-Tuning (FP). Our empirical observations suggest that, in any case, theuse of the traditional strategy provides performance gains. In contrast, directtransferability from FLAIR model allows gains of 2.5%. When fine-tuning thewhole network, the performance gap increases up to 4%. In this case, we showthat avoiding feature deterioration via LP initialization of the classifierallows the best re-use of the rich pre-trained features. Although directtransferability using LP still offers limited performance, we believe thatfoundation models such as FLAIR will drive the evolution of deep-learning-basedfundus image analysis."
    },
    {
        "link": "https://arxiv.org/abs/2401.15529",
        "title": "A Thorough Study of State Leakage Mitigation in Quantum Computing with One-Time Pad",
        "authors": [
            "Chuanqi Xu",
            "Jamie Sikora",
            "Jakub Szefer"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "The ability for users to access quantum computers through the cloud hasincreased rapidly in recent years. Despite still being Noisy Intermediate-ScaleQuantum (NISQ) machines, modern quantum computers are now being activelyemployed for research and by numerous startups. Quantum algorithms typicallyproduce probabilistic results, necessitating repeated execution to produce thedesired outcomes. In order for the execution to begin from the specified groundstate each time and for the results of the prior execution not to interferewith the results of the subsequent execution, the reset mechanism must beperformed between each iteration to effectively reset the qubits. However, dueto noise and errors in quantum computers and specifically these resetmechanisms, a noisy reset operation may lead to systematic errors in theoverall computation, as well as potential security and privacy vulnerabilitiesof information leakage. To counter this issue, we thoroughly examine the stateleakage problem in quantum computing, and then propose a solution by employingthe classical and quantum one-time pads before the reset mechanism to preventthe state leakage, which works by randomly applying simple gates for eachexecution of the circuit. In addition, this work explores conditions underwhich the classical one-time pad, which uses fewer resources, is sufficient toprotect state leakage. Finally, we study the role of various errors in stateleakage, by evaluating the degrees of leakage under different error levels ofgate, measurement, and sampling errors. Our findings offer new perspectives onthe design of reset mechanisms and secure quantum computing systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15530",
        "title": "An Information-Theoretic Analysis of In-Context Learning",
        "authors": [
            "Hong Jun Jeon",
            "Jason D. Lee",
            "Qi Lei",
            "Benjamin Van Roy"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Previous theoretical results pertaining to meta-learning on sequences buildon contrived assumptions and are somewhat convoluted. We introduce newinformation-theoretic tools that lead to an elegant and very generaldecomposition of error into three components: irreducible error, meta-learningerror, and intra-task error. These tools unify analyses across manymeta-learning challenges. To illustrate, we apply them to establish new resultsabout in-context learning with transformers. Our theoretical resultscharacterizes how error decays in both the number of training sequences andsequence lengths. Our results are very general; for example, they avoidcontrived mixing time assumptions made by all prior results that establishdecay of error with sequence length."
    },
    {
        "link": "https://arxiv.org/abs/2401.15532",
        "title": "Byte Pair Encoding Is All You Need For Automatic Bengali Speech Recognition",
        "authors": [
            "Ahnaf Mozib Samin"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Byte pair encoding (BPE) emerges as an effective tokenization method fortackling the out-of-vocabulary (OOV) challenge in various natural language andspeech processing tasks. Recent research highlights the dependency of BPEsubword tokenization's efficacy on the morphological nature of the language,particularly in languages rich in inflectional morphology, where fewer BPEmerges suffice for generating highly productive tokens. Motivated by this, ourstudy empirically identifies the optimal number of BPE tokens for Bengali, alanguage known for its morphological complexity, thus enhancingout-of-distribution automatic speech recognition (ASR) performance.Experimental evaluation reveals that an excessively high number of BPE tokenscan lead to overfitting, while approximately 500-1000 tokens result in superiorOOV performance. Furthermore, we conduct a comparative analysis of BPE withcharacter-based and unigram-based tokenization methods. By introducing BPEtokenization to Bengali ASR, we achieve a substantial reduction in the worderror rate (WER) from 66.44% in our character-based baseline system to 63.80%on the LB-ASRTD eval set and from 46.34% to 42.80% on the SHRUTI eval set, bothof which include out-of-distribution data."
    },
    {
        "link": "https://arxiv.org/abs/2401.15534",
        "title": "CRYSTALS-Kyber With Lattice Quantizer",
        "authors": [
            "Shuiyin Liu",
            "Amin Sakzad"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Module Learning with Errors (M-LWE) based key reconciliation mechanisms (KRM)can be viewed as quantizing an M-LWE sample according to a lattice codebook.This paper describes a generic M-LWE-based KRM framework, valid for anydimensional lattices and any modulus q without a dither. Our main result isan explicit upper bound on the decryption failure rate (DFR) of M-LWE-basedKRM. This bound allows us to construct optimal lattice quantizers to reduce theDFR and communication cost simultaneously. Moreover, we present a KRM schemeusing the same security parameters (q,k,\u03b71,\u03b72) as in Kyber. Comparedwith Kyber, the communication cost is reduced by up to 36.47% and the DFR isreduced by a factor of up to 299. The security arguments remain the sameas Kyber."
    },
    {
        "link": "https://arxiv.org/abs/2401.15535",
        "title": "Quantifying Stereotypes in Language",
        "authors": [
            "Yang Liu"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "A stereotype is a generalized perception of a specific group of humans. It isoften potentially encoded in human language, which is more common in texts onsocial issues. Previous works simply define a sentence as stereotypical andanti-stereotypical. However, the stereotype of a sentence may requirefine-grained quantification. In this paper, to fill this gap, we quantifystereotypes in language by annotating a dataset. We use the pre-trainedlanguage models (PLMs) to learn this dataset to predict stereotypes ofsentences. Then, we discuss stereotypes about common social issues such as hatespeech, sexism, sentiments, and disadvantaged and advantaged groups. Wedemonstrate the connections and differences between stereotypes and commonsocial issues, and all four studies validate the general findings of thecurrent studies. In addition, our work suggests that fine-grained stereotypescores are a highly relevant and competitive dimension for research on socialissues."
    },
    {
        "link": "https://arxiv.org/abs/2401.15541",
        "title": "Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO Satellite Learning",
        "authors": [
            "Mohamed Elmahallawy",
            "Tie Luo"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "In the ambitious realm of space AI, the integration of federated learning(FL) with low Earth orbit (LEO) satellite constellations holds immense promise.However, many challenges persist in terms of feasibility, learning efficiency,and convergence. These hurdles stem from the bottleneck in communication,characterized by sporadic and irregular connectivity between LEO satellites andground stations, coupled with the limited computation capability of satelliteedge computing (SEC). This paper proposes a novel FL-SEC framework thatempowers LEO satellites to execute large-scale machine learning (ML) tasksonboard efficiently. Its key components include i) personalized learning viadivide-and-conquer, which identifies and eliminates redundant satellite imagesand converts complex multi-class classification problems to simple binaryclassification, enabling rapid and energy-efficient training of lightweight MLmodels suitable for IoT/edge devices on satellites; ii) orbital modelretraining, which generates an aggregated \"orbital model\" per orbit andretrains it before sending to the ground station, significantly reducing therequired communication rounds. We conducted experiments using Jetson Nano, anedge device closely mimicking the limited compute on LEO satellites, and a realsatellite dataset. The results underscore the effectiveness of our approach,highlighting SEC's ability to run lightweight ML models on real andhigh-resolution satellite imagery. Our approach dramatically reduces FLconvergence time by nearly 30 times, and satellite energy consumption down toas low as 1.38 watts, all while maintaining an exceptional accuracy of up to96%."
    },
    {
        "link": "https://arxiv.org/abs/2401.15543",
        "title": "Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep Learning Technology",
        "authors": [
            "Zhiyuan Chen",
            "Wei Lu",
            "Radhika Bhong",
            "Yimin Hu",
            "Brian Freeman",
            "Adam Carpenter"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "A stable, reliable, and controllable orbit lock system is crucial to anelectron (or ion) accelerator because the beam orbit and beam energyinstability strongly affect the quality of the beam delivered to experimentalhalls. Currently, when the orbit lock system fails operators must manuallyintervene. This paper develops a Machine Learning based fault detectionmethodology to identify orbit lock anomalies and notify accelerator operationsstaff of the off-normal behavior. Our method is unsupervised, so it does notrequire labeled data. It uses Long-Short Memory Networks (LSTM) Auto Encoder tocapture normal patterns and predict future values of monitoring sensors in theorbit lock system. Anomalies are detected when the prediction error exceeds athreshold. We conducted experiments using monitoring data from Jefferson Lab'sContinuous Electron Beam Accelerator Facility (CEBAF). The results arepromising: the percentage of real anomalies identified by our solution is68.6%-89.3% using monitoring data of a single component in the orbit lockcontrol system. The accuracy can be as high as 82%."
    },
    {
        "link": "https://arxiv.org/abs/2401.15544",
        "title": "Analog and Multi-modal Manufacturing Datasets Acquired on the Future Factories Platform",
        "authors": [
            "Ramy Harik",
            "Fadi El Kalach",
            "Jad Samaha",
            "Devon Clark",
            "Drew Sander",
            "Philip Samaha",
            "Liam Burns",
            "Ibrahim Yousif",
            "Victor Gadow",
            "Theodros Tarekegne",
            "Nitol Saha"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Two industry-grade datasets are presented in this paper that were collectedat the Future Factories Lab at the University of South Carolina on December11th and 12th of 2023. These datasets are generated by a manufacturing assemblyline that utilizes industrial standards with respect to actuators, controlmechanisms, and transducers. The two datasets were both generatedsimultaneously by operating the assembly line for 30 consecutive hours (withminor filtering) and collecting data from sensors equipped throughout thesystem. During operation, defects were also introduced into the assemblyoperation by manually removing parts needed for the final assembly. Thedatasets generated include a time series analog dataset and the other is a timeseries multi-modal dataset which includes images of the system alongside theanalog data. These datasets were generated with the objective of providingtools to further the research towards enhancing intelligence in manufacturing.Real manufacturing datasets can be scarce let alone datasets with anomalies ordefects. As such these datasets hope to address this gap and provideresearchers with a foundation to build and train Artificial Intelligence modelsapplicable for the manufacturing industry. Finally, these datasets are thefirst iteration of published data from the future Factories lab and can befurther adjusted to fit more researchers needs moving forward."
    },
    {
        "link": "https://arxiv.org/abs/2401.15545",
        "title": "PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models",
        "authors": [
            "Simin Chen",
            "Xiaoning Feng",
            "Xiaohong Han",
            "Cong Liu",
            "Wei Yang"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "In recent times, a plethora of Large Code Generation Models (LCGMs) have beenproposed, showcasing significant potential in assisting developers with complexprogramming tasks. Benchmarking LCGMs necessitates the creation of a set ofdiverse programming problems, and each problem comprises the prompt (includingthe task description), canonical solution, and test inputs. The existingmethods for constructing such a problem set can be categorized into two maintypes: manual methods and perturbation-based methods. However, manual methodsdemand high effort and lack scalability, while also risking data integrity dueto LCGMs' potentially contaminated data collection, and perturbation-basedapproaches mainly generate semantically homogeneous problems with the samecanonical solutions and introduce typos that can be easily auto-corrected byIDE, making them ineffective and unrealistic. In this work, we propose the ideaof programming problem merging (PPM) and provide two implementation of thisidea, we utilize our tool on two widely-used datasets and compare it againstnine baseline methods using eight code generation models. The resultsdemonstrate the effectiveness of our tool in generating more challenging,diverse, and natural programming problems, comparing to the baselines."
    },
    {
        "link": "https://arxiv.org/abs/2401.15550",
        "title": "Dynamic Maximal Matching in Clique Networks",
        "authors": [
            "Minming Li",
            "Peter Robinson",
            "Xianbin Zhu"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "We consider the problem of computing a maximal matching with a distributedalgorithm in the presence of batch-dynamic changes to the graph topology. Weassume that a graph of n nodes is vertex-partitioned among k players thatcommunicate via message passing. Our goal is to provide an efficient algorithmthat quickly updates the matching even if an adversary determines batches of\u2113 edge insertions or deletions.Assuming a link bandwidth of O(\u03b2logn) bits per round, for a parameter\u03b2\u22651, we first show a lower bound of \u03a9(\u2113logk\u03b2k2logn) rounds for recomputing a matching assuming an obliviousadversary who is unaware of the initial (random) vertex partition as well asthe current state of the players, and a stronger lower bound of\u03a9(\u2113\u03b2klogn) rounds against an adaptive adversary, whomay choose any balanced (but not necessarily random) vertex partition initiallyand who knows the current state of the players.We also present a randomized algorithm that has an initialization time of O(\u2308n\u03b2k\u2309logn) rounds, while achieving an update timethat that is independent of n: In more detail, the update time is O(\u2308\u2113\u03b2k\u2309log(\u03b2k)) against an oblivious adversary,who must fix all updates in advance. If we consider the stronger adaptiveadversary, the update time becomes O(\u2308\u2113\u03b2k\u221a\u2309log(\u03b2k)) rounds."
    },
    {
        "link": "https://arxiv.org/abs/2401.15554",
        "title": "Pericoronary adipose tissue feature analysis in CT calcium score images with comparison to coronary CTA",
        "authors": [
            "Yingnan Song",
            "Hao Wu",
            "Juhwan Lee",
            "Justin Kim",
            "Ammar Hoori",
            "Tao Hu",
            "Vladislav Zimin",
            "Mohamed Makhlouf",
            "Sadeer Al-Kindi",
            "Sanjay Rajagopalan",
            "Chun-Ho Yun",
            "Chung-Lieh Hung",
            "David L. Wilson"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We investigated the feasibility and advantages of using non-contrast CTcalcium score (CTCS) images to assess pericoronary adipose tissue (PCAT) andits association with major adverse cardiovascular events (MACE). PCAT featuresfrom coronary CTA (CCTA) have been shown to be associated with cardiovascularrisk but are potentially confounded by iodine. If PCAT in CTCS images can besimilarly analyzed, it would avoid this issue and enable its inclusion informal risk assessment from readily available, low-cost CTCS images. Toidentify coronaries in CTCS images that have subtle visual evidence of vessels,we registered CTCS with paired CCTA images having coronary labels. We developeda novel axial-disk method giving regions for analyzing PCAT features in threemain coronary arteries. We analyzed novel hand-crafted and radiomic featuresusing univariate and multivariate logistic regression prediction of MACE andcompared results against those from CCTA. Registration accuracy was sufficientto enable the identification of PCAT regions in CTCS images. Motion or beamhardening artifacts were often present in high-contrast CCTA but not CTCS. MeanHU and volume were increased in both CTCS and CCTA for MACE group. There weresignificant positive correlations between some CTCS and CCTA features,suggesting that similar characteristics were obtained. Usinghand-crafted/radiomics from CTCS and CCTA, AUCs were 0.82/0.79 and 0.83/0.77respectively, while Agatston gave AUC=0.73. Preliminarily, PCAT features can beassessed from three main coronary arteries in non-contrast CTCS images withperformance characteristics that are at the very least comparable to CCTA."
    },
    {
        "link": "https://arxiv.org/abs/2401.15555",
        "title": "Augment before You Try: Knowledge-Enhanced Table Question Answering via Table Expansion",
        "authors": [
            "Yujian Liu",
            "Jiabao Ji",
            "Tong Yu",
            "Ryan Rossi",
            "Sungchul Kim",
            "Handong Zhao",
            "Ritwik Sinha",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Table question answering is a popular task that assesses a model's ability tounderstand and interact with structured data. However, the given table oftendoes not contain sufficient information for answering the question,necessitating the integration of external knowledge. Existing methods eitherconvert both the table and external knowledge into text, which neglects thestructured nature of the table; or they embed queries for external sources inthe interaction with the table, which complicates the process. In this paper,we propose a simple yet effective method to integrate external information in agiven table. Our method first constructs an augmenting table containing themissing information and then generates a SQL query over the two tables toanswer the question. Experiments show that our method outperforms strongbaselines on three table QA benchmarks. Our code is publicly available athttps://github.com/UCSB-NLP-Chang/Augment_tableQA."
    },
    {
        "link": "https://arxiv.org/abs/2401.15557",
        "title": "Vectorized implementation of primal hybrid FEM in MATLAB",
        "authors": [
            "Harish Nagula Mallesham",
            "Kamana Porwal",
            "Jan Valdman",
            "Sanjib Kumar Acharya"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We present efficient MATLAB implementations of the lowest-order primal hybridfinite element method (FEM) for linear second-order elliptic and parabolicproblems with mixed boundary conditions in two spatial dimensions. We employthe Crank-Nicolson finite difference scheme for the complete discrete setup ofthe parabolic problem. All the codes presented are fully vectorized usingmatrix-wise array operations. Numerical experiments are conducted to show theperformance of the software."
    },
    {
        "link": "https://arxiv.org/abs/2401.15558",
        "title": "numaPTE: Managing Page-Tables and TLBs on NUMA Systems",
        "authors": [
            "Bin Gao",
            "Qingxuan Kang",
            "Hao-Wei Tee",
            "Kyle Timothy Ng Chu",
            "Alireza Sanaee",
            "Djordje Jevdjic"
        ],
        "primary_subject": "Operating Systems (cs.OS)",
        "abstract": "Memory management operations that modify page-tables, typically performedduring memory allocation/deallocation, are infamous for their poor performancein highly threaded applications, largely due to process-wide TLB shootdownsthat the OS must issue due to the lack of hardware support for TLB coherence.We study these operations in NUMA settings, where we observe up to 40x overheadfor basic operations such as munmap or mprotect. The overhead further increasesif page-table replication is used, where complete coherent copies of thepage-tables are maintained across all NUMA nodes. While eager system-widereplication is extremely effective at localizing page-table reads duringaddress translation, we find that it creates additional penalties upon anypage-table changes due to the need to maintain all replicas coherent.In this paper, we propose a novel page-table management mechanism, callednumaPTE, to enable transparent, on-demand, and partial page-table replicationacross NUMA nodes in order to perform address translation locally, whileavoiding the overheads and scalability issues of system-wide full page-tablereplication. We then show that numaPTE's precise knowledge of page-tablesharers can be leveraged to significantly reduce the number of TLB shootdownsissued upon any memory-management operation. As a result, numaPTE not onlyavoids replication-related slowdowns, but also provides significant speedupover the baseline on memory allocation/deallocation and access controloperations. We implement numaPTEin Linux on x86_64, evaluate it on 4- and8-socket systems, and show that numaPTE achieves the full benefits of eagerpage-table replication on a wide range of applications, while also achieving a12% and 36% runtime improvement on Webserver and Memcached respectively due toa significant reduction in TLB shootdowns."
    },
    {
        "link": "https://arxiv.org/abs/2401.15559",
        "title": "IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models",
        "authors": [
            "Xingchen Zeng",
            "Ziyao Gao",
            "Yilin Ye",
            "Wei Zeng"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Fine-tuning facilitates the adaptation of text-to-image generative models tonovel concepts (e.g., styles and portraits), empowering users to forgecreatively customized content. Recent efforts on fine-tuning focus on reducingtraining data and lightening computation overload but neglect alignment withuser intentions, particularly in manual curation of multi-modal training dataand intent-oriented evaluation. Informed by a formative study with fine-tuningpractitioners for comprehending user intentions, we propose IntentTuner, aninteractive framework that intelligently incorporates human intentionsthroughout each phase of the fine-tuning workflow. IntentTuner enables users toarticulate training intentions with imagery exemplars and textual descriptions,automatically converting them into effective data augmentation strategies.Furthermore, IntentTuner introduces novel metrics to measure user intentalignment, allowing intent-aware monitoring and evaluation of model training.Application exemplars and user studies demonstrate that IntentTuner streamlinesfine-tuning, reducing cognitive effort and yielding superior models compared tothe common baseline tool."
    },
    {
        "link": "https://arxiv.org/abs/2401.15560",
        "title": "An Analysis of Letter Dynamics in the English Alphabet",
        "authors": [
            "Neil Zhao",
            "Diana Zheng"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "The frequency with which the letters of the English alphabet appear inwritings has been applied to the field of cryptography, the development ofkeyboard mechanics, and the study of linguistics. We expanded on thestatistical analysis of the English alphabet by examining the average frequencywhich each letter appears in different categories of writings. We evaluatednews articles, novels, plays, scientific publications and calculated thefrequency of each letter of the alphabet, the information density of eachletter, and the overall letter distribution. Furthermore, we developed a metricknown as distance, d that can be used to algorithmically recognize differentcategories of writings. The results of our study can be applied to informationtransmission, large data curation, and linguistics."
    },
    {
        "link": "https://arxiv.org/abs/2401.15561",
        "title": "A Parameter Privacy-Preserving Strategy for Mixed-Autonomy Platoon Control",
        "authors": [
            "Jingyuan Zhou",
            "Kaidi Yang"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "It has been demonstrated that leading cruise control (LCC) can improve theoperation of mixed-autonomy platoons by allowing connected and automatedvehicles (CAVs) to make longitudinal control decisions based on the informationprovided by surrounding vehicles. However, LCC generally requires surroundinghuman-driven vehicles (HDVs) to share their real-time states, which can be usedby adversaries to infer drivers' car-following behavior, potentially leading tofinancial losses or safety concerns. This paper aims to address such privacyconcerns and protect the behavioral characteristics of HDVs by devising aparameter privacy-preserving approach for mixed-autonomy platoon control.First, we integrate a parameter privacy filter into LCC to protect sensitivecar-following parameters. The privacy filter allows each vehicle to generateseemingly realistic pseudo states by distorting the true parameters to pseudoparameters, which can protect drivers' privacy in behavioral parameters withoutsignificantly influencing the control performance. Second, to enhance thepracticality and reliability of the privacy filter within LCC, we first extendthe current approach to accommodate continuous parameter spaces through aneural network estimator. Subsequently, we introduce an individual-levelparameter privacy preservation constraint, focusing on the privacy level ofeach individual parameter pair, further enhancing the approach's reliability.Third, analysis of head-to-tail string stability reveals the potential impactof privacy filters in degrading mixed traffic flow performance. Simulationshows that this approach can effectively trade off privacy and controlperformance in LCC. We further demonstrate the benefit of such an approach innetworked systems, i.e., by applying the privacy filter to a proceedingvehicle, one can also achieve a certain level of privacy for the followingvehicle."
    },
    {
        "link": "https://arxiv.org/abs/2401.15563",
        "title": "BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry",
        "authors": [
            "Xiang Xu",
            "Joseph G. Lambourne",
            "Pradeep Kumar Jayaraman",
            "Zhengqing Wang",
            "Karl D.D. Willis",
            "Yasutaka Furukawa"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents BrepGen, a diffusion-based generative approach thatdirectly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)model. BrepGen represents a B-rep model as a novel structured latent geometryin a hierarchical tree. With the root node representing a whole CAD solid, eachelement of a B-rep model (i.e., a face, an edge, or a vertex) progressivelyturns into a child-node from top to bottom. B-rep geometry information goesinto the nodes as the global bounding box of each primitive along with a latentcode describing the local geometric shape. The B-rep topology information isimplicitly represented by node duplication. When two faces share an edge, theedge curve will appear twice in the tree, and a T-junction vertex with threeincident edges appears six times in the tree with identical node features.Starting from the root and progressing to the leaf, BrepGen employsTransformer-based diffusion models to sequentially denoise node features whileduplicated nodes are detected and merged, recovering the B-Rep topologyinformation. Extensive experiments show that BrepGen sets a new milestone inCAD B-rep generation, surpassing existing methods on various benchmarks.Results on our newly collected furniture dataset further showcase itsexceptional capability in generating complicated geometry. While previousmethods were limited to generating simple prismatic shapes, BrepGenincorporates free-form and doubly-curved surfaces for the first time.Additional applications of BrepGen include CAD autocomplete and designinterpolation. The code, pretrained models, and dataset will be released."
    },
    {
        "link": "https://arxiv.org/abs/2401.15564",
        "title": "Design of UAV flight state recognition and trajectory prediction system based on trajectory feature construction",
        "authors": [
            "Xingyu Zhou",
            "Zhuoyong Shi"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "With the impact of artificial intelligence on the traditional UAV industry,autonomous UAV flight has become a current hot research field. Based on thedemand for research on critical technologies for autonomous flying UAVs, thispaper addresses the field of flight state recognition and trajectory predictionof UAVs. This paper proposes a method to improve the accuracy of UAV trajectoryprediction based on UAV flight state recognition and verifies it using twoprediction models. Firstly, UAV flight data acquisition and data preprocessingare carried out; secondly, UAV flight trajectory features are extracted basedon data fusion and a UAV flight state recognition model based on PCA-DAGSVMmodel is established; finally, two UAV flight trajectory prediction models areestablished and the trajectory prediction errors of the two prediction modelsare compared and analyzed after flight state recognition. The results showthat: 1) the UAV flight state recognition model based on PCA-DAGSVM has goodrecognition effect. 2) compared with the traditional UAV trajectory predictionmodel, the prediction model based on flight state recognition can effectivelyreduce the prediction error."
    },
    {
        "link": "https://arxiv.org/abs/2401.15568",
        "title": "Intriguing Equivalence Structures of the Embedding Space of Vision Transformers",
        "authors": [
            "Shaeke Salman",
            "Md Montasir Bin Shams",
            "Xiuwen Liu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pre-trained large foundation models play a central role in the recent surgeof artificial intelligence, resulting in fine-tuned models with remarkableabilities when measured on benchmark datasets, standard exams, andapplications. Due to their inherent complexity, these models are not wellunderstood. While small adversarial inputs to such models are well known, thestructures of the representation space are not well characterized despite theirfundamental importance. In this paper, using the vision transformers as anexample due to the continuous nature of their input space, we show via analysesand systematic experiments that the representation space consists of largepiecewise linear subspaces where there exist very different inputs sharing thesame representations, and at the same time, local normal spaces where there arevisually indistinguishable inputs having very different representations. Theempirical results are further verified using the local directional estimationsof the Lipschitz constants of the underlying models. Consequently, theresulting representations change the results of downstream models, and suchmodels are subject to overgeneralization and with limited semanticallymeaningful generalization capability."
    },
    {
        "link": "https://arxiv.org/abs/2401.15569",
        "title": "Efficient Tuning and Inference for Large Language Models on Textual Graphs",
        "authors": [
            "Yun Zhu",
            "Yaoke Wang",
            "Haizhou Shi",
            "Siliang Tang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Rich textual and topological information of textual graphs need to be modeledin real-world applications such as webpages, e-commerce, and academic articles.Practitioners have been long following the path of adopting a shallow textencoder and a subsequent graph neural network (GNN) to solve this problem. Inlight of recent advancements in large language models (LLMs), it is apparentthat integrating LLMs for enhanced textual encoding can substantially improvethe performance of textual graphs. Nevertheless, the efficiency of thesemethods poses a significant challenge. In this paper, we propose ENGINE, aparameter- and memory-efficient fine-tuning method for textual graphs with anLLM encoder. The key insight is to combine the LLMs and GNNs through a tunableside structure, which significantly reduces the training complexity withoutimpairing the joint model's capacity. Extensive experiments on textual graphsdemonstrate our method's effectiveness by achieving the best model performance,meanwhile having the lowest training cost compared to previous methods.Moreover, we introduce two variants with caching and dynamic early exit tofurther enhance training and inference speed. Specifically, caching acceleratesENGINE's training by 12x, and dynamic early exit achieves up to 5x fasterinference with a negligible performance drop (at maximum 1.17% relevant dropacross 7 datasets)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15573",
        "title": "A Novel PML-type Technique for Acoustic Scattering Problems based on A Real Coordinate Transformation",
        "authors": [
            "Jiangxing Wang",
            "Lilian Wang",
            "Bo Wang"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "It is known that any {\\em real coordinate transformation} (RCT) to compresswaves in an unbounded domain into a bounded domain results in infiniteoscillations that cannot be resolved by any grid-based method. In this paper,we intend to show that it is viable if the outgoing waves are compressed alongthe radial direction and the resulting oscillatory pattern is extractedexplicitly. We therefore construct a perfectly matched layer (PML)-typetechnique for domain reduction of wave scattering problems using RCT, termed asreal compressed layer (RCL). Different from all existing approaches, the RCLtechnique has two features: (i) the RCL-equation only involves real-valuedcoefficients, which is more desirable for computation and analysis; and (ii)the layer is not ``artificial'' in the sense that the computed field in thelayer can recover the outgoing wave of the original scattering problem in theunbounded domain. Here we demonstrate the essential idea and performance of theRCL for the two-dimensional Helmholtz problem with a bounded scatterer, butthis technique can be extended to three dimensions in a similar setting."
    },
    {
        "link": "https://arxiv.org/abs/2401.15578",
        "title": "ARCNet: An Asymmetric Residual Wavelet Column Correction Network for Infrared Image Destriping",
        "authors": [
            "Shuai Yuan",
            "Hanlin Qin",
            "Xiang Yan",
            "Naveed Akhtar",
            "Shiqi Yang",
            "Shuowen Yang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Infrared image destriping seeks to restore high-quality content from degradedimages. Recent works mainly address this task by leveraging prior knowledge toseparate stripe noise from the degraded image. However, constructing a robustdecoupling model for that purpose remains challenging, especially whensignificant similarities exist between the stripe noise and vertical backgroundstructure. Addressing that, we introduce Asymmetric Residual wavelet Columncorrection Network (ARCNet) for image destriping, aiming to consistentlypreserve spatially precise high-resolution representations. Our neural modelleverages a novel downsampler, residual haar discrete wavelet transform(RHDWT), stripe directional prior knowledge and data-driven learning to inducea model with enriched feature representation of stripe noise and background. Inour technique, the inverse wavelet transform is replaced by transposedconvolution for feature upsampling, which can suppress noise crosstalk andencourage the network to focus on robust image reconstruction. After eachsampling, a proposed column non-uniformity correction module (CNCM) isleveraged by our method to enhance column uniformity, spatial correlation, andglobal self-dependence between each layer component. CNCM can establishstructural characteristics of stripe noise and utilize contextual informationat long-range dependencies to distinguish stripes with varying intensities anddistributions. Extensive experiments on synthetic data, real data, and infraredsmall target detection tasks show that the proposed method outperformsstate-of-the-art single-image destriping methods both visually andquantitatively by a considerable margin. Our code will be made publiclyavailable at \\url{https://github.com/xdFai}."
    },
    {
        "link": "https://arxiv.org/abs/2401.15579",
        "title": "MunTTS: A Text-to-Speech System for Mundari",
        "authors": [
            "Varun Gumma",
            "Rishav Hada",
            "Aditya Yadavalli",
            "Pamir Gogoi",
            "Ishani Mondal",
            "Vivek Seshadri",
            "Kalika Bali"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically forMundari, a low-resource Indian language of the Austo-Asiatic family. Our workaddresses the gap in linguistic technology for underrepresented languages bycollecting and processing data to build a speech synthesis system. We begin ourstudy by gathering a substantial dataset of Mundari text and speech and trainend-to-end speech models. We also delve into the methods used for training ourmodels, ensuring they are efficient and effective despite the data constraints.We evaluate our system with native speakers and objective metrics,demonstrating its potential as a tool for preserving and promoting the Mundarilanguage in the digital age."
    },
    {
        "link": "https://arxiv.org/abs/2401.15582",
        "title": "A two-grid Adaptive Finite Element Method for the Dirichlet Boundary Control Problem Governed by Stokes Equation",
        "authors": [
            "Thirupathi Gudi",
            "Ramesh Chandra Sau"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this article, we derive \\textit{a posteriori} error estimates for theDirichlet boundary control problem governed by Stokes equation. An energy-basedmethod has been deployed to solve the Dirichlet boundary control problem. Weemploy an inf-sup stable finite element discretization scheme by usingP1 elements(in the fine mesh) for the velocity and control variableand P0 elements(in the coarse mesh) for the pressure variable. We derive an\\textit{a posteriori} error estimator for the state, adjoint state, and controlerror. The control error estimator generalizes the standard residual typeestimator of the unconstrained Dirichlet boundary control problems, byadditional terms at the contact boundary addressing the non-linearity. We provethe reliability and efficiency of the estimator. Theoretical results areillustrated by some numerical experiments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15583",
        "title": "SCTransNet: Spatial-channel Cross Transformer Network for Infrared Small Target Detection",
        "authors": [
            "Shuai Yuan",
            "Hanlin Qin",
            "Xiang Yan",
            "Naveed AKhtar",
            "Ajmal Mian"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Infrared small target detection (IRSTD) has recently benefitted greatly fromU-shaped neural models. However, largely overlooking effective globalinformation modeling, existing techniques struggle when the target has highsimilarities with the background. We present a Spatial-channel CrossTransformer Network (SCTransNet) that leverages spatial-channel crosstransformer blocks (SCTBs) on top of long-range skip connections to address theaforementioned challenge. In the proposed SCTBs, the outputs of all encodersare interacted with cross transformer to generate mixed features, which areredistributed to all decoders to effectively reinforce semantic differencesbetween the target and clutter at full scales. Specifically, SCTB contains thefollowing two key elements: (a) spatial-embedded single-head channel-crossattention (SSCA) for exchanging local spatial features and full-level globalchannel information to eliminate ambiguity among the encoders and facilitatehigh-level semantic associations of the images, and (b) a complementaryfeed-forward network (CFN) for enhancing the feature discriminability via amulti-scale strategy and cross-spatial-channel information interaction topromote beneficial information transfer. Our SCTransNet effectively encodes thesemantic differences between targets and backgrounds to boost its internalrepresentation for detecting small infrared targets accurately. Extensiveexperiments on three public datasets, NUDT-SIRST, NUAA-SIRST, and IRSTD-1k,demonstrate that the proposed SCTransNet outperforms existing IRSTD methods.Our code will be made public at https://github.com/xdFai."
    },
    {
        "link": "https://arxiv.org/abs/2401.15584",
        "title": "DGNN: Decoupled Graph Neural Networks with Structural Consistency between Attribute and Graph Embedding Representations",
        "authors": [
            "Jinlu Wang",
            "Jipeng Guo",
            "Yanfeng Sun",
            "Junbin Gao",
            "Shaofan Wang",
            "Yachao Yang",
            "Baocai Yin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Graph neural networks (GNNs) demonstrate a robust capability forrepresentation learning on graphs with complex structures, showcasing superiorperformance in various applications. The majority of existing GNNs employ agraph convolution operation by using both attribute and structure informationthrough coupled learning. In essence, GNNs, from an optimization perspective,seek to learn a consensus and compromise embedding representation that balancesattribute and graph information, selectively exploring and retaining validinformation. To obtain a more comprehensive embedding representation of nodes,a novel GNNs framework, dubbed Decoupled Graph Neural Networks (DGNN), isintroduced. DGNN explores distinctive embedding representations from theattribute and graph spaces by decoupled terms. Considering that semantic graph,constructed from attribute feature space, consists of different node connectioninformation and provides enhancement for the topological graph, bothtopological and semantic graphs are combined for the embedding representationlearning. Further, structural consistency among attribute embedding and graphembeddings is promoted to effectively remove redundant information andestablish soft connection. This involves promoting factor sharing for adjacencyreconstruction matrices, facilitating the exploration of a consensus andhigh-level correlation. Finally, a more powerful and complete representation isachieved through the concatenation of these embeddings. Experimental resultsconducted on several graph benchmark datasets verify its superiority in nodeclassification task."
    },
    {
        "link": "https://arxiv.org/abs/2401.15585",
        "title": "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting",
        "authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Naoaki Okazaki",
            "Timothy Baldwin"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "There exist both scalable tasks, like reading comprehension andfact-checking, where model performance improves with model size, and unscalabletasks, like arithmetic reasoning and symbolic reasoning, where modelperformance does not necessarily improve with model size. Large language models(LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurateincremental predictions even on unscalable tasks. Unfortunately, despite theirexceptional reasoning abilities, LLMs tend to internalize and reproducediscriminatory societal biases. Whether CoT can provide discriminatory oregalitarian rationalizations for the implicit information in unscalable tasksremains an open question.In this study, we examine the impact of LLMs' step-by-step predictions ongender bias in unscalable tasks. For this purpose, we construct a benchmark foran unscalable task where the LLM is given a list of words comprising feminine,masculine, and gendered occupational words, and is required to count the numberof feminine and masculine words. In our CoT prompts, we require the LLM toexplicitly indicate whether each word in the word list is a feminine ormasculine before making the final predictions. With counting and handling themeaning of words, this benchmark has characteristics of both arithmeticreasoning and symbolic reasoning. Experimental results in English show thatwithout step-by-step prediction, most LLMs make socially biased predictions,despite the task being as simple as counting words. Interestingly, CoTprompting reduces this unconscious social bias in LLMs and encourages fairpredictions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15587",
        "title": "Hyperedge Interaction-aware Hypergraph Neural Network",
        "authors": [
            "Xiaobing Pei",
            "Rongping Ye",
            "Haoran Yang",
            "Ruiqi Wang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Hypergraphs provide an effective modeling approach for modeling high-orderrelationships in many real-world datasets. To capture such complexrelationships, several hypergraph neural networks have been proposed forlearning hypergraph structure, which propagate information from nodes tohyperedges and then from hyperedges back to nodes. However, most existingmethods focus on information propagation between hyperedges and nodes,neglecting the interactions among hyperedges themselves. In this paper, wepropose HeIHNN, a hyperedge interaction-aware hypergraph neural network, whichcaptures the interactions among hyperedges during the convolution process andintroduce a novel mechanism to enhance information flow between hyperedges andnodes. Specifically, HeIHNN integrates the interactions between hyperedges intothe hypergraph convolution by constructing a three-stage informationpropagation process. After propagating information from nodes to hyperedges, weintroduce a hyperedge-level convolution to update the hyperedge embeddings.Finally, the embeddings that capture rich information from the interactionamong hyperedges will be utilized to update the node embeddings. Additionally,we introduce a hyperedge outlier removal mechanism in the informationpropagation stages between nodes and hyperedges, which dynamically adjusts thehypergraph structure using the learned embeddings, effectively removingoutliers. Extensive experiments conducted on real-world datasets show thecompetitive performance of HeIHNN compared with state-of-the-art methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15589",
        "title": "OpineBot: Class Feedback Reimagined Using a Conversational LLM",
        "authors": [
            "Henansh Tanwar",
            "Kunal Shrivastva",
            "Rahul Singh",
            "Dhruv Kumar"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Conventional class feedback systems often fall short, relying on static,unengaging surveys offering little incentive for student participation. Toaddress this, we present OpineBot, a novel system employing large languagemodels (LLMs) to conduct personalized, conversational class feedback viachatbot interface. We assessed OpineBot's effectiveness in a user study with 20students from an Indian university's Operating-Systems class, utilizing surveysand interviews to analyze their experiences. Findings revealed a resoundingpreference for OpineBot compared to conventional methods, highlighting itsability to engage students, produce deeper feedback, offering a dynamic surveyexperience. This research represents a work in progress, providing earlyresults, marking a significant step towards revolutionizing class feedbackthrough LLM-based technology, promoting student engagement, and leading toricher data for instructors. This ongoing research presents preliminaryfindings and marks a notable advancement in transforming classroom feedbackusing LLM-based technology to enhance student engagement and generatecomprehensive data for educators."
    },
    {
        "link": "https://arxiv.org/abs/2401.15595",
        "title": "Comuniqa : Exploring Large Language Models for improving speaking skills",
        "authors": [
            "Manas Mhasakar",
            "Shikhar Sharma",
            "Apurv Mehra",
            "Utkarsh Venaik",
            "Ujjwal Singhal",
            "Dhruv Kumar",
            "Kashish Mittal"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "This research paper explores the potential of Large Language Models (LLMs) toenhance speaking skills. We first present a novel LLM-based system, Comuniqa,for this task. We then take a humancentric approach to evaluate this system,comparing it with human experts. We also investigate the possibility ofcombining feedback from both LLM and human experts to enhance overall learningoutcomes. We use purposive and random sampling for recruiting participants,categorizing them into three groups: those who use LLM-enabled apps forimproving speaking skills, those guided by human experts for the same task andthose who utilize both the LLM-enabled apps as well as the human experts. Usingsurveys, interviews, and actual study sessions, we provide a detailedperspective on the effectiveness of different learning modalities. Ourpreliminary findings suggest that while LLM-based systems have commendableaccuracy, they lack human-level cognitive capabilities, both in terms ofaccuracy and empathy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15600",
        "title": "A Mechatronic System for the Visualisation and Analysis of Orchestral Conducting",
        "authors": [
            "Courtney Coates",
            "Liao Wu"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This paper quantitatively analysed orchestral conducting patterns, anddetected variations as a result of extraneous body movement during conducting,in the first experiment of its kind. A novel live conducting system featuringdata capture, processing, and analysis was developed. Reliable data of anexpert conductor's movements was collected, processed, and used to calculateaverage trajectories for different conducting techniques with variousextraneous body movements; variations between extraneous body movementtechniques and controlled technique were definitively determined in a novelquantitative analysis. A portable and affordable mechatronic system was createdto capture and process live baton tip data, and was found to be accuratethrough calibration against a reliable reference. Experimental conducting fielddata was captured through the mechatronic system, and analysed againstpreviously calculated average trajectories; the extraneous movement used duringthe field data capture was successfully identified by the system."
    },
    {
        "link": "https://arxiv.org/abs/2401.15603",
        "title": "Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction",
        "authors": [
            "Kangkang Lu",
            "Yanhua Yu",
            "Hao Fei",
            "Xuan Li",
            "Zixuan Yang",
            "Zirui Guo",
            "Meiyu Liang",
            "Mengran Yin",
            "Tat-Seng Chua"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In recent years, spectral graph neural networks, characterized by polynomialfilters, have garnered increasing attention and have achieved remarkableperformance in tasks such as node classification. These models typically assumethat eigenvalues for the normalized Laplacian matrix are distinct from eachother, thus expecting a polynomial filter to have a high fitting ability.However, this paper empirically observes that normalized Laplacian matricesfrequently possess repeated eigenvalues. Moreover, we theoretically establishthat the number of distinguishable eigenvalues plays a pivotal role indetermining the expressive power of spectral graph neural networks. In light ofthis observation, we propose an eigenvalue correction strategy that can freepolynomial filters from the constraints of repeated eigenvalue inputs.Concretely, the proposed eigenvalue correction strategy enhances the uniformdistribution of eigenvalues, thus mitigating repeated eigenvalues, andimproving the fitting capacity and expressive power of polynomial filters.Extensive experimental results on both synthetic and real-world datasetsdemonstrate the superiority of our method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15604",
        "title": "Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization",
        "authors": [
            "Yinbin Han",
            "Meisam Razaviyayn",
            "Renyuan Xu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Diffusion models have emerged as a powerful tool rivaling GANs in generatinghigh-quality samples with improved fidelity, flexibility, and robustness. A keycomponent of these models is to learn the score function through scorematching. Despite empirical success on various tasks, it remains unclearwhether gradient-based algorithms can learn the score function with a provableaccuracy. As a first step toward answering this question, this paperestablishes a mathematical framework for analyzing score estimation usingneural networks trained by gradient descent. Our analysis covers both theoptimization and the generalization aspects of the learning procedure. Inparticular, we propose a parametric form to formulate the denoisingscore-matching problem as a regression with noisy labels. Compared to thestandard supervised learning setup, the score-matching problem introducesdistinct challenges, including unbounded input, vector-valued output, and anadditional time variable, preventing existing techniques from being applieddirectly. In this paper, we show that with a properly designed neural networkarchitecture, the score function can be accurately approximated by areproducing kernel Hilbert space induced by neural tangent kernels.Furthermore, by applying an early-stopping rule for gradient descent andleveraging certain coupling arguments between neural network training andkernel regression, we establish the first generalization error (samplecomplexity) bounds for learning the score function despite the presence ofnoise in the observations. Our analysis is grounded in a novel parametric formof the neural network and an innovative connection between score matching andregression analysis, facilitating the application of advanced statistical andoptimization techniques."
    },
    {
        "link": "https://arxiv.org/abs/2401.15605",
        "title": "AI as a Medical Ally: Evaluating ChatGPT's Usage and Impact in Indian Healthcare",
        "authors": [
            "Aryaman Raina",
            "Prateek Mishra",
            "Harshit goyal",
            "Dhruv Kumar"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "This study investigates the integration and impact of Large Language Models(LLMs), like ChatGPT, in India's healthcare sector. Our research employs a dualapproach, engaging both general users and medical professionals through surveysand interviews respectively. Our findings reveal that healthcare professionalsvalue ChatGPT in medical education and preliminary clinical settings, butexercise caution due to concerns about reliability, privacy, and the need forcross-verification with medical references. General users show a preference forAI interactions in healthcare, but concerns regarding accuracy and trustpersist. The study underscores the need for these technologies to complement,not replace, human medical expertise, highlighting the importance of developingLLMs in collaboration with healthcare providers. This paper enhances theunderstanding of LLMs in healthcare, detailing current usage, user trust, andimprovement areas. Our insights inform future research and development,underscoring the need for ethically compliant, user-focused LLM advancementsthat address healthcare-specific challenges."
    },
    {
        "link": "https://arxiv.org/abs/2401.15607",
        "title": "Survey of Distributed Algorithms for Resource Allocation over Multi-Agent Systems",
        "authors": [
            "Mohammadreza Doostmohammadian",
            "Alireza Aghasi",
            "Mohammad Pirani",
            "Ehsan Nekouei",
            "Houman Zarrabi",
            "Reza Keypour",
            "Apostolos I. Rikos",
            "Karl H. Johansson"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Resource allocation and scheduling in multi-agent systems present challengesdue to complex interactions and decentralization. This survey paper provides acomprehensive analysis of distributed algorithms for addressing the distributedresource allocation (DRA) problem over multi-agent systems. It covers asignificant area of research at the intersection of optimization, multi-agentsystems, and distributed consensus-based computing. The paper begins bypresenting a mathematical formulation of the DRA problem, establishing a solidfoundation for further exploration. Real-world applications of DRA in variousdomains are examined to underscore the importance of efficient resourceallocation, and relevant distributed optimization formulations are presented.The survey then delves into existing solutions for DRA, encompassing linear,nonlinear, primal-based, and dual-formulation-based approaches. Furthermore,this paper evaluates the features and properties of DRA algorithms, addressingkey aspects such as feasibility, convergence rate, and network reliability. Theanalysis of mathematical foundations, diverse applications, existing solutions,and algorithmic properties contributes to a broader comprehension of thechallenges and potential solutions for this domain."
    },
    {
        "link": "https://arxiv.org/abs/2401.15608",
        "title": "Analysis for a class of stochastic fractional nonlinear Schr\u00f6dinger equations",
        "authors": [
            "Yanjie Zhang",
            "Ao Zhang",
            "Pengde Wang",
            "Xiao Wang",
            "Jinqiao Duan"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We investigate the global existence of a solution for the stochasticfractional nonlinear Schr\\\"odinger equation with radially symmetric initialdata in a suitable energy space H\u03b1. Using a variational principle, wedemonstrate that the stochastic fractional nonlinear Schr\\\"odinger equation inthe Stratonovich sense forms an infinite-dimensional stochastic Hamiltoniansystem, with its phase flow preserving symplecticity. We develop astructure-preserving algorithm for the stochastic fractional nonlinearSchr\\\"odinger equation from the perspective of symplectic geometry. It isestablished that the stochastic midpoint scheme satisfies the correspondingsymplectic law in the discrete sense. Furthermore, since the midpoint scheme isimplicit, we also develop a more effective mass-preserving splitting scheme.Consequently, the convergence order of the splitting scheme is shown to be 1.Two numerical examples are conducted to validate the efficiency of the theory."
    },
    {
        "link": "https://arxiv.org/abs/2401.15610",
        "title": "Prevalidated ridge regression is a highly-efficient drop-in replacement for logistic regression for high-dimensional data",
        "authors": [
            "Angus Dempster",
            "Geoffrey I. Webb",
            "Daniel F. Schmidt"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Logistic regression is a ubiquitous method for probabilistic classification.However, the effectiveness of logistic regression depends upon careful andrelatively computationally expensive tuning, especially for the regularisationhyperparameter, and especially in the context of high-dimensional data. Wepresent a prevalidated ridge regression model that closely matches logisticregression in terms of classification error and log-loss, particularly forhigh-dimensional data, while being significantly more computationally efficientand having effectively no hyperparameters beyond regularisation. We scale thecoefficients of the model so as to minimise log-loss for a set of prevalidatedpredictions derived from the estimated leave-one-out cross-validation error.This exploits quantities already computed in the course of fitting the ridgeregression model in order to find the scaling parameter with nominal additionalcomputational expense."
    },
    {
        "link": "https://arxiv.org/abs/2401.15615",
        "title": "Addressing Noise and Efficiency Issues in Graph-Based Machine Learning Models From the Perspective of Adversarial Attack",
        "authors": [
            "Yongyu Wang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Given that no existing graph construction method can generate a perfect graphfor a given dataset, graph-based algorithms are invariably affected by theplethora of redundant and erroneous edges present within the constructedgraphs. In this paper, we propose treating these noisy edges as adversarialattack and use a spectral adversarial robustness evaluation method to diminishthe impact of noisy edges on the performance of graph algorithms. Our methodidentifies those points that are less vulnerable to noisy edges and leveragesonly these robust points to perform graph-based algorithms. Our experimentswith spectral clustering, one of the most representative and widely utilizedgraph algorithms, reveal that our methodology not only substantially elevatesthe precision of the algorithm but also greatly accelerates its computationalefficiency by leveraging only a select number of robust data points."
    },
    {
        "link": "https://arxiv.org/abs/2401.15616",
        "title": "Multi-Person 3D Pose Estimation from Multi-View Uncalibrated Depth Cameras",
        "authors": [
            "Yu-Jhe Li",
            "Yan Xu",
            "Rawal Khirodkar",
            "Jinhyung Park",
            "Kris Kitani"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We tackle the task of multi-view, multi-person 3D human pose estimation froma limited number of uncalibrated depth cameras. Recently, many approaches havebeen proposed for 3D human pose estimation from multi-view RGB cameras.However, these works (1) assume the number of RGB camera views is large enoughfor 3D reconstruction, (2) the cameras are calibrated, and (3) rely on groundtruth 3D poses for training their regression model. In this work, we propose toleverage sparse, uncalibrated depth cameras providing RGBD video streams for 3Dhuman pose estimation. We present a simple pipeline for Multi-View Depth HumanPose Estimation (MVD-HPE) for jointly predicting the camera poses and 3D humanposes without training a deep 3D human pose regression model. This frameworkutilizes 3D Re-ID appearance features from RGBD images to formulate moreaccurate correspondences (for deriving camera positions) compared to usingRGB-only features. We further propose (1) depth-guided camera-pose estimationby leveraging 3D rigid transformations as guidance and (2) depth-constrained 3Dhuman pose estimation by utilizing depth-projected 3D points as an alternativeobjective for optimization. In order to evaluate our proposed pipeline, wecollect three video sets of RGBD videos recorded from multiple sparse-viewdepth cameras and ground truth 3D poses are manually annotated. Experimentsshow that our proposed method outperforms the current 3D human poseregression-free pipelines in terms of both camera pose estimation and 3D humanpose estimation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15617",
        "title": "Diffusion-based graph generative methods",
        "authors": [
            "Hongyang Chen",
            "Can Xu",
            "Lingyu Zheng",
            "Qiang Zhang",
            "Xuemin Lin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Being the most cutting-edge generative methods, diffusion methods have showngreat advances in wide generation tasks. Among them, graph generation attractssignificant research attention for its broad application in real life. In oursurvey, we systematically and comprehensively review on diffusion-based graphgenerative methods. We first make a review on three mainstream paradigms ofdiffusion methods, which are denoising diffusion probabilistic models,score-based genrative models, and stochastic differential equations. Then wefurther categorize and introduce the latest applications of diffusion models ongraphs. In the end, we point out some limitations of current studies and futuredirections of future explorations. The summary of existing methods metioned inthis survey is inhttps://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15620",
        "title": "Data-Driven Strategies for Coping with Incomplete DVL Measurements",
        "authors": [
            "Nadav Cohen",
            "Itzik Klein"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Autonomous underwater vehicles are specialized platforms engineered for deepunderwater operations. Critical to their functionality is autonomousnavigation, typically relying on an inertial navigation system and a Dopplervelocity log. In real-world scenarios, incomplete Doppler velocity logmeasurements occur, resulting in positioning errors and mission aborts. To copewith such situations, a model and learning approaches were derived. This paperpresents a comparative analysis of two cutting-edge deep learningmethodologies, namely LiBeamsNet and MissBeamNet, alongside a model-basedaverage estimator. These approaches are evaluated for their efficacy inregressing missing Doppler velocity log beams when two beams are unavailable.In our study, we used data recorded by a DVL mounted on an autonomousunderwater vehicle operated in the Mediterranean Sea. We found that both deeplearning architectures outperformed model-based approaches by over 16% invelocity prediction accuracy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15621",
        "title": "SNAP: Semantic Stories for Next Activity Prediction",
        "authors": [
            "Alon Oved",
            "Segev Shlomov",
            "Sergey Zeltyn",
            "Nir Mashkif",
            "Avi Yaeli"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Predicting the next activity in an ongoing process is one of the most commonclassification tasks in the business process management (BPM) domain. It allowsbusinesses to optimize resource allocation, enhance operational efficiency, andaids in risk mitigation and strategic decision-making. This provides acompetitive edge in the rapidly evolving confluence of BPM and AI. Existingstate-of-the-art AI models for business process prediction do not fullycapitalize on available semantic information within process event logs. Ascurrent advanced AI-BPM systems provide semantically-richer textual data, theneed for novel adequate models grows. To address this gap, we propose the novelSNAP method that leverages language foundation models by constructing semanticcontextual stories from the process historical event logs and using them forthe next activity prediction. We compared the SNAP algorithm with ninestate-of-the-art models on six benchmark datasets and show that SNAPsignificantly outperforms them, especially for datasets with high levels ofsemantic content."
    },
    {
        "link": "https://arxiv.org/abs/2401.15625",
        "title": "Generative AI-enabled Blockchain Networks: Fundamentals, Applications, and Case Study",
        "authors": [
            "Cong T. Nguyen",
            "Yinqiu Liu",
            "Hongyang Du",
            "Dinh Thai Hoang",
            "Dusit Niyato",
            "Diep N. Nguyen",
            "Shiwen Mao"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Generative Artificial Intelligence (GAI) has recently emerged as a promisingsolution to address critical challenges of blockchain technology, includingscalability, security, privacy, and interoperability. In this paper, we firstintroduce GAI techniques, outline their applications, and discuss existingsolutions for integrating GAI into blockchains. Then, we discuss emergingsolutions that demonstrate the effectiveness of GAI in addressing variouschallenges of blockchain, such as detecting unknown blockchain attacks andsmart contract vulnerabilities, designing key secret sharing schemes, andenhancing privacy. Moreover, we present a case study to demonstrate that GAI,specifically the generative diffusion model, can be employed to optimizeblockchain network performance metrics. Experimental results clearly show that,compared to a baseline traditional AI approach, the proposed generativediffusion model approach can converge faster, achieve higher rewards, andsignificantly improve the throughput and latency of the blockchain network.Additionally, we highlight future research directions for GAI in blockchainapplications, including personalized GAI-enabled blockchains, GAI-blockchainsynergy, and privacy and security considerations within blockchain ecosystems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15626",
        "title": "TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks and Action-Tree Based Scheduled Sampling",
        "authors": [
            "Longxiang Liu",
            "Xiuxing Li",
            "Yang Feng"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Task-oriented dialog systems have witnessed substantial progress due toconversational pre-training techniques. Yet, two significant challengespersist. First, most systems primarily utilize the latest turn's state labelfor the generator. This practice overlooks the comprehensive value of statelabels in boosting the model's understanding for future generations. Second, anoverreliance on generated policy often leads to error accumulation, resultingin suboptimal responses when adhering to incorrect actions. To combat thesechallenges, we propose turn-level multi-task objectives for the encoder. Withthe guidance of essential information from labeled intermediate states, weestablish a more robust representation for both understanding and generation.For the decoder, we introduce an action tree-based scheduled samplingtechnique. Specifically, we model the hierarchical policy as trees and utilizethe similarity between trees to sample negative policy based on scheduledsampling, hoping the model to generate invariant responses under perturbations.This method simulates potential pitfalls by sampling similar negative policy,bridging the gap between task-oriented dialog training and inference. Amongmethods without continual pre-training, our approach achieved state-of-the-art(SOTA) performance on the MultiWOZ dataset series and was also competitive withpre-trained SOTA methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15628",
        "title": "WetSpongeCake: a Surface Appearance Model Considering Porosity and Saturation",
        "authors": [
            "Gaole Pan",
            "Yuang Cui",
            "Jian Yang",
            "Beibei Wang"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "Wet powdered materials, such as wet ground or moist walls, are common in thereal world. Despite their particle size being larger than the wavelength, theyremain invisible from a macro view. Reproducing these appearances accurately iscrucial for various applications. Existing methods use different approaches,such as Monte Carlo path tracing on implicit shapes, which is accurate butcomputationally expensive. Another approach involves modeling powderedmaterials with a medium using the radiative transfer equation, but thesemethods are computationally intensive and lack intuitive parameters. Some worksrepresent porosity with cylinder-shaped holes on surfaces, but they havelimitations. In this paper, we propose a practical BSDF model calledWetSpongeCake for wet powdered materials. This model includes controllablephysical parameters to faithfully reproduce real-world appearances whileremaining computationally efficient. We reformulate Monte Carlo light transporton implicit shapes into a medium, utilizing an equivalent phase function forlight transport within ellipsoid-shaped particles and a modified RTE forporosity and saturation effects. Our novel WetSpongeCake BSDF integrates thismedium into the SpongeCake framework, allowing representation of various wetpowdered material appearances, including both reflection and transmission. Weshowcase our model with examples, such as wet paper, sand saturated withdifferent liquids, and sculptures made of multiple particles."
    },
    {
        "link": "https://arxiv.org/abs/2401.15635",
        "title": "RecDCL: Dual Contrastive Learning for Recommendation",
        "authors": [
            "Dan Zhang",
            "Yangliao Geng",
            "Wenwen Gong",
            "Zhongang Qi",
            "Zhiyu Chen",
            "Xing Tang",
            "Ying Shan",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "Self-supervised recommendation (SSR) has achieved great success in mining thepotential interacted behaviors for collaborative filtering in recent years. Asa major branch, Contrastive Learning (CL) based SSR conquers data sparsity inWeb platforms by contrasting the embedding between raw data and augmented data.However, existing CL-based SSR methods mostly focus on contrasting in abatch-wise way, failing to exploit potential regularity in the feature-wisedimension, leading to redundant solutions during the representation learningprocess of users (items) from Websites. Furthermore, the joint benefits ofutilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) forrecommendations remain underexplored. To address these issues, we investigatethe relationship of objectives between BCL and FCL. Our study suggests acooperative benefit of employing both methods, as evidenced from theoreticaland experimental perspectives. Based on these insights, we propose a dual CLmethod for recommendation, referred to as RecDCL. RecDCL first eliminatesredundant solutions on user-item positive pairs in a feature-wise manner. Itthen optimizes the uniform distributions within users and items using apolynomial kernel from an FCL perspective. Finally, it generates contrastiveembedding on output vectors in a batch-wise objective. We conduct experimentson four widely-used benchmarks and an industrial dataset. The resultsconsistently demonstrate that the proposed RecDCL outperforms thestate-of-the-art GNNs-based and SSL-based models (with up to a 5.65\\%improvement in terms of Recall@20), thereby confirming the effectiveness of thejoint-wise objective. All source codes used in this paper are publiclyavailable at \\url{https://github.com/THUDM/RecDCL}}."
    },
    {
        "link": "https://arxiv.org/abs/2401.15636",
        "title": "FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion Models",
        "authors": [
            "Feihong He",
            "Gang Li",
            "Mengyuan Zhang",
            "Leilei Yan",
            "Lingyu Si",
            "Fanzhang Li"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid development of generative diffusion models has significantlyadvanced the field of style transfer. However, most current style transfermethods based on diffusion models typically involve a slow iterativeoptimization process, e.g., model fine-tuning and textual inversion of styleconcept. In this paper, we introduce FreeStyle, an innovative style transfermethod built upon a pre-trained large diffusion model, requiring no furtheroptimization. Besides, our method enables style transfer only through a textdescription of the desired style, eliminating the necessity of style images.Specifically, we propose a dual-stream encoder and single-stream decoderarchitecture, replacing the conventional U-Net in diffusion models. In thedual-stream encoder, two distinct branches take the content image and styletext prompt as inputs, achieving content and style decoupling. In the decoder,we further modulate features from the dual streams based on a given contentimage and the corresponding style text prompt for precise style transfer. Ourexperimental results demonstrate high-quality synthesis and fidelity of ourmethod across various content images and style text prompts. The code and moreresults are available at our projectwebsite:https://freestylefreelunch.github.io/."
    },
    {
        "link": "https://arxiv.org/abs/2401.15638",
        "title": "Cyto R-CNN and CytoNuke Dataset: Towards reliable whole-cell segmentation in bright-field histological images",
        "authors": [
            "Johannes Raufeisen",
            "Kunpeng Xie",
            "Fabian H\u00f6rst",
            "Till Braunschweig",
            "Jianning Li",
            "Jens Kleesiek",
            "Rainer R\u00f6hrig",
            "Jan Egger",
            "Bastian Leibe",
            "Frank H\u00f6lzle",
            "Alexander Hermans",
            "Behrus Puladi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Background: Cell segmentation in bright-field histological slides is acrucial topic in medical image analysis. Having access to accurate segmentationallows researchers to examine the relationship between cellular morphology andclinical observations. Unfortunately, most segmentation methods known today arelimited to nuclei and cannot segmentate the cytoplasm.Material & Methods: We present a new network architecture Cyto R-CNN that isable to accurately segment whole cells (with both the nucleus and thecytoplasm) in bright-field images. We also present a new dataset CytoNuke,consisting of multiple thousand manual annotations of head and neck squamouscell carcinoma cells. Utilizing this dataset, we compared the performance ofCyto R-CNN to other popular cell segmentation algorithms, including QuPath'sbuilt-in algorithm, StarDist and Cellpose. To evaluate segmentationperformance, we calculated AP50, AP75 and measured 17 morphological andstaining-related features for all detected cells. We compared thesemeasurements to the gold standard of manual segmentation using theKolmogorov-Smirnov test.Results: Cyto R-CNN achieved an AP50 of 58.65\\% and an AP75 of 11.56\\% inwhole-cell segmentation, outperforming all other methods (QuPath19.46/0.91%; StarDist 45.33/2.32%; Cellpose 31.85/5.61%). Cellfeatures derived from Cyto R-CNN showed the best agreement to the gold standard(D\u00af=0.15) outperforming QuPath (D\u00af=0.22), StarDist (D\u00af=0.25) and Cellpose (D\u00af=0.23).Conclusion: Our newly proposed Cyto R-CNN architecture outperforms currentalgorithms in whole-cell segmentation while providing more reliable cellmeasurements than any other model. This could improve digital pathologyworkflows, potentially leading to improved diagnosis. Moreover, our publisheddataset can be used to develop further models in the future."
    },
    {
        "link": "https://arxiv.org/abs/2401.15639",
        "title": "TOP: Towards Open & Predictable Heterogeneous SoCs",
        "authors": [
            "Luca Valente",
            "Francesco Restuccia",
            "Davide Rossi",
            "Ryan Kastner",
            "Luca Benini"
        ],
        "primary_subject": "Hardware Architecture (cs.AR)",
        "abstract": "Ensuring predictability in modern real-time Systems-on-Chip (SoCs) is anincreasingly critical concern for many application domains such as automotive,robotics, and industrial automation. An effective approach involves themodeling and development of hardware components, such as interconnects andshared memory resources, to evaluate or enforce their deterministic behavior.Unfortunately, these IPs are often closed-source, and these studies are limitedto the single modules that must later be integrated with third-party IPs inmore complex SoCs, hindering the precision and scope of modeling andcompromising the overall predictability. With the coming-of-age of open-sourceinstruction set architectures (RISC-V) and hardware, major opportunities forchanging this status quo are emerging. This study introduces an innovativemethodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCsfor low-power cyber-physical systems. Our approach models and analyzes theentire set of open-source IPs within these SoCs and then provides acomprehensive analysis of the entire architecture. We validate this methodologyon a sample heterogenous low-power RISC-V architecture through RTL simulationand FPGA implementation, minimizing pessimism in bounding the service time oftransactions crossing the architecture between 28% and 1%, which isconsiderably lower when compared to similar SoA works."
    },
    {
        "link": "https://arxiv.org/abs/2401.15641",
        "title": "PRE: A Peer Review Based Large Language Model Evaluator",
        "authors": [
            "Zhumin Chu",
            "Qingyao Ai",
            "Yiteng Tu",
            "Haitao Li",
            "Yiqun Liu"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "The impressive performance of large language models (LLMs) has attractedconsiderable attention from the academic and industrial communities. Besideshow to construct and train LLMs, how to effectively evaluate and compare thecapacity of LLMs has also been well recognized as an important yet difficultproblem. Existing paradigms rely on either human annotators or model-basedevaluators to evaluate the performance of LLMs on different tasks. However,these paradigms often suffer from high cost, low generalizability, andinherited biases in practice, which make them incapable of supporting thesustainable development of LLMs in long term. In order to address these issues,inspired by the peer review systems widely used in academic publicationprocess, we propose a novel framework that can automatically evaluate LLMsthrough a peer-review process. Specifically, for the evaluation of a specifictask, we first construct a small qualification exam to select \"reviewers\" froma couple of powerful LLMs. Then, to actually evaluate the \"submissions\" writtenby different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs torate or compare the submissions. The final ranking of evaluatee LLMs isgenerated based on the results provided by all reviewers. We conductedextensive experiments on text summarization tasks with eleven LLMs includingGPT-4. The results demonstrate the existence of biasness when evaluating usinga single LLM. Also, our PRE model outperforms all the baselines, illustratingthe effectiveness of the peer review mechanism."
    },
    {
        "link": "https://arxiv.org/abs/2401.15646",
        "title": "Improving Data Augmentation for Robust Visual Question Answering with Effective Curriculum Learning",
        "authors": [
            "Yuhang Zheng",
            "Zhen Wang",
            "Long Chen"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Being widely used in learning unbiased visual question answering (VQA)models, Data Augmentation (DA) helps mitigate language biases by generatingextra training samples beyond the original samples. While today's DA methodscan generate robust samples, the augmented training set, significantly largerthan the original dataset, often exhibits redundancy in terms of difficulty orcontent repetition, leading to inefficient model training and even compromisingthe model performance. To this end, we design an Effective Curriculum Learningstrategy ECL to enhance DA-based VQA methods. Intuitively, ECL trains VQAmodels on relatively ``easy'' samples first, and then gradually changes to``harder'' samples, and less-valuable samples are dynamically removed. Comparedto training on the entire augmented dataset, our ECL strategy can furtherenhance VQA models' performance with fewer training samples. Extensiveablations have demonstrated the effectiveness of ECL on various methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15647",
        "title": "UP-CrackNet: Unsupervised Pixel-Wise Road Crack Detection via Adversarial Image Restoration",
        "authors": [
            "Nachuan Ma",
            "Rui Fan",
            "Lihua Xie"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Over the past decade, automated methods have been developed to detect cracksmore efficiently, accurately, and objectively, with the ultimate goal ofreplacing conventional manual visual inspection techniques. Among thesemethods, semantic segmentation algorithms have demonstrated promising resultsin pixel-wise crack detection tasks. However, training such data-drivenalgorithms requires a large amount of human-annotated datasets with pixel-levelannotations, which is a highly labor-intensive and time-consuming process.Moreover, supervised learning-based methods often struggle with poorgeneralization ability in unseen datasets. Therefore, we propose anunsupervised pixel-wise road crack detection network, known as UP-CrackNet. Ourapproach first generates multi-scale square masks and randomly selects them tocorrupt undamaged road images by removing certain regions. Subsequently, agenerative adversarial network is trained to restore the corrupted regions byleveraging the semantic context learned from surrounding uncorrupted regions.During the testing phase, an error map is generated by calculating thedifference between the input and restored images, which allows for pixel-wisecrack detection. Our comprehensive experimental results demonstrate thatUP-CrackNet outperforms other general-purpose unsupervised anomaly detectionalgorithms, and exhibits comparable performance and superior generalizabilitywhen compared with state-of-the-art supervised crack segmentation algorithms.Our source code is publicly available at mias.group/UP-CrackNet."
    },
    {
        "link": "https://arxiv.org/abs/2401.15648",
        "title": "A computational approach to identify the material parameters of the relaxed micromorphic model",
        "authors": [
            "Mohammad Sarhil",
            "Lisa Scheunemann",
            "Peter Lewintan",
            "J\u00f6rg Schr\u00f6der",
            "Patrizio Neff"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We determine the material parameters in the relaxed micromorphic generalizedcontinuum model for a given periodic microstructure in this work. This isachieved through a least squares fitting of the total energy of the relaxedmicromorphic homogeneous continuum to the total energy of the fully-resolvedheterogeneous microstructure, governed by classical linear elasticity. Therelaxed micromorphic model is a generalized continuum that utilizes the $\\Curl$of a micro-distortion field instead of its full gradient as in the classicalmicromorphic theory, leading to several advantages and differences. The mostcrucial advantage is that it operates between two well-defined scales. Thesescales are determined by linear elasticity with microscopic and macroscopicelasticity tensors, which respectively bound the stiffness of the relaxedmicromorphic continuum from above and below. While the macroscopic elasticitytensor is established a priori through standard periodic first-orderhomogenization, the microscopic elasticity tensor remains to be determined.Additionally, the characteristic length parameter, associated with curvaturemeasurement, controls the transition between the micro- and macro-scales. Boththe microscopic elasticity tensor and the characteristic length parameter arehere determined using a computational approach based on the least squaresfitting of energies. This process involves the consideration of an adequatenumber of quadratic deformation modes and different specimen sizes. We conducta comparative analysis between the least square fitting results of the relaxedmicromorphic model, the fitting of a skew-symmetric micro-distortion field(Cosserat-micropolar model), and the fitting of the classical micromorphicmodel with two different formulations for the curvature..."
    },
    {
        "link": "https://arxiv.org/abs/2401.15649",
        "title": "CPDM: Content-Preserving Diffusion Model for Underwater Image Enhancement",
        "authors": [
            "Xiaowen Shi",
            "Yuan-Gen Wang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Underwater image enhancement (UIE) is challenging since image degradation inaquatic environments is complicated and changing over time. Existing mainstreammethods rely on either physical-model or data-driven, suffering fromperformance bottlenecks due to changes in imaging conditions or traininginstability. In this article, we make the first attempt to adapt the diffusionmodel to the UIE task and propose a Content-Preserving Diffusion Model (CPDM)to address the above challenges. CPDM first leverages a diffusion model as itsfundamental model for stable training and then designs a content-preservingframework to deal with changes in imaging conditions. Specifically, weconstruct a conditional input module by adopting both the raw image and thedifference between the raw and noisy images as the input, which can enhance themodel's adaptability by considering the changes involving the raw images inunderwater environments. To preserve the essential content of the raw images,we construct a content compensation module for content-aware training byextracting low-level features from the raw images. Extensive experimentalresults validate the effectiveness of our CPDM, surpassing the state-of-the-artmethods in terms of both subjective and objective metrics."
    },
    {
        "link": "https://arxiv.org/abs/2401.15652",
        "title": "Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach",
        "authors": [
            "Shaofeng Zhang",
            "Jinfa Huang",
            "Qiang Zhou",
            "Zhibin Wang",
            "Fan Wang",
            "Jiebo Luo",
            "Junchi Yan"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Image outpainting aims to generate the content of an input sub-image beyondits original boundaries. It is an important task in content generation yetremains an open problem for generative models. This paper pushes the technicalfrontier of image outpainting in two directions that have not been resolved inliterature: 1) outpainting with arbitrary and continuous multiples (withoutrestriction), and 2) outpainting in a single step (even for large expansionmultiples). Moreover, we develop a method that does not depend on a pre-trainedbackbone network, which is in contrast commonly required by the previous SOTAoutpainting methods. The arbitrary multiple outpainting is achieved byutilizing randomly cropped views from the same image during training to capturearbitrary relative positional information. Specifically, by feeding one viewand positional embeddings as queries, we can reconstruct another view. Atinference, we generate images with arbitrary expansion multiples by inputtingan anchor image and its corresponding positional embeddings. The one-stepoutpainting ability here is particularly noteworthy in contrast to previousmethods that need to be performed for N times to obtain a final multiplewhich is N times of its basic and fixed multiple. We evaluate the proposedapproach (called PQDiff as we adopt a diffusion-based generator as ourembodiment, under our proposed \\textbf{P}ositional \\textbf{Q}uery scheme) onpublic benchmarks, demonstrating its superior performance over state-of-the-artapproaches. Specifically, PQDiff achieves state-of-the-art FID scores on theScenery (\\textbf{21.512}), Building Facades (\\textbf{25.310}), and WikiArts(\\textbf{36.212}) datasets. Furthermore, under the 2.25x, 5x and 11.7xoutpainting settings, PQDiff only takes \\textbf{40.6\\%}, \\textbf{20.3\\%} and\\textbf{10.2\\%} of the time of the benchmark state-of-the-art (SOTA) method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15656",
        "title": "LLsM: Generative Linguistic Steganography with Large Language Model",
        "authors": [
            "Yihao Wang",
            "Ruiqi Song",
            "Ru Zhang",
            "Jianyi Liu",
            "Lingxiao Li"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Linguistic Steganography (LS) tasks aim to generate steganographic texts(stego) based on secret information. Only authorized recipients can perceivethe existence of secret information in the texts and accurately extract it,thereby preserving privacy. However, the controllability of the stego generatedby existing schemes is poor, and the generated stego is difficult to containspecific discourse characteristics such as style, genre, and theme. As aresult, the stego are often easily detectable, compromising covertcommunication. To address these problems, this paper proposes a novel schemenamed LLsM, a generative LS based on a Large Language Model (LLM). Wefine-tuned the LLM LLaMA2 with a large-scale constructed dataset encompassingrich discourse characteristics, which enables the fine-tuned LLM to generatetexts with specific discourse in a controllable manner. Then the discoursecharacteristics are used as guiding information and inputted into thefine-tuned LLM in the form of Prompt together with secret information. Thecandidate pool, derived from sampling and truncation, undergoes range encodingto ensure the stego imitate natural text distribution. Experiments demonstratethat LLsM performs superior to prevalent baselines regarding text quality,statistical analysis, discourse matching, and anti-steganalysis. In particular,LLsM's MAUVE surpasses that of some baselines by 70%-80%, and itsanti-steganalysis performance is 30%-40% higher. Notably, we also present thelong stego generated by LLsM, showing its potential superiority in long LStasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15657",
        "title": "Data-Free Generalized Zero-Shot Learning",
        "authors": [
            "Bowen Tang",
            "Long Yan",
            "Jing Zhang",
            "Qian Yu",
            "Lu Sheng",
            "Dong Xu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning models have the ability to extract rich knowledge fromlarge-scale datasets. However, the sharing of data has become increasinglychallenging due to concerns regarding data copyright and privacy. Consequently,this hampers the effective transfer of knowledge from existing data to noveldownstream tasks and concepts. Zero-shot learning (ZSL) approaches aim torecognize new classes by transferring semantic knowledge learned from baseclasses. However, traditional generative ZSL methods often require access toreal images from base classes and rely on manually annotated attributes, whichpresents challenges in terms of data restrictions and model scalability. Tothis end, this paper tackles a challenging and practical problem dubbed asdata-free zero-shot learning (DFZSL), where only the CLIP-based base classesdata pre-trained classifier is available for zero-shot classification.Specifically, we propose a generic framework for DFZSL, which consists of threemain components. Firstly, to recover the virtual features of the base data, wemodel the CLIP features of base class images as samples from a von Mises-Fisher(vMF) distribution based on the pre-trained classifier. Secondly, we leveragethe text features of CLIP as low-cost semantic information and propose afeature-language prompt tuning (FLPT) method to further align the virtual imagefeatures and textual features. Thirdly, we train a conditional generative modelusing the well-aligned virtual image features and corresponding semantic textfeatures, enabling the generation of new classes features and achieve betterzero-shot generalization. Our framework has been evaluated on five commonlyused benchmarks for generalized ZSL, as well as 11 benchmarks for thebase-to-new ZSL. The results demonstrate the superiority and effectiveness ofour approach. Our code is available in https://github.com/ylong4/DFZSL"
    },
    {
        "link": "https://arxiv.org/abs/2401.15661",
        "title": "From Complexity to Simplicity: Brain-Inspired Modularization of PINN Solvers",
        "authors": [
            "Stefano Markidis"
        ],
        "primary_subject": "Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool forsolving partial differential equations (PDEs) in various scientific andengineering domains. However, traditional PINN architectures typically rely onfully connected multilayer perceptrons (MLPs), lacking the sparsity andmodularity inherent in many traditional numerical solvers. This studyinvestigates a novel approach by merging established PINN methodologies withbrain-inspired neural network techniques to address this architecturallimitation. We leverage Brain-Inspired Modular Training (BIMT), leveragingconcepts such as locality, sparsity, and modularity inspired by theorganization of the brain. Through BIMT, we demonstrate the evolution of PINNarchitectures from fully connected structures to highly sparse and modularforms, resulting in reduced computational complexity and memory requirements.We showcase the efficacy of this approach by solving differential equationswith varying spectral components, revealing insights into the spectral biasphenomenon and its impact on neural network architecture. Moreover, we derivebasic PINN building blocks through BIMT training on simple problems akin toconvolutional and attention modules in deep neural networks, enabling theconstruction of modular PINN architectures. Our experiments show that thesemodular architectures offer improved accuracy compared to traditional fullyconnected MLP PINNs, showcasing their potential for enhancing PINN performancewhile reducing computational overhead. Overall, this study contributes toadvancing the understanding and development of efficient and effective neuralnetwork architectures for solving PDEs, bridging the gap between PINNs andtraditional numerical methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15662",
        "title": "Transit Functions and Clustering Systems",
        "authors": [
            "Manoj Changat",
            "Ameera Vaheeda Shanavas",
            "Peter F. Stadler"
        ],
        "primary_subject": "Discrete Mathematics (cs.DM)",
        "abstract": "Transit functions serve not only as abstractions of betweenness and convexitybut are also closely connected with clustering systems. Here, we investigatethe canonical transit functions of binary clustering systems inspired bypyramids, i.e., interval hypergraphs. We provide alternative characterizationsof weak hierarchies, and describe union-closed binary clustering systems as asubclass of pyramids and weakly pyramidal clustering systems as an interestinggeneralization."
    },
    {
        "link": "https://arxiv.org/abs/2401.15664",
        "title": "Learning Human-like Locomotion Based on Biological Actuation and Rewards",
        "authors": [
            "Minkwan Kim",
            "Yoonsang Lee"
        ],
        "primary_subject": "Graphics (cs.GR)",
        "abstract": "We propose a method of learning a policy for human-like locomotion via deepreinforcement learning based on a human anatomical model, muscle actuation, andbiologically inspired rewards, without any inherent control rules or referencemotions. Our main ideas involve providing a dense reward using metabolic energyconsumption at every step during the initial stages of learning and thentransitioning to a sparse reward as learning progresses, and adjusting theinitial posture of the human model to facilitate the exploration of locomotion.Additionally, we compared and analyzed differences in learning outcomes acrossvarious settings other than the proposed method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15666",
        "title": "Error-Correcting Codes for Combinatorial DNA Composite",
        "authors": [
            "Omer Sabary",
            "Inbal Preuss",
            "Ryan Gabrys",
            "Zohar Yakhini",
            "Leon Anavy",
            "Eitan Yaakobi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Data storage in DNA is developing as a possible solution for archival digitaldata. Recently, to further increase the potential capacity of DNA-based datastorage systems, the combinatorial composite DNA synthesis method wassuggested. This approach extends the DNA alphabet by harnessing short DNAfragment reagents, known as shortmers. The shortmers are building blocks of thealphabet symbols, consisting of a fixed number of shortmers. Thus, wheninformation is read, it is possible that one of the shortmers that forms partof the composition of a symbol is missing and therefore the symbol cannot bedetermined. In this paper, we model this type of error as a type of asymmetricerror and propose code constructions that can correct such errors in thissetup. We also provide a lower bound on the redundancy of such error-correctingcodes and give an explicit encoder and decoder pair for our construction. Oursuggested error model is also supported by an analysis of data from actualexperiments that produced DNA according to the combinatorial scheme. Lastly, wealso provide a statistical evaluation of the probability of observing sucherror events, as a function of read depth."
    },
    {
        "link": "https://arxiv.org/abs/2401.15668",
        "title": "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes",
        "authors": [
            "Weifeng Liu",
            "Tianyi She",
            "Jiawei Liu",
            "Run Wang",
            "Dongyu Yao",
            "Ziyou Liang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In recent years, DeepFake technology has achieved unprecedented success inhigh-quality video synthesis, whereas these methods also pose potential andsevere security threats to humanity. DeepFake can be bifurcated intoentertainment applications like face swapping and illicit uses such aslip-syncing fraud. However, lip-forgery videos, which neither change identitynor have discernible visual artifacts, present a formidable challenge toexisting DeepFake detection methods. Our preliminary experiments have shownthat the effectiveness of the existing methods often drastically decreases oreven fails when tackling lip-syncing videos.In this paper, for the first time, we propose a novel approach dedicated tolip-forgery identification that exploits the inconsistency between lipmovements and audio signals. We also mimic human natural cognition by capturingsubtle biological links between lips and head regions to boost accuracy. Tobetter illustrate the effectiveness and advances of our proposed method, wecurate a high-quality LipSync dataset by employing the SOTA lip generator. Wehope this high-quality and diverse dataset could be well served the furtherresearch on this challenging and interesting field. Experimental results showthat our approach gives an average accuracy of more than 95.3% in spottinglip-syncing videos, significantly outperforming the baselines. Extensiveexperiments demonstrate the capability to tackle deepfakes and the robustnessin surviving diverse input transformations. Our method achieves an accuracy ofup to 90.2% in real-world scenarios (e.g., WeChat video call) and shows itspowerful capabilities in real scenario deployment. To facilitate the progressof this research community, we release all resources athttps://github.com/AaronComo/LipFD."
    },
    {
        "link": "https://arxiv.org/abs/2401.15669",
        "title": "Programmable biomolecule-mediated processors",
        "authors": [
            "Jian-Jun Shu",
            "Zi Hian Tan",
            "Qi-Wen Wang",
            "Kian-Yan Yong"
        ],
        "primary_subject": "Emerging Technologies (cs.ET)",
        "abstract": "Programmable biomolecule-mediated computing is a new computing paradigm ascompared to contemporary electronic computing. It employs nucleic acids andanalogous biomolecular structures as information-storing and -processingsubstrates to tackle computational problems. It is of great significance toinvestigate the various issues of programmable biomolecule-mediated processorsthat are capable of automatically processing, storing, and displayinginformation. This Perspective provides several conceptual designs ofprogrammable biomolecule-mediated processors and provides some insights intopotential future research directions for programmable biomolecule-mediatedprocessors."
    },
    {
        "link": "https://arxiv.org/abs/2401.15670",
        "title": "YODA: Teacher-Student Progressive Learning for Language Models",
        "authors": [
            "Jianqiao Lu",
            "Wanjun Zhong",
            "Yufei Wang",
            "Zhijiang Guo",
            "Qi Zhu",
            "Wenyong Huang",
            "Yanlin Wang",
            "Fei Mi",
            "Baojun Wang",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Although large language models (LLMs) have demonstrated adeptness in a rangeof tasks, they still lag behind human learning efficiency. This disparity isoften linked to the inherent human capacity to learn from basic examples,gradually generalize and handle more complex problems, and refine their skillswith continuous feedback. Inspired by this, this paper introduces YODA, a novelteacher-student progressive learning framework that emulates theteacher-student education process to improve the efficacy of model fine-tuning.The framework operates on an interactive \\textit{basic-generalized-harder}loop. The teacher agent provides tailored feedback on the student's answers,and systematically organizes the education process. This process unfolds byteaching the student basic examples, reinforcing understanding throughgeneralized questions, and then enhancing learning by posing questions withprogressively enhanced complexity. With the teacher's guidance, the studentlearns to iteratively refine its answer with feedback, and forms a robust andcomprehensive understanding of the posed questions. The systematic proceduraldata, which reflects the progressive learning process of humans, is thenutilized for model training. Taking math reasoning as a testbed, experimentsshow that training LLaMA2 with data from YODA improves SFT with significantperformance gain (+17.01\\% on GSM8K and +9.98\\% on MATH). In addition, we findthat training with curriculum learning further improves learning robustness."
    },
    {
        "link": "https://arxiv.org/abs/2401.15672",
        "title": "Evaluating Echo State Network for Parkinson's Disease Prediction using Voice Features",
        "authors": [
            "Seyedeh Zahra Seyedi Hosseininian",
            "Ahmadreza Tajari",
            "Mohsen Ghalehnoie",
            "Alireza Alfi"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Parkinson's disease (PD) is a debilitating neurological disorder thatnecessitates precise and early diagnosis for effective patient care. This studyaims to develop a diagnostic model capable of achieving both high accuracy andminimizing false negatives, a critical factor in clinical practice. Given thelimited training data, a feature selection strategy utilizing ANOVA is employedto identify the most informative features. Subsequently, various machinelearning methods, including Echo State Networks (ESN), Random Forest, k-nearestNeighbors, Support Vector Classifier, Extreme Gradient Boosting, and DecisionTree, are employed and thoroughly evaluated. The statistical analyses of theresults highlight ESN's exceptional performance, showcasing not only superioraccuracy but also the lowest false negative rate among all methods.Consistently, statistical data indicates that the ESN method consistentlymaintains a false negative rate of less than 8% in 83% of cases. ESN's capacityto strike a delicate balance between diagnostic precision and minimizingmisclassifications positions it as an exemplary choice for PD diagnosis,especially in scenarios characterized by limited data. This research marks asignificant step towards more efficient and reliable PD diagnosis, withpotential implications for enhanced patient outcomes and healthcare dynamics."
    },
    {
        "link": "https://arxiv.org/abs/2401.15674",
        "title": "Cooperative Receding Horizon 3D Coverage Control with a Team of Networked Aerial Agents",
        "authors": [
            "Savvas Papaioannou",
            "Panayiotis Kolios",
            "Theocharis Theocharides",
            "Christos G. Panayiotou",
            "Marios M. Polycarpou"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This work proposes a receding horizon coverage control approach which allowsmultiple autonomous aerial agents to work cooperatively in order cover thetotal surface area of a 3D object of interest. The cooperative coverage problemwhich is posed in this work as an optimal control problem, jointly optimizesthe agents' kinematic and camera control inputs, while considering couplingconstraints amongst the team of agents which aim at minimizing the duplicationof work. To generate look-ahead coverage trajectories over a finite planninghorizon, the proposed approach integrates visibility constraints into theproposed coverage controller in order to determine the visible part of theobject with respect to the agents' future states. In particular, we show hownon-linear and non-convex visibility determination constraints can betransformed into logical constraints which can easily be embedded into a mixedinteger optimization program."
    },
    {
        "link": "https://arxiv.org/abs/2401.15675",
        "title": "Detection of a facemask in real-time using deep learning methods: Prevention of Covid 19",
        "authors": [
            "Gautam Siddharth Kashyap",
            "Jatin Sohlot",
            "Ayesha Siddiqui",
            "Ramsha Siddiqui",
            "Karan Malik",
            "Samar Wazir",
            "Alexander E. I. Brownlee"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "A health crisis is raging all over the world with the rapid transmission ofthe novel-coronavirus disease (Covid-19). Out of the guidelines issued by theWorld Health Organisation (WHO) to protect us against Covid-19, wearing afacemask is the most effective. Many countries have necessitated the wearing offace masks, but monitoring a large number of people to ensure that they arewearing masks in a crowded place is a challenging task in itself. Thenovel-coronavirus disease (Covid-19) has already affected our day-to-day lifeas well as world trade movements. By the end of April 2021, the world hasrecorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19)including 3,066,113 deaths according to the world health organization (WHO).These increasing numbers motivate automated techniques for the detection of afacemask in real-time scenarios for the prevention of Covid-19. We propose atechnique using deep learning that works for single and multiple people in aframe recorded via webcam in still or in motion. We have also experimented withour approach in night light. The accuracy of our model is good compared to theother approaches in the literature; ranging from 74% for multiple people in anightlight to 99% for a single person in daylight."
    },
    {
        "link": "https://arxiv.org/abs/2401.15677",
        "title": "A probabilistic analysis on general probabilistic scheduling problems",
        "authors": [
            "Daiki Suruga"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "The scheduling problem is a key class of optimization problems and hasvarious kinds of applications both in practical and theoretical scenarios. Inthe scheduling problem, probabilistic analysis is a basic tool forinvestigating performance of scheduling algorithms, and therefore has beencarried out by plenty amount of prior works. However, probabilistic analysishas several potential problems. For example, current research interest in thescheduling problem is limited to i.i.d. scenarios, due to its simplicity foranalysis. This paper provides a new framework for probabilistic analysis in thescheduling problem and aims to deal with such problems. As a consequence, weobtain several theorems including a theoretical limit of the scheduling problemwhich can be applied to \\emph{general, non-i.i.d. probability distributions}.Several information theoretic techniques, such as \\emph{information-spectrummethod}, turned out to be useful to prove our results. Since the schedulingproblem has relations to many other research fields, our framework hopefullyyields other interesting applications in the future."
    },
    {
        "link": "https://arxiv.org/abs/2401.15678",
        "title": "Recursive Subproduct Codes with Reed-Muller-like Structure",
        "authors": [
            "Aditya Siddheshwar",
            "Lakshmi Prasad Natarajan",
            "Prasad Krishnan"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We study a family of subcodes of the m-dimensional product codeC\u2297m ('subproduct codes') that have a recursivePlotkin-like structure, and which include Reed-Muller (RM) codes and DualBerman codes as special cases. We denote the codes in this family asC\u2297[r,m], where 0\u2264r\u2264m is the 'order' of thecode. These codes allow a 'projection' operation that can be exploited initerative decoding, viz., the sum of two carefully chosen subvectors of anycodeword in C\u2297[r,m] belongs to C\u2297[r\u22121,m\u22121]. Recursive subproduct codes provide a wide range of rates and blocklengths compared to RM codes while possessing several of their structuralproperties, such as the Plotkin-like design, the projection property, and fastML decoding of first-order codes. Our simulation results for first-order andsecond-order codes, that are based on a belief propagation decoder and a localgraph search algorithm, show instances of subproduct codes that perform eitherbetter than or within 0.5 dB of comparable RM codes and CRC-aided Polar codes."
    },
    {
        "link": "https://arxiv.org/abs/2401.15681",
        "title": "From Word Embedding to Reading Embedding Using Large Language Model, EEG and Eye-tracking",
        "authors": [
            "Yuhong Zhang",
            "Shilai Yang",
            "Gert Cauwenberghs",
            "Tzyy-Ping Jung"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Reading comprehension, a fundamental cognitive ability essential forknowledge acquisition, is a complex skill, with a notable number of learnerslacking proficiency in this domain. This study introduces innovative tasks forBrain-Computer Interface (BCI), predicting the relevance of words or tokensread by individuals to the target inference words. We use state-of-the-artLarge Language Models (LLMs) to guide a new reading embedding representation intraining. This representation, integrating EEG and eye-tracking biomarkersthrough an attention-based transformer encoder, achieved a mean 5-foldcross-validation accuracy of 68.7% across nine subjects using a balancedsample, with the highest single-subject accuracy reaching 71.2%. This studypioneers the integration of LLMs, EEG, and eye-tracking for predicting humanreading comprehension at the word level. We fine-tune the pre-trainedBidirectional Encoder Representations from Transformers (BERT) model for wordembedding, devoid of information about the reading tasks. Despite this absenceof task-specific details, the model effortlessly attains an accuracy of 92.7%,thereby validating our findings from LLMs. This work represents a preliminarystep toward developing tools to assist reading."
    },
    {
        "link": "https://arxiv.org/abs/2401.15683",
        "title": "On Small-depth Frege Proofs for PHP",
        "authors": [
            "Johan H\u00e5stad"
        ],
        "primary_subject": "Computational Complexity (cs.CC)",
        "abstract": "We study Frege proofs for the one-to-one graph Pigeon Hole Principle definedon the n\u00d7n grid where n is odd. We are interested in the case whereeach formula in the proof is a depth d formula in the basis given by \u2227,\u2228, and \u00ac. We prove that in this situation the proof needs to be ofsize exponential in n\u03a9(1/d). If we restrict the size of each line inthe proof to be of size M then the number of lines needed is exponential inn/(logM)O(d). The main technical component of the proofs is to design anew family of random restrictions and to prove the appropriate switchinglemmas."
    },
    {
        "link": "https://arxiv.org/abs/2401.15685",
        "title": "Assessment of Autism and ADHD: A Comparative Analysis of Drawing Velocity Profiles and the NEPSY Test",
        "authors": [
            "S. Fortea-Sevilla",
            "A. Garcia-Sosa.",
            "P. Morales-Almeida",
            "C. Carmona-Duarte"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The increasing prevalence of Autism Spectrum Disorder and Attention-Deficit/Hyperactivity Disorder among students highlights the need to improve evaluationand diagnostic techniques, as well as effective tools to mitigate the negativeconsequences associated with these disorders. With the widespread use oftouchscreen mobile devices, there is an opportunity to gather comprehensivedata beyond visual cues. These devices enable the collection and visualizationof information on velocity profiles and the time taken to complete drawing andhandwriting tasks. These data can be leveraged to develop newneuropsychological tests based on the velocity profile that assists indistinguishing between challenging cases of ASD and ADHD that are difficult todifferentiate in clinical practice. In this paper, we present a proof ofconcept that compares and combines the results obtained from standardized tasksin the NEPSY-II assessment with a proposed observational scale based on thevisual analysis of the velocity profile collected using digital tablets."
    },
    {
        "link": "https://arxiv.org/abs/2401.15687",
        "title": "Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance",
        "authors": [
            "Qingcheng Zhao",
            "Pengyu Long",
            "Qixuan Zhang",
            "Dafei Qin",
            "Han Liang",
            "Longwen Zhang",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The synthesis of 3D facial animations from speech has garnered considerableattention. Due to the scarcity of high-quality 4D facial data andwell-annotated abundant multi-modality labels, previous methods often sufferfrom limited realism and a lack of lexible conditioning. We address thischallenge through a trilogy. We first introduce Generalized Neural ParametricFacial Asset (GNPFA), an efficient variational auto-encoder mapping facialgeometry and images to a highly generalized expression latent space, decouplingexpressions and identities. Then, we utilize GNPFA to extract high-qualityexpressions and accurate head poses from a large array of videos. This presentsthe M2F-D dataset, a large, diverse, and scan-level co-speech 3D facialanimation dataset with well-annotated emotional and style labels. Finally, wepropose Media2Face, a diffusion model in GNPFA latent space for co-speechfacial animation generation, accepting rich multi-modality guidances fromaudio, text, and image. Extensive experiments demonstrate that our model notonly achieves high fidelity in facial animation synthesis but also broadens thescope of expressiveness and style adaptability in 3D facial animation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15688",
        "title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation",
        "authors": [
            "Zhenyu Wang",
            "Enze Xie",
            "Aoxue Li",
            "Zhongdao Wang",
            "Xihui Liu",
            "Zhenguo Li"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Despite significant advancements in text-to-image models for generatinghigh-quality images, these methods still struggle to ensure the controllabilityof text prompts over images in the context of complex text prompts, especiallywhen it comes to retaining object attributes and relationships. In this paper,we propose CompAgent, a training-free approach for compositional text-to-imagegeneration, with a large language model (LLM) agent as its core. Thefundamental idea underlying CompAgent is premised on a divide-and-conquermethodology. Given a complex text prompt containing multiple concepts includingobjects, attributes, and relationships, the LLM agent initially decomposes it,which entails the extraction of individual objects, their associatedattributes, and the prediction of a coherent scene layout. These individualobjects can then be independently conquered. Subsequently, the agent performsreasoning by analyzing the text, plans and employs the tools to compose theseisolated objects. The verification and human feedback mechanism is finallyincorporated into our agent to further correct the potential attribute errorsand refine the generated images. Guided by the LLM agent, we propose atuning-free multi-concept customization model and a layout-to-image generationmodel as the tools for concept composition, and a local image editing method asthe tool to interact with the agent for verification. The scene layout controlsthe image generation process among these tools to prevent confusion amongmultiple objects. Extensive experiments demonstrate the superiority of ourapproach for compositional text-to-image generation: CompAgent achieves morethan 10\\% improvement on T2I-CompBench, a comprehensive benchmark foropen-world compositional T2I generation. The extension to various related tasksalso illustrates the flexibility of our CompAgent for potential applications."
    },
    {
        "link": "https://arxiv.org/abs/2401.15691",
        "title": "One for all: A novel Dual-space Co-training baseline for Large-scale Multi-View Clustering",
        "authors": [
            "Zisen Kong",
            "Zhiqiang Fu",
            "Dongxia Chang",
            "Yiming Wang",
            "Yao Zhao"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we propose a novel multi-view clustering model, namedDual-space Co-training Large-scale Multi-view Clustering (DSCMC). The mainobjective of our approach is to enhance the clustering performance byleveraging co-training in two distinct spaces. In the original space, we learna projection matrix to obtain latent consistent anchor graphs from differentviews. This process involves capturing the inherent relationships andstructures between data points within each view. Concurrently, we employ afeature transformation matrix to map samples from various views to a sharedlatent space. This transformation facilitates the alignment of information frommultiple views, enabling a comprehensive understanding of the underlying datadistribution. We jointly optimize the construction of the latent consistentanchor graph and the feature transformation to generate a discriminative anchorgraph. This anchor graph effectively captures the essential characteristics ofthe multi-view data and serves as a reliable basis for subsequent clusteringanalysis. Moreover, the element-wise method is proposed to avoid the impact ofdiverse information between different views. Our algorithm has an approximatelinear computational complexity, which guarantees its successful application onlarge-scale datasets. Through experimental validation, we demonstrate that ourmethod significantly reduces computational complexity while yielding superiorclustering performance compared to existing approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15695",
        "title": "HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation",
        "authors": [
            "David Bethge",
            "Daniel Bulanda",
            "Adam Kozlowski",
            "Thomas Kosch",
            "Albrecht Schmidt",
            "Tobias Grosse-Puppendahl"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Routes represent an integral part of triggering emotions in drivers.Navigation systems allow users to choose a navigation strategy, such as thefastest or shortest route. However, they do not consider the driver's emotionalwell-being. We present HappyRouting, a novel navigation-based empathic carinterface guiding drivers through real-world traffic while evoking positiveemotions. We propose design considerations, derive a technical architecture,and implement a routing optimization framework. Our contribution is a machinelearning-based generated emotion map layer, predicting emotions along routesbased on static and dynamic contextual data. We evaluated HappyRouting in areal-world driving study (N=13), finding that happy routes increasesubjectively perceived valence by 11% (p=.007). Although happy routes take 1.25times longer on average, participants perceived the happy route as shorter,presenting an emotion-enhanced alternative to today's fastest routingmechanisms. We discuss how emotion-based routing can be integrated intonavigation apps, promoting emotional well-being for mobility use."
    },
    {
        "link": "https://arxiv.org/abs/2401.15696",
        "title": "Optimal order FEM for dynamic poroelasticity: Error analysis for equal order elements",
        "authors": [
            "Markus Bause",
            "Mathias Anselmann"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "The numerical approximation of dynamic poroelasticity, modeling flow indeformable porous media, by a family of continuous space-time finite elementmethods is investigated. Equal order approximation in space without any furtherstabilization is used for the displacement and pore pressure variable. Optimalorder L\u221e(L2) error estimates are proved and numerically confirmed."
    },
    {
        "link": "https://arxiv.org/abs/2401.15700",
        "title": "AI-based Personalization and Trust in Digital Finance",
        "authors": [
            "Vijaya Kanaparthi"
        ],
        "primary_subject": "Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Personalized services bridge the gap between a financial institution and itscustomers and are built on trust. The more we trust the product, the keener weare to disclose our personal information in order to receive a highlypersonalized service that maximizes consumer value. Artificial Intelligence(AI) can help financial institutions tailor relevant products and services totheir customers as well as improve their credit risk management, compliance,and fraud detection capabilities by incorporating chatbots and face recognitionsystems. The present study has analyzed sixteen research papers using thePRISMA model to perform a Systematic Literature Review (SLR). It has identifiedfive research gaps and corresponding questions to analyze the present scenario.One of the gaps is credit risk detection for improved personalization andtrust. Finally, an AI-based credit risk detection model has been built usingfour supervised machine learning classifiers viz., Support Vector Machine,Random Forest, Decision Tree, and Logistic Regression. Performance comparisonshows an optimal performance of the model giving accuracy of ~89%, precision of~88%, recall of ~89%, specificity of ~89%, F1_score of ~88%, and AUC of 0.77for the Random Forest classifier. This model is foreseen to be most suitablefor envisaging customer characteristics for which personalized credit riskmitigation strategies are particularly effective as compared to other existingworks presented in this study."
    },
    {
        "link": "https://arxiv.org/abs/2401.15704",
        "title": "Phoneme-Based Proactive Anti-Eavesdropping with Controlled Recording Privilege",
        "authors": [
            "Peng Huang",
            "Yao Wei",
            "Peng Cheng",
            "Zhongjie Ba",
            "Li Lu",
            "Feng Lin",
            "Yang Wang",
            "Kui Ren"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "The widespread smart devices raise people's concerns of being eavesdroppedon. To enhance voice privacy, recent studies exploit the nonlinearity inmicrophone to jam audio recorders with inaudible ultrasound. However, existingsolutions solely rely on energetic masking. Their simple-form noise leads toseveral problems, such as high energy requirements and being easily removed byspeech enhancement techniques. Besides, most of these solutions do not supportauthorized recording, which restricts their usage scenarios. In this paper, wedesign an efficient yet robust system that can jam microphones while preservingauthorized recording. Specifically, we propose a novel phoneme-based noise withthe idea of informational masking, which can distract both machines and humansand is resistant to denoising techniques. Besides, we optimize the noisetransmission strategy for broader coverage and implement a hardware prototypeof our system. Experimental results show that our system can reduce therecognition accuracy of recordings to below 50\\% under all tested speechrecognition systems, which is much better than existing solutions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15708",
        "title": "Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding",
        "authors": [
            "Jianxiang Lu",
            "Cong Xie",
            "Hui Guo"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "As large-scale text-to-image generation models have made remarkable progressin the field of text-to-image generation, many fine-tuning methods have beenproposed. However, these models often struggle with novel objects, especiallywith one-shot scenarios. Our proposed method aims to address the challenges ofgeneralizability and fidelity in an object-driven way, using only a singleinput image and the object-specific regions of interest. To improvegeneralizability and mitigate overfitting, in our paradigm, a prototypicalembedding is initialized based on the object's appearance and its class, beforefine-tuning the diffusion model. And during fine-tuning, we propose aclass-characterizing regularization to preserve prior knowledge of objectclasses. To further improve fidelity, we introduce object-specific loss, whichcan also use to implant multiple objects. Overall, our proposed object-drivenmethod for implanting new objects can integrate seamlessly with existingconcepts as well as with high fidelity and generalization. Our methodoutperforms several existing works. The code will be released."
    },
    {
        "link": "https://arxiv.org/abs/2401.15710",
        "title": "Transformational application of Artificial Intelligence and Machine learning in Financial Technologies and Financial services: A bibliometric review",
        "authors": [
            "Vijaya Kanaparthi"
        ],
        "primary_subject": "Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "In this study, I employ a multifaceted comprehensive scientometric approachto explore the intellectual underpinnings of AI and ML in financial research byexamining the publication patterns of articles, journals, authors,institutions, and nations by leveraging quantitative techniques, that transcendconventional systematic literature reviews, enabling the effective analysis ofvast scientometric and bibliographic data. By applying these approaches, Iidentify influential works, seminal contributions, thought leaders, topicalclusters, research streams, and new research frontiers, ultimately fostering adeeper understanding of the knowledge structure in AI and ML finance researchby considering publication records from 2010 to 2022 from several searchengines and database sources. The present study finds a marked increase inpublications from 2017 to 2022, which highlights a growing interest andexpanding research activity in the field, indicating its potential significanceand relevance in the contemporary academic landscape."
    },
    {
        "link": "https://arxiv.org/abs/2401.15713",
        "title": "Contrastive Learning and Mixture of Experts Enables Precise Vector Embeddings",
        "authors": [
            "Rohan Kapur",
            "Logan Hallee",
            "Arjun Patel",
            "Bohdan Khomtchouk"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The advancement of transformer neural networks has significantly elevated thecapabilities of sentence similarity models, particularly in creating effectivevector representations of natural language inputs. However, these models facenotable challenges in domain-specific contexts, especially in highlyspecialized scientific sub-fields. Traditional methods often struggle in thisregime, either overgeneralizing similarities within a niche or being overlysensitive to minor differences, resulting in inaccurate text classification andsubpar vector representation. In an era where retrieval augmentation and searchare increasingly crucial, precise and concise numerical representations areessential. In this paper, we target this issue by assembling niche datasetsusing co-citations as a similarity metric, focusing on biomedical domains. Weemploy two key strategies for fine-tuning state-of-the-art models: 1.Domain-specific Fine-Tuning, which tailors pretrained models to a singledomain, and 2. Universal Applicability with Mixture of Experts (MoE), adaptingpretrained models with enforced routing for multiple domains simultaneously.Our training approach emphasizes the use of abstracts for faster training,incorporating Multiple Negative Rankings loss for efficient contrastivelearning. Notably, our MoE variants, equipped with N experts, achieve theefficacy of N individual models, heralding a new era of versatile,One-Size-Fits-All transformer networks for various tasks. This methodologymarks significant advancements in scientific text classification metrics andholds promise for enhancing vector database search and compilation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15715",
        "title": "Exploring the Impact of Blockchain, AI, and ML on Financial Accounting Efficiency and Transformation",
        "authors": [
            "Vijaya Kanaparthi"
        ],
        "primary_subject": "Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Continuous innovations profoundly impact the financial and commercialdomains, reshaping conventional business practices. Among the disruptiveforces, Artificial Intelligence (AI), Machine Learning (ML), and blockchaintechnology stand out prominently. This study aims to evaluate the integrationof blockchain, AI, and ML within financial accounting practices. It suggests apotential revolutionary impact on financial accounting through the adoption ofblockchain technology and ML, promising reduced accounting expenses, heightenedprecision, real-time financial reporting capabilities, and expeditious auditingprocesses. AI's role in automating repetitive financial accounting tasksassists organizations in circumventing the need for additional staff, therebyminimizing associated costs. Consequently, to bolster efficiency, businessesare increasingly embracing blockchain technology and AI applications in theirfinancial accounting operations."
    },
    {
        "link": "https://arxiv.org/abs/2401.15717",
        "title": "Check News in One Click: NLP-Empowered Pro-Kremlin Propaganda Detection",
        "authors": [
            "Veronika Solopova",
            "Viktoriia Herman",
            "Christoph Benzm\u00fcller",
            "Tim Landgraf"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Many European citizens become targets of the Kremlin propaganda campaigns,aiming to minimise public support for Ukraine, foster a climate of mistrust anddisunity, and shape elections (Meister, 2022). To address this challenge, wedeveloped ''Check News in 1 Click'', the first NLP-empowered pro-Kremlinpropaganda detection application available in 7 languages, which provides thelay user with feedback on their news, and explains manipulative linguisticfeatures and keywords. We conducted a user study, analysed user entries andmodels' behaviour paired with questionnaire answers, and investigated theadvantages and disadvantages of the proposed interpretative solution."
    },
    {
        "link": "https://arxiv.org/abs/2401.15720",
        "title": "The Impact of Snippet Reliability on Misinformation in Online Health Search",
        "authors": [
            "Anat Hashavit",
            "Tamar Stern",
            "Hongning Wang",
            "Sarit Kraus"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "Search result snippets are crucial in modern search engines, providing userswith a quick overview of a website's content. Snippets help users determine therelevance of a document to their information needs, and in certain scenarioseven enable them to satisfy those needs without visiting web documents. Hence,it is crucial for snippets to reliably represent the content of theircorresponding documents. While this may be a straightforward requirement forsome queries, it can become challenging in the complex domain of healthcare,and can lead to misinformation. This paper aims to examine snippets'reliability in representing their corresponding documents, specifically in thehealth domain. To achieve this, we conduct a series of user studies usingGoogle's search results, where participants are asked to infer viewpoints ofsearch results pertaining to queries about the effectiveness of a medicalintervention for a medical condition, based solely on their titles andsnippets. Our findings reveal that a considerable portion of Google's snippets(28%) failed to present any viewpoint on the intervention's effectiveness, andthat 35% were interpreted by participants as having a different viewpointcompared to their corresponding documents. To address this issue, we propose asnippet extraction solution tailored directly to users' information needs,i.e., extracting snippets that summarize documents' viewpoints regarding theintervention and condition that appear in the query. User study demonstratesthat our information need-focused solution outperforms the mainstreamquery-based approach. With only 19.67% of snippets generated by our solutionreported as not presenting a viewpoint and a mere 20.33% misinterpreted byparticipants. These results strongly suggest that an information need-focusedapproach can significantly improve the reliability of extracted snippets inonline health search."
    },
    {
        "link": "https://arxiv.org/abs/2401.15721",
        "title": "A Study of Acquisition Functions for Medical Imaging Deep Active Learning",
        "authors": [
            "Bonaventure F. P. Dossou"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Deep Learning revolution has enabled groundbreaking achievements inrecent years. From breast cancer detection to protein folding, deep learningalgorithms have been at the core of very important advancements. However, thesemodern advancements are becoming more and more data-hungry, especially onlabeled data whose availability is scarce: this is even more prevalent in themedical context. In this work, we show how active learning could be veryeffective in data scarcity situations, where obtaining labeled data (orannotation budget is very limited). We compare several selection criteria(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored theeffect of acquired pool size on the model's performance. Our results suggestthat uncertainty is useful to the Melanoma detection task, and confirms thehypotheses of the author of the paper of interest, that \\textit{bald} performson average better than other acquisition functions. Our extended analyseshowever revealed that all acquisition functions perform badly on the positive(cancerous) samples, suggesting exploitation of class unbalance, which could becrucial in real-world settings. We finish by suggesting future work directionsthat would be useful to improve this current work. The code of ourimplementation is open-sourced at\\url{https://github.com/bonaventuredossou/ece526_course_project}"
    },
    {
        "link": "https://arxiv.org/abs/2401.15722",
        "title": "Reducing Coverage Depth in DNA Storage: A Combinatorial Perspective on Random Access Efficiency",
        "authors": [
            "Anina Gruica",
            "Daniella Bar-Lev",
            "Alberto Ravagnani",
            "Eitan Yaakobi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We investigate the fundamental limits of the recently proposed random accesscoverage depth problem for DNA data storage. Under this paradigm, it is assumedthat the user information consists of k information strands, which areencoded into n strands via some generator matrix G. In the sequencingprocess, the strands are read uniformly at random, since each strand isavailable in a large number of copies. In this context, the random accesscoverage depth problem refers to the expected number of reads (i.e., sequencedstrands) until it is possible to decode a specific information strand, which isrequested by the user. The goal is to minimize the maximum expectation over allpossible requested information strands, and this value is denoted byTmax(G). This paper introduces new techniques to investigate the randomaccess coverage depth problem, which capture its combinatorial nature. Weestablish two general formulas to find Tmax(G) for arbitrary matrices. Weintroduce the concept of recovery balanced codes and combine all these resultsand notions to compute Tmax(G) for MDS, simplex, and Hamming codes. Wealso study the performance of modified systematic MDS matrices and our resultsshow that the best results for Tmax(G) are achieved with a specific mixof encoded strands and replication of the information strands."
    },
    {
        "link": "https://arxiv.org/abs/2401.15724",
        "title": "RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced Query Responses",
        "authors": [
            "Sahil Girhepuje",
            "Siva Sankar Sajeev",
            "Purvam Jain",
            "Arya Sikder",
            "Adithya Rama Varma",
            "Ryan George",
            "Akshay Govind Srinivasan",
            "Mahendra Kurup",
            "Ashmit Sinha",
            "Sudip Mondal"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Despite the remarkable success of LLMs, they still suffer from toolinvocation and tool chaining due to inadequate input queries and/or toolargument descriptions. We propose two novel frameworks, RE-GAINS and EnCHANT,enabling LLMs to tackle tool manipulation for solving complex user queries bymaking API calls. EnCHANT is an open-source solution that makes use of an LLMformat enforcer, an LLM(OpenChat 3.5) and a retriever(ToolBench's APIRetriever). RE-GAINS is based on OpenAI models and embeddings using a specialprompt based on the RAP paper. Both solutions cost less than $0.01 per querywith minimal latency, therefore showcasing the usefulness of the frameworks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15726",
        "title": "Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data",
        "authors": [
            "Young-Jae Park",
            "Minseok Seo",
            "Doyi Kim",
            "Hyeri Kim",
            "Sanghoon Choi",
            "Beomkyu Choi",
            "Jeongwon Ryu",
            "Sohee Son",
            "Hae-Gon Jeon",
            "Yeji Choi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the face of escalating climate changes, typhoon intensities and theirensuing damage have surged. Accurate trajectory prediction is crucial foreffective damage control. Traditional physics-based models, whilecomprehensive, are computationally intensive and rely heavily on the expertiseof forecasters. Contemporary data-driven methods often rely on reanalysis data,which can be considered to be the closest to the true representation of weatherconditions. However, reanalysis data is not produced in real-time and requirestime for adjustment because prediction models are calibrated with observationaldata. This reanalysis data, such as ERA5, falls short in challenging real-worldsituations. Optimal preparedness necessitates predictions at least 72 hours inadvance, beyond the capabilities of standard physics models. In response tothese constraints, we present an approach that harnesses real-time UnifiedModel (UM) data, sidestepping the limitations of reanalysis data. Our modelprovides predictions at 6-hour intervals for up to 72 hours in advance andoutperforms both state-of-the-art data-driven methods and numerical weatherprediction models. In line with our efforts to mitigate adversities inflictedby \\rthree{typhoons}, we release our preprocessed \\textit{PHYSICS TRACK}dataset, which includes ERA5 reanalysis data, typhoon best-track, and UMforecast data."
    },
    {
        "link": "https://arxiv.org/abs/2401.15729",
        "title": "Power based adaptive compensator of output oscillations",
        "authors": [
            "Michael Ruderman"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Power-based output feedback compensator for oscillatory systems is proposed.The average input-output power of an oscillatory signal serves as an equivalentcontrol effort, while the unknown oscillation's amplitude and frequency aredetected at each half-period. This makes the compensator adaptive and discrete,while the measured oscillatory output is the single available signal in use.The proposed compensator is derived for second-order systems, while anextension to higher-order dynamics, like e.g. in case of two-inertia systems,is also provided. An illustrative experimental case study of the fifth-orderoscillatory system is provided."
    },
    {
        "link": "https://arxiv.org/abs/2401.15733",
        "title": "Achieving DNA Labeling Capacity with Minimum Labels through Extremal de Bruijn Subgraphs",
        "authors": [
            "Christoph Hofmeister",
            "Anina Gruica",
            "Dganit Hanania",
            "Rawad Bitar",
            "Eitan Yaakobi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "DNA labeling is a tool in molecular biology and biotechnology to visualize,detect, and study DNA at the molecular level. In this process, a DNA moleculeis labeled by a set of specific patterns, referred to as labels, and is thenimaged. The resulting image is modeled as an (\u2113+1)-ary sequence, where\u2113 is the number of labels, in which any non-zero symbol indicates theappearance of the corresponding label in the DNA molecule. The labelingcapacity refers to the maximum information rate that can be achieved by thelabeling process for any given set of labels. The main goal of this paper is tostudy the minimum number of labels of the same length required to achieve themaximum labeling capacity of 2 for DNA sequences or log2q for an arbitraryalphabet of size q. The solution to this problem requires the study of pathunique subgraphs of the de Bruijn graph with the largest number of edges and weprovide upper and lower bounds on this value."
    },
    {
        "link": "https://arxiv.org/abs/2401.15739",
        "title": "SegmentAnyTree: A sensor and platform agnostic deep learning model for tree segmentation using laser scanning data",
        "authors": [
            "Maciej Wielgosz",
            "Stefano Puliti",
            "Binbin Xiang",
            "Konrad Schindler",
            "Rasmus Astrup"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This research advances individual tree crown (ITC) segmentation in lidardata, using a deep learning model applicable to various laser scanning types:airborne (ULS), terrestrial (TLS), and mobile (MLS). It addresses the challengeof transferability across different data characteristics in 3D forest sceneanalysis. The study evaluates the model's performance based on platform (ULS,MLS) and data density, testing five scenarios with varying input data,including sparse versions, to gauge adaptability and canopy layer efficacy. Themodel, based on PointGroup architecture, is a 3D CNN with separate heads forsemantic and instance segmentation, validated on diverse point cloud datasets.Results show point cloud sparsification enhances performance, aiding sparsedata handling and improving detection in dense forests. The model performs wellwith >50 points per sq. m densities but less so at 10 points per sq. m due tohigher omission rates. It outperforms existing methods (e.g., Point2Tree,TLS2trees) in detection, omission, commission rates, and F1 score, setting newbenchmarks on LAUTx, Wytham Woods, and TreeLearn datasets. In conclusion, thisstudy shows the feasibility of a sensor-agnostic model for diverse lidar data,surpassing sensor-specific approaches and setting new standards in treesegmentation, particularly in complex forests. This contributes to futureecological modeling and forest management advancements."
    },
    {
        "link": "https://arxiv.org/abs/2401.15741",
        "title": "SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks",
        "authors": [
            "Serdar Erisen"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Improving the efficiency of state-of-the-art methods in semantic segmentationrequires overcoming the increasing computational cost as well as issues such asfusing semantic information from global and local contexts. Based on the recentsuccess and problems that convolutional neural networks (CNNs) encounter insemantic segmentation, this research proposes an encoder-decoder architecturewith a unique efficient residual network. Attention-boosting gates (AbGs) andattention-boosting modules (AbMs) are deployed by aiming to fuse thefeature-based semantic information with the global context of the efficientresidual network in the encoder. Respectively, the decoder network is developedwith the additional attention-fusion networks (AfNs) inspired by AbM. AfNs aredesigned to improve the efficiency in the one-to-one conversion of the semanticinformation by deploying additional convolution layers in the decoder part. Ournetwork is tested on the challenging CamVid and Cityscapes datasets, and theproposed methods reveal significant improvements on the existing baselines,such as ResNet-50. To the best of our knowledge, the developed network,SERNet-Former, achieves state-of-the-art results (84.62 % mean IoU) on CamViddataset and challenging results (87.35 % mean IoU) on Cityscapes validationdataset."
    },
    {
        "link": "https://arxiv.org/abs/2401.15742",
        "title": "Efficient Data-Driven MPC for Demand Response of Commercial Buildings",
        "authors": [
            "Marie-Christine Par\u00e9",
            "Vasken Dermardiros",
            "Antoine Lesage-Landry"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Model predictive control (MPC) has been shown to significantly improve theenergy efficiency of buildings while maintaining thermal comfort. Data-drivenapproaches based on neural networks have been proposed to facilitate systemmodelling. However, such approaches are generally nonconvex and result incomputationally intractable optimization problems. In this work, we design areadily implementable energy management method for small commercial buildings.We then leverage our approach to formulate a real-time demand bidding strategy.We propose a data-driven and mixed-integer convex MPC which is solved viaderivative-free optimization given a limited computational time of 5 minutes torespect operational constraints. We consider rooftop unit heating, ventilation,and air conditioning systems with discrete controls to accurately model theoperation of most commercial buildings. Our approach uses an input convexrecurrent neural network to model the thermal dynamics. We apply our approachin several demand response (DR) settings, including a demand bidding, atime-of-use, and a critical peak rebate program. Controller performance isevaluated on a state-of-the-art building simulation. The proposed approachimproves thermal comfort while reducing energy consumption and cost through DRparticipation, when compared to other data-driven approaches or a set-pointcontroller."
    },
    {
        "link": "https://arxiv.org/abs/2401.15747",
        "title": "Numerical modelling of protein misfolding in neurodegenerative diseases: a computational study",
        "authors": [
            "Paola F. Antonietti",
            "Mattia Corti"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "The spreading of misfolded proteins is a known hallmark in someneurodegenerative diseases, known as proteinopathies. A significant example isthe tau protein, associated with many pathologies, such as Alzheimer's. In thiswork, we discuss and compare two different models for the mathematicalmodelling of protein misfolding, namely the heterodimer model and theFisher-Kolmogorov model, as well as their numerical discretizations. Weintroduce a discontinuous Galerkin method on polygonal and polyhedral grids forspace discretization to accurately simulate the wavefronts typically observedin the prionic spreading. Starting from the semidiscrete formulations, we use aCrank-Nicolson scheme to advance in time. Finally, we simulate the spreading ofthe misfolded tau protein in a two-dimensional brain slice in the sagittalplane with a polygonal agglomerated grid. The simulation is performed usingboth the presented models, and we compare the results and the differencesderiving from the modelling choices."
    },
    {
        "link": "https://arxiv.org/abs/2401.15752",
        "title": "Integrated Sensing and Communication in the Finite Blocklength Regime",
        "authors": [
            "Homa Nikbakht",
            "Mich\u00e8le Wigger",
            "Shlomo Shamai",
            "H.Vincent Poor"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "A point-to-point integrated sensing and communication (ISAC) system isconsidered where a transmitter conveys a message to a receiver over a discretememoryless channel (DMC) and simultaneously estimates the state of the channelthrough the backscattered signals of the emitted waveform.We derive achievability and converse bounds on the rate-distortion-errortradeoff in the finite blocklength regime, and also characterize thesecond-order rate-distortion-error region for the proposed setup. Numericalanalysis shows that our proposed joint ISAC scheme significantly outperformstraditional time-sharing based schemes where the available resources are splitbetween the sensing and communication tasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15753",
        "title": "An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion",
        "authors": [
            "Sharib Ali",
            "Yamid Espinel",
            "Yueming Jin",
            "Peng Liu",
            "Bianca G\u00fcttner",
            "Xukun Zhang",
            "Lihua Zhang",
            "Tom Dowrick",
            "Matthew J. Clarkson",
            "Shiting Xiao",
            "Yifan Wu",
            "Yijun Yang",
            "Lei Zhu",
            "Dai Sun",
            "Lan Li",
            "Micha Pfeiffer",
            "Shahid Farid",
            "Lena Maier-Hein",
            "Emmanuel Buc",
            "Adrien Bartoli"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Augmented reality for laparoscopic liver resection is a visualisation modethat allows a surgeon to localise tumours and vessels embedded within the liverby projecting them on top of a laparoscopic image. Preoperative 3D modelsextracted from CT or MRI data are registered to the intraoperative laparoscopicimages during this process. In terms of 3D-2D fusion, most of the algorithmsmake use of anatomical landmarks to guide registration. These landmarks includethe liver's inferior ridge, the falciform ligament, and the occluding contours.They are usually marked by hand in both the laparoscopic image and the 3Dmodel, which is time-consuming and may contain errors if done by anon-experienced user. Therefore, there is a need to automate this process sothat augmented reality can be used effectively in the operating room. Wepresent the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge(P2ILF), held during the Medical Imaging and Computer Assisted Interventions(MICCAI 2022) conference, which investigates the possibilities of detectingthese landmarks automatically and using them in registration. The challenge wasdivided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2Dregistration task. The teams were provided with training data consisting of 167laparoscopic images and 9 preoperative 3D models from 9 patients, with thecorresponding 2D and 3D landmark annotations. A total of 6 teams from 4countries participated, whose proposed methods were evaluated on 16 images andtwo preoperative 3D models from two patients. All the teams proposed deeplearning-based methods for the 2D and 3D landmark segmentation tasks anddifferentiable rendering-based methods for the registration task. Based on theexperimental outcomes, we propose three key hypotheses that determine currentlimitations and future directions for research in this domain."
    },
    {
        "link": "https://arxiv.org/abs/2401.15756",
        "title": "Resource Allocation in C-V2X: A review",
        "authors": [
            "Tahmid Zaman Tahi"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Cellular Vehicle-to-Everything (C-V2X) is a cutting-edge wirelesscommunication technology that enables seamless connectivity and informationexchange among vehicles, infrastructure, networks, and pedestrians. As a vitalcomponent of Intelligent Transportation Systems (ITS), C-V2X is designed tosupport a wide range of applications aimed at enhancing traffic efficiency,improving road safety, reducing accident rates, and facilitating thedevelopment of autonomous and connected vehicles. C-V2X technology is builtupon the Long-Term Evolution (LTE) and 5G New Radio (NR) stan- dards,leveraging the robustness, reliability, and scalability of cellular networks.It encompasses two distinct communication modes: (1) direct communication,which includes Vehicle-to-Vehicle (V2V), Vehicle-to-Infrastructure (V2I), andVehicle-to-Pedestrian (V2P) communication, and (2) network-based communication,which involves Vehicle-to-Network (V2N) communication. Resource allocation is acritical challenge in the design and operation of C-V2X systems, as it isresponsible for determining the optimal distribution of communication resourcesamong users, ensuring efficient utilization and fair sharing. In C-V2X,resource allocation is complicated by factors such as highly dynamic networktopologies, diverse quality of service (QoS) requirements, and spectrumscarcity. Therefore, it is essential to explore and analyze various resourceallocation strategies and techniques that can effectively address thesechallenges. This review paper provides a comprehensive overview of the recentprogress in resource allocation for C-V2X communications. As C-V2X technologyevolves, it is expected to play a crucial role in transforming thetransportation landscape, paving the way for smarter, safer, and more efficienttransportation systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15758",
        "title": "Randomized Preconditioned Solvers for Strong Constraint 4D-Var Data Assimilation",
        "authors": [
            "Amit N. Subrahmanya",
            "Vishwas Rao",
            "Arvind K. Saibaba"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Strong Constraint 4D Variational (SC-4DVAR) is a data assimilation methodthat is widely used in climate and weather applications. SC-4DVAR involvessolving a minimization problem to compute the maximum a posteriori estimate,which we tackle using the Gauss-Newton method. The computation of the descentdirection is expensive since it involves the solution of a large-scale andpotentially ill-conditioned linear system, solved using the preconditionedconjugate gradient (PCG) method. To address this cost, we efficiently constructscalable preconditioners using three different randomization techniques, whichall rely on a certain low-rank structure involving the Gauss-Newton Hessian.The proposed techniques come with theoretical (probabilistic) guarantees on thecondition number, and at the same time, are amenable to parallelization. Wealso develop an adaptive approach to estimate the sketch size and to choosebetween the reuse or recomputation of the preconditioner. We demonstrate theperformance and effectiveness of our methodology on two representative modelproblems -- the Burgers and barotropic vorticity equation -- showing a drasticreduction in both the number of PCG iterations and the number of Gauss-NewtonHessian products (after including the preconditioner construction cost)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15760",
        "title": "HRI Challenges Influencing Low Usage of Robotic Systems in Disaster Response and Rescue Operations",
        "authors": [
            "Shahinul Hoque",
            "Farhin Farhad Riya",
            "Jinyuan Sun"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "The breakthrough in AI and Machine Learning has brought a new revolution inrobotics, resulting in the construction of more sophisticated robotic systems.Not only can these robotic systems benefit all domains, but also can accomplishtasks that seemed to be unimaginable a few years ago. From swarms of autonomoussmall robots working together to more very heavy and large objects, toseemingly indestructible robots capable of going to the harshest environments,we can see robotic systems designed for every task imaginable. Among them, akey scenario where robotic systems can benefit is in disaster responsescenarios and rescue operations. Robotic systems are capable of successfullyconducting tasks such as removing heavy materials, utilizing multiple advancedsensors for finding objects of interest, moving through debris and variousinhospitable environments, and not the least have flying capabilities. Evenwith so much potential, we rarely see the utilization of robotic systems indisaster response scenarios and rescue missions. Many factors could beresponsible for the low utilization of robotic systems in such scenarios. Oneof the key factors involve challenges related to Human-Robot Interaction (HRI)issues. Therefore, in this paper, we try to understand the HRI challengesinvolving the utilization of robotic systems in disaster response and rescueoperations. Furthermore, we go through some of the proposed robotic systemsdesigned for disaster response scenarios and identify the HRI challenges ofthose systems. Finally, we try to address the challenges by introducing ideasfrom various proposed research works."
    },
    {
        "link": "https://arxiv.org/abs/2401.15762",
        "title": "Smart Driver Monitoring Robotic System to Enhance Road Safety : A Comprehensive Review",
        "authors": [
            "Farhin Farhad Riya",
            "Shahinul Hoque",
            "Xiaopeng Zhao",
            "Jinyuan Stella Sun"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "The future of transportation is being shaped by technology, and onerevolutionary step in improving road safety is the incorporation of roboticsystems into driver monitoring infrastructure. This literature review exploresthe current landscape of driver monitoring systems, ranging from traditionalphysiological parameter monitoring to advanced technologies such as facialrecognition to steering analysis. Exploring the challenges faced by existingsystems, the review then investigates the integration of robots as intelligententities within this framework. These robotic systems, equipped with artificialintelligence and sophisticated sensors, not only monitor but actively engagewith the driver, addressing cognitive and emotional states in real-time. Thesynthesis of existing research reveals a dynamic interplay between human andmachine, offering promising avenues for innovation in adaptive, personalized,and ethically responsible human-robot interactions for driver monitoring. Thisreview establishes a groundwork for comprehending the intricacies and potentialavenues within this dynamic field. It encourages further investigation andadvancement at the intersection of human-robot interaction and automotivesafety, introducing a novel direction. This involves various sections detailingtechnological enhancements that can be integrated to propose an innovative andimproved driver monitoring system."
    },
    {
        "link": "https://arxiv.org/abs/2401.15763",
        "title": "Developing an Analytical Fixed Source Solver for the 1D Multigroup",
        "authors": [
            "Jilang Miao",
            "Miaomiao Jin"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We extend the analytical multigroup SN method to solve 1D fixed sourceproblems. The fixed source solver is applied in power iteration of eigenvalueproblems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15765",
        "title": "Coarse Mesh Iteration Approach for Analytical 1D Multigroup",
        "authors": [
            "Jilang Miao",
            "Miaomiao Jin"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This paper extends the fixed source capability of analytical 1D multigroupSN equations to solve eigenvalue problems on coarse mesh."
    },
    {
        "link": "https://arxiv.org/abs/2401.15766",
        "title": "EEG for fatigue monitoring",
        "authors": [
            "Ildar Rakhmatulin"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Physiological fatigue, a state of reduced cognitive and physical performanceresulting from prolonged mental or physical exertion, poses significantchallenges in various domains, including healthcare, aviation, transportation,and industrial sectors. As the understanding of fatigue's impact on humanperformance grows, there is a growing interest in developing effective fatiguemonitoring techniques. Among these techniques, electroencephalography (EEG) hasemerged as a promising tool for objectively assessing physiological fatigue dueto its non-invasiveness, high temporal resolution, and sensitivity to neuralactivity. This paper aims to provide a comprehensive analysis of the currentstate of the use of EEG for monitoring physiological fatigue."
    },
    {
        "link": "https://arxiv.org/abs/2401.15767",
        "title": "A Centralized Reinforcement Learning Framework for Adaptive Clustering with Low Control Overhead in IoT Networks",
        "authors": [
            "F. Fernando Jurado-Lasso",
            "J. F. Jurado",
            "Xenofon Fafoutis"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Wireless Sensor Networks (WSNs) play a pivotal role in enabling Internet ofThings (IoT) devices with sensing and actuation capabilities. Operating inremote and resource-constrained environments, these IoT devices face challengesrelated to energy consumption, crucial for network longevity. Clusteringprotocols have emerged as an effective solution to alleviate energy burdens onIoT devices. This paper introduces Low-Energy Adaptive Clustering Hierarchywith Reinforcement Learning-based Controller (LEACH-RLC), a novel clusteringprotocol that employs a Mixed Integer Linear Programming (MILP) for strategicselection of cluster heads (CHs) and node-to-cluster assignments. Additionally,it integrates a Reinforcement Learning (RL) agent to minimize control overheadby learning optimal timings for generating new clusters. Addressing keyresearch questions, LEACH-RLC seeks to balance control overhead reductionwithout compromising overall network performance. Through extensivesimulations, this paper investigates the frequency and opportune moments forgenerating new clustering solutions. Results demonstrate the superiorperformance of LEACH-RLC over conventional LEACH and LEACH-C, showcasingenhanced network lifetime, reduced average energy consumption, and minimizedcontrol overhead. The proposed protocol contributes to advancing the efficiencyand adaptability of WSNs, addressing critical challenges in IoT deployments."
    },
    {
        "link": "https://arxiv.org/abs/2401.15770",
        "title": "PILOT: Legal Case Outcome Prediction with Case Law",
        "authors": [
            "Lang Cao",
            "Zifeng Wang",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Machine learning shows promise in predicting the outcome of legal cases, butmost research has concentrated on civil law cases rather than case law systems.We identified two unique challenges in making legal case outcome predictionswith case law. First, it is crucial to identify relevant precedent cases thatserve as fundamental evidence for judges during decision-making. Second, it isnecessary to consider the evolution of legal principles over time, as earlycases may adhere to different legal contexts.In this paper, we proposed a new model named PILOT (PredictIng Legal caseOuTcome) for case outcome prediction. It comprises two modules for relevantcase retrieval and temporal pattern handling, respectively. To benchmark theperformance of existing legal case outcome prediction models, we curated adataset from a large-scale case law database. We demonstrate the importance ofaccurately identifying precedent cases and mitigating the temporal shift whenmaking predictions for case law, as our method shows a significant improvementover the prior methods that focus on civil law case outcome predictions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15773",
        "title": "Evaluation of k-means time series clustering based on z-normalization and NP-Free",
        "authors": [
            "Ming-Chang Lee",
            "Jia-Chun Lin",
            "Volker Stolz"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Despite the widespread use of k-means time series clustering in variousdomains, there exists a gap in the literature regarding its comprehensiveevaluation with different time series normalization approaches. This paperseeks to fill this gap by conducting a thorough performance evaluation ofk-means time series clustering on real-world open-source time series datasets.The evaluation focuses on two distinct normalization techniques:z-normalization and NP-Free. The former is one of the most commonly usednormalization approach for time series. The latter is a real-time time seriesrepresentation approach, which can serve as a time series normalizationapproach. The primary objective of this paper is to assess the impact of thesetwo normalization techniques on k-means time series clustering in terms of itsclustering quality. The experiments employ the silhouette score, awell-established metric for evaluating the quality of clusters in a dataset. Bysystematically investigating the performance of k-means time series clusteringwith these two normalization techniques, this paper addresses the current gapin k-means time series clustering evaluation and contributes valuable insightsto the development of time series clustering."
    },
    {
        "link": "https://arxiv.org/abs/2401.15774",
        "title": "Integrating Differential Privacy and Contextual Integrity",
        "authors": [
            "Sebastian Benthall",
            "Rachel Cummings"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "In this work, we propose the first framework for integrating DifferentialPrivacy (DP) and Contextual Integrity (CI). DP is a property of an algorithmthat injects statistical noise to obscure information about individualsrepresented within a database. CI defines privacy as information flow that isappropriate to social context. Analyzed together, these paradigms outline twodimensions on which to analyze privacy of information flows: descriptive andnormative properties. We show that our new integrated framework providesbenefits to both CI and DP that cannot be attained when each definition isconsidered in isolation: it enables contextually-guided tuning of the epsilonparameter in DP, and it enables CI to be applied to a broader set ofinformation flows occurring in real-world systems, such as those involving PETsand machine learning. We conclude with a case study based on the use of DP inthe U.S. Census Bureau."
    },
    {
        "link": "https://arxiv.org/abs/2401.15777",
        "title": "cantnlp@LT-EDI-2024: Automatic Detection of Anti-LGBTQ+ Hate Speech in Under-resourced Languages",
        "authors": [
            "Sidney G.-J. Wong",
            "Matthew Durward"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This paper describes our homophobia/transphobia in social media commentsdetection system developed as part of the shared task at LT-EDI-2024. We took atransformer-based approach to develop our multiclass classification model forten language conditions (English, Spanish, Gujarati, Hindi, Kannada, Malayalam,Marathi, Tamil, Tulu, and Telugu). We introduced synthetic and organicinstances of script-switched language data during domain adaptation to mirrorthe linguistic realities of social media language as seen in the labelledtraining data. Our system ranked second for Gujarati and Telugu with varyinglevels of performance for other language conditions. The results suggestincorporating elements of paralinguistic behaviour such as script-switching mayimprove the performance of language detection systems especially in the casesof under-resourced languages conditions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15780",
        "title": "Fine-Tuned Large Language Models for Symptom Recognition from Spanish Clinical Text",
        "authors": [
            "Mai A. Shaaban",
            "Abbas Akkasi",
            "Adnan Khan",
            "Majid Komeili",
            "Mohammad Yaqub"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "The accurate recognition of symptoms in clinical reports is significantlyimportant in the fields of healthcare and biomedical natural languageprocessing. These entities serve as essential building blocks for clinicalinformation extraction, enabling retrieval of critical medical insights fromvast amounts of textual data. Furthermore, the ability to identify andcategorize these entities is fundamental for developing advanced clinicaldecision support systems, aiding healthcare professionals in diagnosis andtreatment planning. In this study, we participated in SympTEMIST, a shared taskon the detection of symptoms, signs and findings in Spanish medical documents.We combine a set of large language models fine-tuned with the data released bythe organizers."
    },
    {
        "link": "https://arxiv.org/abs/2401.15781",
        "title": "The Discrepancy of Shortest Paths",
        "authors": [
            "Greg Bodwin",
            "Chengyuan Deng",
            "Jie Gao",
            "Gary Hoppenworth",
            "Jalaj Upadhyay",
            "Chen Wang"
        ],
        "primary_subject": "Data Structures and Algorithms (cs.DS)",
        "abstract": "The hereditary discrepancy of a set system is a certain quantitative measureof the pseudorandom properties of the system. Roughly, hereditary discrepancymeasures how well one can 2-color the elements of the system so that each setcontains approximately the same number of elements of each color. Hereditarydiscrepancy has well-studied applications e.g. in communication complexity andderandomization. More recently, the hereditary discrepancy of set systems ofshortest paths has found applications in differential privacy [Chen et al.~SODA23].The contribution of this paper is to improve the upper and lower bounds onthe hereditary discrepancy of set systems of unique shortest paths in graphs.In particular, we show that any system of unique shortest paths in anundirected weighted graph has hereditary discrepancy O\u02dc(n1/4),and we construct lower bound examples demonstrating that this bound is tight upto hidden polylog\u00a0n factors. Our lower bounds apply even in theplanar and bipartite settings, and they improve on a previous lower bound of\u03a9(n1/6) obtained by applying the trace bound of Chazelle and Lvov[SoCG'00] to a classical point-line system of Erd\\H{o}s. We also show similarbounds on (non-hereditary) discrepancy and in the setting of directed graphs.As applications, we improve the lower bound on the additive error fordifferentially-private all pairs shortest distances from \u03a9(n1/6)[Chen et al.~SODA 23] to \u03a9(n1/4), and we improve the lower bound onadditive error for the differentially-private all sets range queries problem to\u03a9(n1/4), which is tight up to hidden polylog\u00a0n factors[Deng et al.~WADS 23]."
    },
    {
        "link": "https://arxiv.org/abs/2401.15783",
        "title": "ARGOS: An Automaton Referencing Guided Overtake System for Head-to-Head Autonomous Racing",
        "authors": [
            "Varundev Sukhil",
            "Madhur Behl"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Autonomous overtaking at high speeds is a challenging multi-agent roboticsresearch problem. The high-speed and close proximity situations that arise inmulti-agent autonomous racing require designing algorithms that trade offaggressive overtaking maneuvers and minimize the risk of collision with theopponent. In this paper, we study a special case of multi-agent autonomousrace, called the head-to-head autonomous race, that requires two racecars withsimilar performance envelopes. We present a mathematical formulation of anovertake and position defense in this head-to-head autonomous racing scenario,and we introduce the Automaton Referencing Guided Overtake System (ARGOS)framework that supervises the execution of an overtake or position defensemaneuver depending on the current role of the racecar. The ARGOS frameworkworks by decomposing complex overtake and position-defense maneuvers intosequential and temporal submaneuvers that are individually managed andsupervised by a network of automatons. We verify the properties of the ARGOSframework using model-checking and demonstrate results from multiplesimulations, which show that the framework meets the desired specifications.The ARGOS framework performs similar to what can be observed from real-worldhuman-driven motor sport racing."
    },
    {
        "link": "https://arxiv.org/abs/2401.15785",
        "title": "Real-time object detection and robotic manipulation for agriculture using a YOLO-based learning approach",
        "authors": [
            "Hongyu Zhao",
            "Zezhi Tang",
            "Zhenhong Li",
            "Yi Dong",
            "Yuancheng Si",
            "Mingyang Lu",
            "George Panoutsos"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The optimisation of crop harvesting processes for commonly cultivated cropsis of great importance in the aim of agricultural industrialisation. Nowadays,the utilisation of machine vision has enabled the automated identification ofcrops, leading to the enhancement of harvesting efficiency, but challengesstill exist. This study presents a new framework that combines two separatearchitectures of convolutional neural networks (CNNs) in order tosimultaneously accomplish the tasks of crop detection and harvesting (roboticmanipulation) inside a simulated environment. Crop images in the simulatedenvironment are subjected to random rotations, cropping, brightness, andcontrast adjustments to create augmented images for dataset generation. The youonly look once algorithmic framework is employed with traditional rectangularbounding boxes for crop localization. The proposed method subsequently utilisesthe acquired image data via a visual geometry group model in order to revealthe grasping positions for the robotic manipulation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15788",
        "title": "230,439 Test Failures Later: An Empirical Evaluation of Flaky Failure Classifiers",
        "authors": [
            "Abdulrahman Alshammari",
            "Paul Ammann",
            "Michael Hilton",
            "Jonathan Bell"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Flaky tests are tests that can non-deterministically pass or fail, even inthe absence of code changes.Despite being a source of false alarms, flaky testsoften remain in test suites once they are detected, as they also may be reliedupon to detect true failures. Hence, a key open problem in flaky test researchis: How to quickly determine if a test failed due to flakiness, or if itdetected a bug? The state-of-the-practice is for developers to re-run failingtests: if a test fails and then passes, it is flaky by definition; if the testpersistently fails, it is likely a true failure. However, this approach can beboth ineffective and inefficient. An alternate approach that developers mayalready use for triaging test failures is failure de-duplication, which matchesnewly discovered test failures to previously witnessed flaky and true failures.However, because flaky test failure symptoms might resemble those of truefailures, there is a risk of missclassifying a true test failure as a flakyfailure to be ignored. Using a dataset of 498 flaky tests from 22 open-sourceJava projects, we collect a large dataset of 230,439 failure messages (bothflaky and not), allowing us to empirically investigate the efficacy of failurede-duplication. We find that for some projects, this approach is extremelyeffective (with 100\\% specificity), while for other projects, the approach isentirely ineffective. By analyzing the characteristics of these flaky andnon-flaky failures, we provide useful guidance on how developers should rely onthis approach."
    },
    {
        "link": "https://arxiv.org/abs/2401.15794",
        "title": "Regulation of Algorithmic Collusion",
        "authors": [
            "Jason D. Hartline",
            "Sheng Long",
            "Chenhao Zhang"
        ],
        "primary_subject": "Computer Science and Game Theory (cs.GT)",
        "abstract": "Consider sellers in a competitive market that use algorithms to adapt theirprices from data that they collect. In such a context it is plausible thatalgorithms could arrive at prices that are higher than the competitive pricesand this may benefit sellers at the expense of consumers (i.e., the buyers inthe market). This paper gives a definition of plausible algorithmicnon-collusion for pricing algorithms. The definition allows a regulator toempirically audit algorithms by applying a statistical test to the data thatthey collect. Algorithms that are good, i.e., approximately optimize prices tomarket conditions, can be augmented to contain the data sufficient to pass theaudit. Algorithms that have colluded on, e.g., supra-competitive prices cannotpass the audit. The definition allows sellers to possess useful sideinformation that may be correlated with supply and demand and could affect theprices used by good algorithms. The paper provides an analysis of thestatistical complexity of such an audit, i.e., how much data is sufficient forthe test of non-collusion to be accurate."
    },
    {
        "link": "https://arxiv.org/abs/2401.15798",
        "title": "UnMASKed: Quantifying Gender Biases in Masked Language Models through Linguistically Informed Job Market Prompts",
        "authors": [
            "I\u00f1igo Parra"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Language models (LMs) have become pivotal in the realm of technologicaladvancements. While their capabilities are vast and transformative, they ofteninclude societal biases encoded in the human-produced datasets used for theirtraining. This research delves into the inherent biases present in maskedlanguage models (MLMs), with a specific focus on gender biases. This studyevaluated six prominent models: BERT, RoBERTa, DistilBERT, BERT-multilingual,XLM-RoBERTa, and DistilBERT-multilingual. The methodology employed a noveldataset, bifurcated into two subsets: one containing prompts that encouragedmodels to generate subject pronouns in English, and the other requiring modelsto return the probabilities of verbs, adverbs, and adjectives linked to theprompts' gender pronouns. The analysis reveals stereotypical gender alignmentof all models, with multilingual variants showing comparatively reduced biases."
    },
    {
        "link": "https://arxiv.org/abs/2401.15803",
        "title": "GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes and Minimalist Workflow",
        "authors": [
            "Liguo Zhou",
            "Yinglei Song",
            "Yichao Gao",
            "Zhou Yu",
            "Michael Sodamin",
            "Hongshen Liu",
            "Liang Ma",
            "Lian Liu",
            "Hao Liu",
            "Yang Liu",
            "Haichuan Li",
            "Guang Chen",
            "Alois Knoll"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Conducting real road testing for autonomous driving algorithms can beexpensive and sometimes impractical, particularly for small startups andresearch institutes. Thus, simulation becomes an important method forevaluating these algorithms. However, the availability of free and open-sourcesimulators is limited, and the installation and configuration process can bedaunting for beginners and interdisciplinary researchers. We introduce anautonomous driving simulator with photorealistic scenes, meanwhile keeping auser-friendly workflow. The simulator is able to communicate with externalalgorithms through ROS2 or Socket.IO, making it compatible with existingsoftware stacks. Furthermore, we implement a highly accurate vehicle dynamicsmodel within the simulator to enhance the realism of the vehicle's physicaleffects. The simulator is able to serve various functions, including generatingsynthetic data and driving with machine learning-based algorithms. Moreover, weprioritize simplicity in the deployment process, ensuring that beginners findit approachable and user-friendly."
    },
    {
        "link": "https://arxiv.org/abs/2401.15805",
        "title": "Prediction of Breast Cancer Recurrence Risk Using a Multi-Model Approach Integrating Whole Slide Imaging and Clinicopathologic Features",
        "authors": [
            "Manu Goyal",
            "Jonathan D. Marotti",
            "Adrienne A. Workman",
            "Elaine P. Kuhn",
            "Graham M. Tooker",
            "Seth K. Ramin",
            "Mary D. Chamberlin",
            "Roberta M. diFlorio-Alexander",
            "Saeed Hassanpour"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Breast cancer is the most common malignancy affecting women worldwide and isnotable for its morphologic and biologic diversity, with varying risks ofrecurrence following treatment. The Oncotype DX Breast Recurrence Score test isan important predictive and prognostic genomic assay for estrogenreceptor-positive breast cancer that guides therapeutic strategies; however,such tests can be expensive, delay care, and are not widely available. The aimof this study was to develop a multi-model approach integrating the analysis ofwhole slide images and clinicopathologic data to predict their associatedbreast cancer recurrence risks and categorize these patients into two riskgroups according to the predicted score: low and high risk. The proposed novelmethodology uses convolutional neural networks for feature extraction andvision transformers for contextual aggregation, complemented by a logisticregression model that analyzes clinicopathologic data for classification intotwo risk categories. This method was trained and tested on 993 hematoxylin andeosin-stained whole-slide images of breast cancers with correspondingclinicopathological features that had prior Oncotype DX testing. The model'sperformance was evaluated using an internal test set of 198 patients fromDartmouth Health and an external test set of 418 patients from the Universityof Chicago. The multi-model approach achieved an AUC of 0.92 (95 percent CI:0.88-0.96) on the internal set and an AUC of 0.85 (95 percent CI: 0.79-0.90) onthe external cohort. These results suggest that with further validation, theproposed methodology could provide an alternative to assist clinicians inpersonalizing treatment for breast cancer patients and potentially improvingtheir outcomes."
    },
    {
        "link": "https://arxiv.org/abs/2401.15810",
        "title": "Green Runner: A tool for efficient deep learning component selection",
        "authors": [
            "Jai Kannan"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "For software that relies on machine-learned functionality, model selection iskey to finding the right model for the task with desired performancecharacteristics. Evaluating a model requires developers to i) select from manymodels (e.g. the Hugging face model repository), ii) select evaluation metricsand training strategy, and iii) tailor trade-offs based on the problem domain.However, current evaluation approaches are either ad-hoc resulting insub-optimal model selection or brute force leading to wasted compute. In thiswork, we present \\toolname, a novel tool to automatically select and evaluatemodels based on the application scenario provided in natural language. Weleverage the reasoning capabilities of large language models to propose atraining strategy and extract desired trade-offs from a problem description.\\toolname~features a resource-efficient experimentation engine that integratesconstraints and trade-offs based on the problem into the model selectionprocess. Our preliminary evaluation demonstrates that \\toolname{} is bothefficient and accurate compared to ad-hoc evaluations and brute force. Thiswork presents an important step toward energy-efficient tools to help reducethe environmental impact caused by the growing demand for software withmachine-learned functionality."
    },
    {
        "link": "https://arxiv.org/abs/2401.15814",
        "title": "OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for Medication Recommendation",
        "authors": [
            "Weicong Tan",
            "Weiqing Wang",
            "Xin Zhou",
            "Wray Buntine",
            "Gordon Bingham"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Most existing medication recommendation models learn representations formedical concepts based on electronic health records (EHRs) and makerecommendations with learnt representations. However, most medications appearin the dataset for limited times, resulting in insufficient learning of theirrepresentations. Medical ontologies are the hierarchical classification systemsfor medical terms where similar terms are in the same class on a certain level.In this paper, we propose OntoMedRec, the logically-pretrained andmodel-agnostic medical Ontology Encoders for Medication Recommendation thataddresses data sparsity problem with medical ontologies. We conductcomprehensive experiments on benchmark datasets to evaluate the effectivenessof OntoMedRec, and the result shows the integration of OntoMedRec improves theperformance of various models in both the entire EHR datasets and theadmissions with few-shot medications. We provide the GitHub repository for thesource code on https://anonymous.4open.science/r/OntoMedRec-D123"
    },
    {
        "link": "https://arxiv.org/abs/2401.15817",
        "title": "Transparency Attacks: How Imperceptible Image Layers Can Fool AI Perception",
        "authors": [
            "Forrest McKee",
            "David Noever"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper investigates a novel algorithmic vulnerability when imperceptibleimage layers confound multiple vision models into arbitrary label assignmentsand captions. We explore image preprocessing methods to introduce stealthtransparency, which triggers AI misinterpretation of what the human eyeperceives. The research compiles a broad attack surface to investigate theconsequences ranging from traditional watermarking, steganography, andbackground-foreground miscues. We demonstrate dataset poisoning using theattack to mislabel a collection of grayscale landscapes and logos using eithera single attack layer or randomly selected poisoning classes. For example, amilitary tank to the human eye is a mislabeled bridge to object classifiersbased on convolutional networks (YOLO, etc.) and vision transformers (ViT,GPT-Vision, etc.). A notable attack limitation stems from its dependency on thebackground (hidden) layer in grayscale as a rough match to the transparentforeground image that the human eye perceives. This dependency limits thepractical success rate without manual tuning and exposes the hidden layers whenplaced on the opposite display theme (e.g., light background, light transparentforeground visible, works best against a light theme image viewer or browser).The stealth transparency confounds established vision systems, includingevading facial recognition and surveillance, digital watermarking, contentfiltering, dataset curating, automotive and drone autonomy, forensic evidencetampering, and retail product misclassifying. This method stands in contrast totraditional adversarial attacks that typically focus on modifying pixel valuesin ways that are either slightly perceptible or entirely imperceptible for bothhumans and machines."
    },
    {
        "link": "https://arxiv.org/abs/2401.15818",
        "title": "A Middle Way to Traffic Enlightenment",
        "authors": [
            "Matthew W. Nice",
            "George Gunter",
            "Junyi Ji",
            "Yuhang Zhang",
            "Matthew Bunting",
            "Will Barbour",
            "Jonathan Sprinkle",
            "Dan Work"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This paper introduces a novel approach that seeks a middle ground for trafficcontrol in multi-lane congestion, where prevailing traffic speeds are too fast,and speed recommendations designed to dampen traffic waves are too slow.Advanced controllers that modify the speed of an automated car forwave-dampening, eco-driving, or other goals, typically are designed withforward collision safety in mind. Our approach goes further, by considering howdangerous it can be for a controller to drive so slowly relative to prevailingtraffic that it creates a significant issue for safety and comfort. This paperexplores open-road scenarios where large gaps between prevailing speeds anddesired speeds can exist, specifically when infrastructure-based variable speedlimit systems are not strictly followed at all times by other drivers. Ourdesigned, implemented, and deployed algorithm is able to follow variable speedlimits when others also follow it, avoid collisions with vehicles ahead, andadapt to prevailing traffic when other motorists are traveling well above theposted speeds. The key is to reject unsafe speed recommendations frominfrastructure-based traffic smoothing systems, based on real-time localtraffic conditions observed by the vehicle under control. This solution isimplemented and deployed on two control vehicles in heavy multi-lane highwaycongestion. The results include analysis from system design, and field teststhat validate the system's performance using an existing Variable Speed Limitsystem as the external source for speed recommendations, and the on-boardsensors of a stock Toyota Rav4 for inputs that estimate the prevailing speed oftraffic around the vehicle under control."
    },
    {
        "link": "https://arxiv.org/abs/2401.15820",
        "title": "Knowledge-Aware Neuron Interpretation for Scene Classification",
        "authors": [
            "Yong Guan",
            "Freddy Lecue",
            "Jiaoyan Chen",
            "Ru Li",
            "Jeff Z. Pan"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Although neural models have achieved remarkable performance, they stillencounter doubts due to the intransparency. To this end, model predictionexplanation is attracting more and more attentions. However, current methodsrarely incorporate external knowledge and still suffer from three limitations:(1) Neglecting concept completeness. Merely selecting concepts may notsufficient for prediction. (2) Lacking concept fusion. Failure to mergesemantically-equivalent concepts. (3) Difficult in manipulating model behavior.Lack of verification for explanation on original model. To address theseissues, we propose a novel knowledge-aware neuron interpretation framework toexplain model predictions for image scene classification. Specifically, forconcept completeness, we present core concepts of a scene based on knowledgegraph, ConceptNet, to gauge the completeness of concepts. Our method,incorporating complete concepts, effectively provides better predictionexplanations compared to baselines. Furthermore, for concept fusion, weintroduce a knowledge graph-based method known as Concept Filtering, whichproduces over 23% point gain on neuron behaviors for neuron interpretation. Atlast, we propose Model Manipulation, which aims to study whether the coreconcepts based on ConceptNet could be employed to manipulate model behavior.The results show that core concepts can effectively improve the performance oforiginal model by over 26%."
    },
    {
        "link": "https://arxiv.org/abs/2401.15824",
        "title": "Innovation-triggered Learning for Data-driven Predictive Control: Deterministic and Stochastic Formulations",
        "authors": [
            "Kaikai Zheng",
            "Dawei Shi",
            "Sandra Hirche",
            "Yang Shi"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Data-driven control has attracted lots of attention in recent years,especially for plants that are difficult to model based on first-principle. Inparticular, a key issue in data-driven approaches is how to make efficient useof data as the abundance of data becomes overwhelming. {To address this issue,this work proposes an innovation-triggered learning framework and acorresponding data-driven controller design approach with guaranteed stability.Specifically, we consider a linear time-invariant system with unknown dynamicssubject to deterministic/stochastic disturbances, respectively. Two kinds ofdata selection mechanisms are proposed by online evaluating the innovationcontained in the sampled data, wherein the innovation is quantified by itseffect of shrinking the set of potential system dynamics that are compatiblewith the sampled data. Next, after introducing a stability criterion using theset-valued estimation of system dynamics, a robust data-driven predictivecontroller is designed by minimizing a worst-case cost function.} Theclosed-loop stability of the data-driven predictive controller equipped withthe innovation-triggered learning protocol is proved with a high probabilityframework. Finally, numerical experiments are performed to verify the validityof the proposed approaches, and the characteristics and the selection principleof the learning hyper-parameter are also discussed."
    },
    {
        "link": "https://arxiv.org/abs/2401.15828",
        "title": "The Spectre of Surveillance and Censorship in Future Internet Architectures",
        "authors": [
            "Michael Wrana",
            "Diogo Barradas",
            "N. Asokan"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Recent initiatives known as Future Internet Architectures (FIAs) seek toredesign the Internet to improve performance, scalability, and security.However, some governments perceive Internet access as a threat to theirpolitical standing and engage in widespread network surveillance andcensorship. In this paper, we provide an in-depth analysis into the designs ofprominent FIAs, to help understand of how FIAs impact surveillance andcensorship abilities. Then, we survey the applicability of privacy-enhancingtechnologies to FIAs. We conclude by providing guidelines for future researchinto novel FIA-based privacy-enhancing technologies, and recommendations toguide the evaluation of these technologies."
    },
    {
        "link": "https://arxiv.org/abs/2401.15834",
        "title": "Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes",
        "authors": [
            "Raphael Lafargue",
            "Yassir Bendou",
            "Bastien Pasdeloup",
            "Jean-Philippe Diguet",
            "Ian Reid",
            "Vincent Gripon",
            "Jack Valmadre"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "When training data is scarce, it is common to make use of a feature extractorthat has been pre-trained on a large base dataset, either by fine-tuning itsparameters on the ``target'' dataset or by directly adopting its representationas features for a simple classifier. Fine-tuning is ineffective for few-shotlearning, since the target dataset contains only a handful of examples.However, directly adopting the features without fine-tuning relies on the baseand target distributions being similar enough that these features achieveseparability and generalization. This paper investigates whether betterfeatures for the target dataset can be obtained by training on fewer baseclasses, seeking to identify a more useful base dataset for a given task.Weconsider cross-domain few-shot image classification in eight different domainsfrom Meta-Dataset and entertain multiple real-world settings (domain-informed,task-informed and uninformed) where progressively less detail is known aboutthe target task. To our knowledge, this is the first demonstration thatfine-tuning on a subset of carefully selected base classes can significantlyimprove few-shot learning. Our contributions are simple and intuitive methodsthat can be implemented in any few-shot solution. We also give insights intothe conditions in which these solutions are likely to provide a boost inaccuracy. We release the code to reproduce all experiments from this paper onGitHub. https://github.com/RafLaf/Few-and-Fewer.git"
    },
    {
        "link": "https://arxiv.org/abs/2401.15836",
        "title": "Refreshable Tactile Displays for Accessible Data Visualisation",
        "authors": [
            "Leona Holloway",
            "Peter Cracknell",
            "Kate Stephens",
            "Melissa Fanshawe",
            "Samuel Reinders",
            "Kim Marriott",
            "Matthew Butler"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Refreshable tactile displays (RTDs) are predicted to soon become a viableoption for the provision of accessible graphics for people who are blind orhave low vision (BLV). This new technology for the tactile display of brailleand graphics, usually using raised pins, makes it easier to generate and accessa large number of graphics. However, it differs from existing tactile graphicsin terms of scale, height and fidelity. Here, we share the perspectives of fourkey stakeholders -- blind touch readers, vision specialist teachers, accessibleformat producers and assistive technology providers -- to explore the potentialuses, advantages and needs relating to the introduction of RTDs. We alsoprovide advice on what role the data visualisation community can take to helpensure that people who are BLV are best able to benefit from the introductionof affordable RTDs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15839",
        "title": "Swarm: Cost-Efficient Video Content Distribution with a Peer-to-Peer System",
        "authors": [
            "Dehui Wei",
            "Jiao Zhang",
            "Haozhe Li",
            "Zhichen Xue",
            "Yajie Peng",
            "Xiaofei Pang",
            "Rui Han",
            "Yan Ma",
            "Jialin Li"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "As ByteDance's business expands, the substantial infrastructure expensesassociated with centralized Content Delivery Network (CDN) networks haverendered content distribution costs prohibitively high. In response, weembarked on exploring a peer-to-peer (P2P) network as a promising solution toalleviate the escalating costs of content distribution. However, thedecentralized nature of P2P often introduces performance challenges, given thediversity and dispersion of peer devices. This study introduces Swarm,ByteDance's innovative hybrid system for video streaming. Swarm seamlesslyintegrates the robustness of a conventional CDN with the cost-efficiency of adecentralized P2P network. Its primary aim is to provide users with reliablestreaming quality while minimizing traffic expenses. To achieve this, Swarmemploys a centralized control plane comprised of a tracker cluster, overseeinga data plane with numerous edge residual resources. The tracker also takes onthe responsibility of mapping clients to servers. Addressing the performancedisparities among individual peer servers, Swarm utilizes our proprietarymultipath parallel transmission method for communication between clients andpeer servers. Operating stably for six years, Swarm now manages over a hundredthousand peer servers, serving nearly a hundred million users daily and savingthe company hundreds of millions of RMB annually. Experimental results affirmthat, while significantly cutting costs, Swarm performs on par with traditionalCDNs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15840",
        "title": "Emergent Explainability: Adding a causal chain to neural network inference",
        "authors": [
            "Adam Perrett"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This position paper presents a theoretical framework for enhancingexplainable artificial intelligence (xAI) through emergent communication(EmCom), focusing on creating a causal understanding of AI model outputs. Weexplore the novel integration of EmCom into AI systems, offering a paradigmshift from conventional associative relationships between inputs and outputs toa more nuanced, causal interpretation. The framework aims to revolutionize howAI processes are understood, making them more transparent and interpretable.While the initial application of this model is demonstrated on synthetic data,the implications of this research extend beyond these simple applications. Thisgeneral approach has the potential to redefine interactions with AI acrossmultiple domains, fostering trust and informed decision-making in healthcareand in various sectors where AI's decision-making processes are critical. Thepaper discusses the theoretical underpinnings of this approach, its potentialbroad applications, and its alignment with the growing need for responsible andtransparent AI systems in an increasingly digital world."
    },
    {
        "link": "https://arxiv.org/abs/2401.15841",
        "title": "2L3: Lifting Imperfect Generated 2D Images into Accurate 3D",
        "authors": [
            "Yizheng Chen",
            "Rengan Xie",
            "Qi Ye",
            "Sen Yang",
            "Zixuan Xie",
            "Tianxiao Chen",
            "Rong Li",
            "Yuchi Huo"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Reconstructing 3D objects from a single image is an intriguing butchallenging problem. One promising solution is to utilize multi-view (MV) 3Dreconstruction to fuse generated MV images into consistent 3D objects. However,the generated images usually suffer from inconsistent lighting, misalignedgeometry, and sparse views, leading to poor reconstruction quality. To copewith these problems, we present a novel 3D reconstruction framework thatleverages intrinsic decomposition guidance, transient-mono prior guidance, andview augmentation to cope with the three issues, respectively. Specifically, wefirst leverage to decouple the shading information from the generated images toreduce the impact of inconsistent lighting; then, we introduce mono prior withview-dependent transient encoding to enhance the reconstructed normal; andfinally, we design a view augmentation fusion strategy that minimizespixel-level loss in generated sparse views and semantic loss in augmentedrandom views, resulting in view-consistent geometry and detailed textures. Ourapproach, therefore, enables the integration of a pre-trained MV imagegenerator and a neural network-based volumetric signed distance function (SDF)representation for a single image to 3D object reconstruction. We evaluate ourframework on various datasets and demonstrate its superior performance in bothquantitative and qualitative assessments, signifying a significant advancementin 3D object reconstruction. Compared with the latest state-of-the-art methodSyncdreamer~\\cite{liu2023syncdreamer}, we reduce the Chamfer Distance error byabout 36\\% and improve PSNR by about 30\\% ."
    },
    {
        "link": "https://arxiv.org/abs/2401.15842",
        "title": "LCVO: An Efficient Pretraining-Free Framework for Visual Question Answering Grounding",
        "authors": [
            "Yuhan Chen",
            "Lumei Su",
            "Lihua Chen",
            "Zhiwei Lin"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this paper, the LCVO modular method is proposed for the Visual QuestionAnswering (VQA) Grounding task in the vision-language multimodal domain. Thisapproach relies on a frozen large language model (LLM) as intermediate mediatorbetween the off-the-shelf VQA model and the off-the-shelf Open-VocabularyObject Detection (OVD) model, where the LLM transforms and conveys textualinformation between the two modules based on a designed prompt. LCVO establishan integrated plug-and-play framework without the need for any pre-trainingprocess. This framework can be deployed for VQA Grounding tasks under lowcomputational resources. The modularized model within the framework allowsapplication with various state-of-the-art pre-trained models, exhibitingsignificant potential to be advance with the times. Experimentalimplementations were conducted under constrained computational and memoryresources, evaluating the proposed method's performance on benchmark datasetsincluding GQA, CLEVR, and VizWiz-VQA-Grounding. Comparative analyses withbaseline methods demonstrate the robust competitiveness of LCVO."
    },
    {
        "link": "https://arxiv.org/abs/2401.15843",
        "title": "APIGen: Generative API Method Recommendation",
        "authors": [
            "Yujia Chen",
            "Cuiyun Gao",
            "Muyijie Zhu",
            "Qing Liao",
            "Yong Wang",
            "Guoai Xu"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Automatic API method recommendation is an essential task of codeintelligence, which aims to suggest suitable APIs for programming queries.Existing approaches can be categorized into two primary groups: retrieval-basedand learning-based approaches. Although these approaches have achievedremarkable success, they still come with notable limitations. Theretrieval-based approaches rely on the text representation capabilities ofembedding models, while the learning-based approaches require extensivetask-specific labeled data for training. To mitigate the limitations, wepropose APIGen, a generative API recommendation approach through enhancedin-context learning (ICL). APIGen involves two main components: (1) DiverseExamples Selection. APIGen searches for similar posts to the programmingqueries from the lexical, syntactical, and semantic perspectives, providingmore informative examples for ICL. (2) Guided API Recommendation. APIGenenables large language models (LLMs) to perform reasoning before generating APIrecommendations, where the reasoning involves fine-grained matching between thetask intent behind the queries and the factual knowledge of the APIs. With thereasoning process, APIGen makes recommended APIs better meet the programmingrequirement of queries and also enhances the interpretability of results. Wecompare APIGen with four existing approaches on two publicly availablebenchmarks. Experiments show that APIGen outperforms the best baseline CLEAR by105.8% in method-level API recommendation and 54.3% in class-level APIrecommendation in terms of SuccessRate@1. Besides, APIGen achieves an average49.87% increase compared to the zero-shot performance of popular LLMs such asGPT-4 in method-level API recommendation regarding the SuccessRate@3 metric."
    },
    {
        "link": "https://arxiv.org/abs/2401.15844",
        "title": "Cross-Layer Performance Evaluation of C-V2X",
        "authors": [
            "Dhruba Sunuwar",
            "Seungmo Kim"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "As self-driving cars increasingly penetrate our daily lives,vehicle-to-everything (V2X) communications are emerging as one of the keyenabler technologies. However, the dynamicity of vehicles (one of whose causesis the mobility of vehicles) often complicates it even further to evaluate theperformance of a V2X system. We have been building a system-level simulatordedicated to assessing the performance of V2X communications. We highlight thatthe simulator features the incorporation of (i) intelligent transportationsystem (ITS) scenarios in geographical setup and (ii) physical (PHY) and radioresource control (RRC) cross-layer performance evaluation capability. Inparticular, this abstract reports the status of our implementation of themodulation and coding scheme (MCS)."
    },
    {
        "link": "https://arxiv.org/abs/2401.15846",
        "title": "Meta-Learning for Neural Network-based Temporal Point Processes",
        "authors": [
            "Yoshiaki Takimoto",
            "Yusuke Tanaka",
            "Tomoharu Iwata",
            "Maya Okawa",
            "Hideaki Kim",
            "Hiroyuki Toda",
            "Takeshi Kurashima"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Human activities generate various event sequences such as taxi trip records,bike-sharing pick-ups, crime occurrence, and infectious disease transmission.The point process is widely used in many applications to predict such eventsrelated to human activities. However, point processes present two problems inpredicting events related to human activities. First, recent high-performancepoint process models require the input of sufficient numbers of eventscollected over a long period (i.e., long sequences) for training, which areoften unavailable in realistic situations. Second, the long-term predictionsrequired in real-world applications are difficult. To tackle these problems, wepropose a novel meta-learning approach for periodicity-aware prediction offuture events given short sequences. The proposed method first embeds shortsequences into hidden representations (i.e., task representations) viarecurrent neural networks for creating predictions from short sequences. Itthen models the intensity of the point process by monotonic neural networks(MNNs), with the input being the task representations. We transfer the priorknowledge learned from related tasks and can improve event prediction givenshort sequences of target tasks. We design the MNNs to explicitly take temporalperiodic patterns into account, contributing to improved long-term predictionperformance. Experiments on multiple real-world datasets demonstrate that theproposed method has higher prediction performance than existing alternatives."
    },
    {
        "link": "https://arxiv.org/abs/2401.15847",
        "title": "Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA",
        "authors": [
            "Yue Fan",
            "Jing Gu",
            "Kaiwen Zhou",
            "Qianqi Yan",
            "Shan Jiang",
            "Ching-Chen Kuo",
            "Xinze Guan",
            "Xin Eric Wang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multipanel images, commonly seen as web screenshots, posters, etc., pervadeour daily lives. These images, characterized by their composition of multiplesubfigures in distinct layouts, effectively convey information to people.Toward building advanced multimodal AI applications, such as agents thatunderstand complex scenes and navigate through webpages, the skill ofmultipanel visual reasoning is essential, and a comprehensive evaluation ofmodels in this regard is important. Therefore, our paper introduces MultipanelVisual Question Answering (MultipanelVQA), a novel benchmark that specificallychallenges models in comprehending multipanel images. The benchmark comprises6,600 questions and answers related to multipanel images. While these questionsare straightforward for average humans, achieving nearly perfect correctness,they pose significant challenges to the state-of-the-art Large Vision LanguageModels (LVLMs) we tested. In our study, we utilized synthetically curatedmultipanel images specifically designed to isolate and evaluate the impact ofdiverse factors on model performance, revealing the sensitivity of LVLMs tovarious interferences in multipanel images, such as adjacent subfigures andlayout complexity. As a result, MultipanelVQA highlights the need and directionfor improving LVLMs' ability to understand complex visual-language contexts.Code and data are released at https://sites.google.com/view/multipanelvqa/home."
    },
    {
        "link": "https://arxiv.org/abs/2401.15848",
        "title": "Deep Reinforcement Learning for Voltage Control and Renewable Accommodation Using Spatial-Temporal Graph Information",
        "authors": [
            "Jinhao Li",
            "Ruichang Zhang",
            "Hao Wang",
            "Zhi Liu",
            "Hongyang Lai",
            "Yanru Zhang"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Renewable energy resources (RERs) have been increasingly integrated intodistribution networks (DNs) for decarbonization. However, the variable natureof RERs introduces uncertainties to DNs, frequently resulting in voltagefluctuations that threaten system security and hamper the further adoption ofRERs. To incentivize more RER penetration, we propose a deep reinforcementlearning (DRL)-based strategy to dynamically balance the trade-off betweenvoltage fluctuation control and renewable accommodation. To further extractmulti-time-scale spatial-temporal (ST) graphical information of a DN, ourstrategy draws on a multi-grained attention-based spatial-temporal graphconvolution network (MG-ASTGCN), consisting of ST attention mechanism and STconvolution to explore the node correlations in the spatial and temporal views.The continuous decision-making process of balancing such a trade-off can bemodeled as a Markov decision process optimized by the deep deterministic policygradient (DDPG) algorithm with the help of the derived ST information. Wevalidate our strategy on the modified IEEE 33, 69, and 118-bus radialdistribution systems, with outcomes significantly outperforming theoptimization-based benchmarks. Simulations also reveal that our developedMG-ASTGCN can to a great extent accelerate the convergence speed of DDPG andimprove its performance in stabilizing node voltage in an RER-rich DN.Moreover, our method improves the DN's robustness in the presence of generatorfailures."
    },
    {
        "link": "https://arxiv.org/abs/2401.15853",
        "title": "Attentive Convolutional Deep Reinforcement Learning for Optimizing Solar-Storage Systems in Real-Time Electricity Markets",
        "authors": [
            "Jinhao Li",
            "Changlong Wang",
            "Hao Wang"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "This paper studies the synergy of solar-battery energy storage system (BESS)and develops a viable strategy for the BESS to unlock its economic potential byserving as a backup to reduce solar curtailments while also participating inthe electricity market. We model the real-time bidding of the solar-batterysystem as two Markov decision processes for the solar farm and the BESS,respectively. We develop a novel deep reinforcement learning (DRL) algorithm tosolve the problem by leveraging attention mechanism (AC) and multi-grainedfeature convolution to process DRL input for better bidding decisions.Simulation results demonstrate that our AC-DRL outperforms twooptimization-based and one DRL-based benchmarks by generating 23%, 20%, and 11%higher revenue, as well as improving curtailment responses. The excess solargeneration can effectively charge the BESS to bid in the market, significantlyreducing solar curtailments by 76% and creating synergy for the solar-batterysystem to be more viable."
    },
    {
        "link": "https://arxiv.org/abs/2401.15854",
        "title": "LSTM-based Deep Neural Network With A Focus on Sentence Representation for Sequential Sentence Classification in Medical Scientific Abstracts",
        "authors": [
            "Phat Lam",
            "Lam Pham",
            "Tin Nguyen",
            "Hieu Tang",
            "Seidl Michael",
            "Alexander Schindler"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "The Sequential Sentence Classification task within the domain of medicalabstracts, termed as SSC, involves the categorization of sentences intopre-defined headings based on their roles in conveying critical information inthe abstract. In the SSC task, sentences are often sequentially related to eachother. For this reason, the role of sentence embedding is crucial for capturingboth the semantic information between words in the sentence and the contextualrelationship of sentences within the abstract to provide a comprehensiverepresentation for better classification. In this paper, we present ahierarchical deep learning model for the SSC task. First, we propose aLSTM-based network with multiple feature branches to create well-presentedsentence embeddings at the sentence level. To perform the sequence ofsentences, a convolutional-recurrent neural network (C-RNN) at the abstractlevel and a multi-layer perception network (MLP) at the segment level aredeveloped that further enhance the model performance. Additionally, an ablationstudy is also conducted to evaluate the contribution of individual component inthe entire network to the model performance at different levels. Our proposedsystem is very competitive to the state-of-the-art systems and further improveF1 scores of the baseline by 1.0%, 2.8%, and 2.6% on the benchmark datasetsPudMed 200K RCT, PudMed 20K RCT and NICTA-PIBOSO, respectively."
    },
    {
        "link": "https://arxiv.org/abs/2401.15855",
        "title": "Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing",
        "authors": [
            "Maofeng Tang",
            "Andrei Cozma",
            "Konstantinos Georgiou",
            "Hairong Qi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Remote sensing images present unique challenges to image analysis due to theextensive geographic coverage, hardware limitations, and misaligned multi-scaleimages. This paper revisits the classical multi-scale representation learningproblem but under the general framework of self-supervised learning for remotesensing image understanding. We present Cross-Scale MAE, a self-supervisedmodel built upon the Masked Auto-Encoder (MAE).During pre-training, Cross-ScaleMAE employs scale augmentation techniques and enforces cross-scale consistencyconstraints through both contrastive and generative losses to ensure consistentand meaningful representations well-suited for a wide range of downstreamtasks. Further, our implementation leverages the xFormers library to acceleratenetwork pre-training on a single GPU while maintaining the quality of learnedrepresentations. Experimental evaluations demonstrate that Cross-Scale MAEexhibits superior performance compared to standard MAE and otherstate-of-the-art remote sensing MAE methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15856",
        "title": "Look Around! Unexpected gains from training on environments in the vicinity of the target",
        "authors": [
            "Serena Bono",
            "Spandan Madan",
            "Ishaan Grover",
            "Mao Yasueda",
            "Cynthia Breazeal",
            "Hanspeter Pfister",
            "Gabriel Kreiman"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Solutions to Markov Decision Processes (MDP) are often very sensitive tostate transition probabilities. As the estimation of these probabilities isoften inaccurate in practice, it is important to understand when and howReinforcement Learning (RL) agents generalize when transition probabilitieschange. Here we present a new methodology to evaluate such generalization of RLagents under small shifts in the transition probabilities. Specifically, weevaluate agents in new environments (MDPs) in the vicinity of the training MDPcreated by adding quantifiable, parametric noise into the transition functionof the training MDP. We refer to this process as Noise Injection, and theresulting environments as \u03b4-environments. This process allows us tocreate controlled variations of the same environment with the level of thenoise serving as a metric of distance between environments. Conventional wisdomsuggests that training and testing on the same MDP should yield the bestresults. However, we report several cases of the opposite -- when targeting aspecific environment, training the agent in an alternative noise setting canyield superior outcomes. We showcase this phenomenon across 60 differentvariations of ATARI games, including PacMan, Pong, and Breakout."
    },
    {
        "link": "https://arxiv.org/abs/2401.15857",
        "title": "Leadership Dynamics in Social Multiplex Networks with Mono and Bi-directional Interactions",
        "authors": [
            "Amirreza Talebi"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "We explored the dynamics of opinions within a multiplex network, where agentsengage in one-way or two-way communication, and the network may have a des-ignated leader. Additionally, we demonstrated that, under specific conditions,opinions tend to converge despite non-positive diagonal elements in transitionprobability matrices or decomposable layers. Lastly, we contrasted the conver-gence rates of opinion dynamics in networks with one-way interactions againstthose with two-way interactions, revealing that one-way interactions mayfacili- tate faster convergence outcomes. Additionally, we shed light on thepivotal role of designated leaders in steering opinion convergence within thenetwork."
    },
    {
        "link": "https://arxiv.org/abs/2401.15859",
        "title": "Diffusion Facial Forgery Detection",
        "authors": [
            "Harry Cheng",
            "Yangyang Guo",
            "Tianyi Wang",
            "Liqiang Nie",
            "Mohan Kankanhalli"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Detecting diffusion-generated images has recently grown into an emergingresearch area. Existing diffusion-based datasets predominantly focus on generalimage generation. However, facial forgeries, which pose a more severe socialrisk, have remained less explored thus far. To address this gap, this paperintroduces DiFF, a comprehensive dataset dedicated to face-focuseddiffusion-generated images. DiFF comprises over 500,000 images that aresynthesized using thirteen distinct generation methods under four conditions.In particular, this dataset leverages 30,000 carefully collected textual andvisual prompts, ensuring the synthesis of images with both high fidelity andsemantic consistency. We conduct extensive experiments on the DiFF dataset viaa human test and several representative forgery detection methods. The resultsdemonstrate that the binary detection accuracy of both human observers andautomated detectors often falls below 30%, shedding light on the challenges indetecting diffusion-generated facial forgeries. Furthermore, we propose an edgegraph regularization approach to effectively enhance the generalizationcapability of existing detectors."
    },
    {
        "link": "https://arxiv.org/abs/2401.15861",
        "title": "DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining",
        "authors": [
            "Wen Liang",
            "Youzhi Liang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "BERT (Bidirectional Encoder Representations from Transformers) hasrevolutionized the field of natural language processing through its exceptionalperformance on numerous tasks. Yet, the majority of researchers have mainlyconcentrated on enhancements related to the model structure, such as relativeposition embedding and more efficient attention mechanisms. Others have delvedinto pretraining tricks associated with Masked Language Modeling, includingwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT'sencoder model for pretraining, proving to be highly effective. We argue thatthe design and research around enhanced masked language modeling decoders havebeen underappreciated. In this paper, we propose several designs of enhanceddecoders and introduce DrBERT (Decoder-refined BERT), a novel method formodeling training. Typically, a pretrained BERT model is fine-tuned forspecific Natural Language Understanding (NLU) tasks. In our approach, weutilize the original BERT model as the encoder, making only changes to thedecoder without altering the encoder. This approach does not necessitateextensive modifications to the model's architecture and can be seamlesslyintegrated into existing fine-tuning pipelines and services, offering anefficient and effective enhancement strategy. Compared to other methods, whilewe also incur a moderate training cost for the decoder during the pretrainingprocess, our approach does not introduce additional training costs during thefine-tuning phase. We test multiple enhanced decoder structures afterpretraining and evaluate their performance on the GLUE benchmark. Our resultsdemonstrate that DrBERT, having only undergone subtle refinements to the modelstructure during pretraining, significantly enhances model performance withoutescalating the inference time and serving budget."
    },
    {
        "link": "https://arxiv.org/abs/2401.15862",
        "title": "PML-based boundary integral equation method for electromagnetic scattering problems in a layered-medium",
        "authors": [
            "Gang Bao",
            "Wangtao Lu",
            "Tao Yin",
            "Lu Zhang"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This paper proposes a new boundary integral equation (BIE) methodology basedon the perfectly matched layer (PML) truncation technique for solving theelectromagnetic scattering problems in a multi-layered medium. Instead of usingthe original PML stretched fields, artificial fields which are also equivalentto the solutions in the physical region are introduced. This significantlysimplifies the study of the proposed methodology to derive the PML problem.Then some PML transformed layer potentials and the associated boundary integraloperators (BIOs) are defined and the corresponding jump relations are shown.Under the assumption that the fields vanish on the PML boundary, the solutionrepresentations, as well as the related BIEs and regularization of thehyper-singular operators, in terms of the current density functions on thetruncated interface, are derived. Numerical experiments are presented todemonstrate the efficiency and accuracy of the method."
    },
    {
        "link": "https://arxiv.org/abs/2401.15863",
        "title": "Importance-Aware Adaptive Dataset Distillation",
        "authors": [
            "Guang Li",
            "Ren Togo",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Herein, we propose a novel dataset distillation method for constructing smallinformative datasets that preserve the information of the large originaldatasets. The development of deep learning models is enabled by theavailability of large-scale datasets. Despite unprecedented success,large-scale datasets considerably increase the storage and transmission costs,resulting in a cumbersome model training process. Moreover, using raw data fortraining raises privacy and copyright concerns. To address these issues, a newtask named dataset distillation has been introduced, aiming to synthesize acompact dataset that retains the essential information from the large originaldataset. State-of-the-art (SOTA) dataset distillation methods have beenproposed by matching gradients or network parameters obtained during trainingon real and synthetic datasets. The contribution of different networkparameters to the distillation process varies, and uniformly treating themleads to degraded distillation performance. Based on this observation, wepropose an importance-aware adaptive dataset distillation (IADD) method thatcan improve distillation performance by automatically assigning importanceweights to different network parameters during distillation, therebysynthesizing more robust distilled datasets. IADD demonstrates superiorperformance over other SOTA dataset distillation methods based on parametermatching on multiple benchmark datasets and outperforms them in terms ofcross-architecture generalization. In addition, the analysis of self-adaptiveweights demonstrates the effectiveness of IADD. Furthermore, the effectivenessof IADD is validated in a real-world medical application such as COVID-19detection."
    },
    {
        "link": "https://arxiv.org/abs/2401.15864",
        "title": "Spatial Decomposition and Temporal Fusion based Inter Prediction for Learned Video Compression",
        "authors": [
            "Xihua Sheng",
            "Li Li",
            "Dong Liu",
            "Houqiang Li"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Video compression performance is closely related to the accuracy of interprediction. It tends to be difficult to obtain accurate inter prediction forthe local video regions with inconsistent motion and occlusion. Traditionalvideo coding standards propose various technologies to handle motioninconsistency and occlusion, such as recursive partitions, geometricpartitions, and long-term references. However, existing learned videocompression schemes focus on obtaining an overall minimized prediction erroraveraged over all regions while ignoring the motion inconsistency and occlusionin local regions. In this paper, we propose a spatial decomposition andtemporal fusion based inter prediction for learned video compression. To handlemotion inconsistency, we propose to decompose the video into structure anddetail (SDD) components first. Then we perform SDD-based motion estimation andSDD-based temporal context mining for the structure and detail components togenerate short-term temporal contexts. To handle occlusion, we propose topropagate long-term temporal contexts by recurrently accumulating the temporalinformation of each historical reference feature and fuse them with short-termtemporal contexts. With the SDD-based motion model and long short-term temporalcontexts fusion, our proposed learned video codec can obtain more accurateinter prediction. Comprehensive experimental results demonstrate that our codecoutperforms the reference software of H.266/VVC on all common test datasets forboth PSNR and MS-SSIM."
    },
    {
        "link": "https://arxiv.org/abs/2401.15865",
        "title": "LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection",
        "authors": [
            "Sifan Zhou",
            "Liang Li",
            "Xinyu Zhang",
            "Bo Zhang",
            "Shipeng Bai",
            "Miao Sun",
            "Ziyu Zhao",
            "Xiaobo Lu",
            "Xiangxiang Chu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Due to highly constrained computing power and memory, deploying 3Dlidar-based detectors on edge devices equipped in autonomous vehicles androbots poses a crucial challenge. Being a convenient and straightforward modelcompression approach, Post-Training Quantization (PTQ) has been widely adoptedin 2D vision tasks. However, applying it directly to 3D lidar-based tasksinevitably leads to performance degradation. As a remedy, we propose aneffective PTQ method called LiDAR-PTQ, which is particularly curated for 3Dlidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ featuresthree main components, \\textbf{(1)} a sparsity-based calibration method todetermine the initialization of quantization parameters, \\textbf{(2)} aTask-guided Global Positive Loss (TGPL) to reduce the disparity between thefinal predictions before and after quantization, \\textbf{(3)} an adaptiverounding-to-nearest operation to minimize the layerwise reconstruction error.Extensive experiments demonstrate that our LiDAR-PTQ can achievestate-of-the-art quantization performance when applied to CenterPoint (bothPillar-based and Voxel-based). To our knowledge, for the very first time inlidar-based 3D detection tasks, the PTQ INT8 model's accuracy is almost thesame as the FP32 model while enjoying 3\u00d7 inference speedup. Moreover,our LiDAR-PTQ is cost-effective being 30\u00d7 faster than thequantization-aware training method. Code will be released at\\url{https://github.com/StiphyJay/LiDAR-PTQ}."
    },
    {
        "link": "https://arxiv.org/abs/2401.15866",
        "title": "Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution",
        "authors": [
            "Ian Covert",
            "Chanwoo Kim",
            "Su-In Lee",
            "James Zou",
            "Tatsunori Hashimoto"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Many tasks in explainable machine learning, such as data valuation andfeature attribution, perform expensive computation for each data point and canbe intractable for large datasets. These methods require efficientapproximations, and learning a network that directly predicts the desiredoutput, which is commonly known as amortization, is a promising solution.However, training such models with exact labels is often intractable; wetherefore explore training with noisy labels and find that this is inexpensiveand surprisingly effective. Through theoretical analysis of the label noise andexperiments with various models and datasets, we show that this approachsignificantly accelerates several feature attribution and data valuationmethods, often yielding an order of magnitude speedup over existing approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15867",
        "title": "An Information Aggregation Operator",
        "authors": [
            "Heyang Gong"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This study explores a new mathematical operator, symbolized as $\\cupplus$,for information aggregation, aimed at enhancing traditional methods by directlyamalgamating probability distributions. This operator facilitates thecombination of probability densities, contributing a nuanced approach toprobabilistic analysis. We apply this operator to a personalized incentivescenario, illustrating its potential in a practical context. The paper'sprimary contribution lies in introducing this operator and elucidating itselegant mathematical properties. This exploratory work marks a step forward inthe field of information fusion and probabilistic reasoning."
    },
    {
        "link": "https://arxiv.org/abs/2401.15869",
        "title": "Quantum Circuit Reconstruction from Power Side-Channel Attacks on Quantum Computer Controllers",
        "authors": [
            "Ferhat Erata",
            "Chuanqi Xu",
            "Ruzica Piskac",
            "Jakub Szefer"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "The interest in quantum computing has grown rapidly in recent years, and withit grows the importance of securing quantum circuits. A novel type of threat toquantum circuits that dedicated attackers could launch are power trace attacks.To address this threat, this paper presents first formalization anddemonstration of using power traces to unlock and steal quantum circuitsecrets. With access to power traces, attackers can recover information aboutthe control pulses sent to quantum computers. From the control pulses, the gatelevel description of the circuits, and eventually the secret algorithms can bereverse engineered. This work demonstrates how and what information could berecovered. This work uses algebraic reconstruction from power traces to realizetwo new types of single trace attacks: per-channel and total power attacks. Theformer attack relies on per-channel measurements to perform a brute-forceattack to reconstruct the quantum circuits. The latter attack performs asingle-trace attack using Mixed-Integer Linear Programming optimization.Through the use of algebraic reconstruction, this work demonstrates thatquantum circuit secrets can be stolen with high accuracy. Evaluation on 32 realbenchmark quantum circuits shows that our technique is highly effective atreconstructing quantum circuits. The findings not only show the veracity of thepotential attacks, but also the need to develop new means to protect quantumcircuits from power trace attacks. Throughout this work real control pulseinformation from real quantum computers is used to demonstrate potentialattacks based on simulation of collection of power traces."
    },
    {
        "link": "https://arxiv.org/abs/2401.15870",
        "title": "DF* PageRank: Improved Incrementally Expanding Approaches for Updating PageRank on Dynamic Graphs",
        "authors": [
            "Subhajit Sahu"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "PageRank is a widely used centrality measure that assesses the significanceof vertices in a graph by considering their connections and the importance ofthose connections. Efficiently updating PageRank on dynamic graphs is essentialfor various applications due to the increasing scale of datasets. Thistechnical report introduces our improved Dynamic Frontier (DF) and DynamicFrontier with Pruning (DF-P) approaches. Given a batch update comprising edgeinsertions and deletions, these approaches iteratively identify vertices likelyto change their ranks with minimal overhead. On a server featuring a 64-coreAMD EPYC-7742 processor, our approaches outperform Static and Dynamic TraversalPageRank by 5.2x/15.2x and 1.3x/3.5x respectively - on real-world dynamicgraphs, and by 7.2x/9.6x and 4.0x/5.6x on large static graphs with random batchupdates. Furthermore, our approaches improve performance at a rate of 1.8x/1.7xfor every doubling of threads."
    },
    {
        "link": "https://arxiv.org/abs/2401.15872",
        "title": "A Deep Q-Network Based on Radial Basis Functions for Multi-Echelon Inventory Management",
        "authors": [
            "Liqiang Cheng",
            "Jun Luo",
            "Weiwei Fan",
            "Yidong Zhang",
            "Yuan Li"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "This paper addresses a multi-echelon inventory management problem with acomplex network topology where deriving optimal ordering decisions isdifficult. Deep reinforcement learning (DRL) has recently shown potential insolving such problems, while designing the neural networks in DRL remains achallenge. In order to address this, a DRL model is developed whose Q-networkis based on radial basis functions. The approach can be more easily constructedcompared to classic DRL models based on neural networks, thus alleviating thecomputational burden of hyperparameter tuning. Through a series of simulationexperiments, the superior performance of this approach is demonstrated comparedto the simple base-stock policy, producing a better policy in the multi-echelonsystem and competitive performance in the serial system where the base-stockpolicy is optimal. In addition, the approach outperforms current DRLapproaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15874",
        "title": "Rethinking Personalized Federated Learning with Clustering-based Dynamic Graph Propagation",
        "authors": [
            "Jiaqi Wang",
            "Yuzhong Chen",
            "Yuhang Wu",
            "Mahashweta Das",
            "Hao Yang",
            "Fenglong Ma"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Most existing personalized federated learning approaches are based onintricate designs, which often require complex implementation and tuning. Inorder to address this limitation, we propose a simple yet effectivepersonalized federated learning framework. Specifically, during eachcommunication round, we group clients into multiple clusters based on theirmodel training status and data distribution on the server side. We thenconsider each cluster center as a node equipped with model parameters andconstruct a graph that connects these nodes using weighted edges. Additionally,we update the model parameters at each node by propagating information acrossthe entire graph. Subsequently, we design a precise personalized modeldistribution strategy to allow clients to obtain the most suitable model fromthe server side. We conduct experiments on three image benchmark datasets andcreate synthetic structured datasets with three types of typologies.Experimental results demonstrate the effectiveness of the proposed work."
    },
    {
        "link": "https://arxiv.org/abs/2401.15875",
        "title": "Combining Satellite and Weather Data for Crop Type Mapping: An Inverse Modelling Approach",
        "authors": [
            "Praveen Ravirathinam",
            "Rahul Ghosh",
            "Ankush Khandelwal",
            "Xiaowei Jia",
            "David Mulla",
            "Vipin Kumar"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate and timely crop mapping is essential for yield estimation, insuranceclaims, and conservation efforts. Over the years, many successful machinelearning models for crop mapping have been developed that use just themulti-spectral imagery from satellites to predict crop type over the area ofinterest. However, these traditional methods do not account for the physicalprocesses that govern crop growth. At a high level, crop growth can beenvisioned as physical parameters, such as weather and soil type, acting uponthe plant leading to crop growth which can be observed via satellites. In thispaper, we propose Weather-based Spatio-Temporal segmentation network withATTention (WSTATT), a deep learning model that leverages this understanding ofcrop growth by formulating it as an inverse model that combines weather(Daymet) and satellite imagery (Sentinel-2) to generate accurate crop maps. Weshow that our approach provides significant improvements over existingalgorithms that solely rely on spectral imagery by comparing segmentation mapsand F1 classification scores. Furthermore, effective use of attention in WSTATTarchitecture enables detection of crop types earlier in the season (up to 5months in advance), which is very useful for improving food supply projections.We finally discuss the impact of weather by correlating our results with cropphenology to show that WSTATT is able to capture physical properties of cropgrowth."
    },
    {
        "link": "https://arxiv.org/abs/2401.15876",
        "title": "CMA-ES with Learning Rate Adaptation",
        "authors": [
            "Masahiro Nomura",
            "Youhei Akimoto",
            "Isao Ono"
        ],
        "primary_subject": "Neural and Evolutionary Computing (cs.NE)",
        "abstract": "The covariance matrix adaptation evolution strategy (CMA-ES) is one of themost successful methods for solving continuous black-box optimization problems.A practically useful aspect of the CMA-ES is that it can be used withouthyperparameter tuning. However, the hyperparameter settings still have aconsiderable impact on performance, especially for difficult tasks, such assolving multimodal or noisy problems. This study comprehensively explores theimpact of learning rate on the CMA-ES performance and demonstrates thenecessity of a small learning rate by considering ordinary differentialequations. Thereafter, it discusses the setting of an ideal learning rate.Based on these discussions, we develop a novel learning rate adaptationmechanism for the CMA-ES that maintains a constant signal-to-noise ratio.Additionally, we investigate the behavior of the CMA-ES with the proposedlearning rate adaptation mechanism through numerical experiments, and comparethe results with those obtained for the CMA-ES with a fixed learning rate andwith population size adaptation. The results show that the CMA-ES with theproposed learning rate adaptation works well for multimodal and/or noisyproblems without extremely expensive learning rate tuning."
    },
    {
        "link": "https://arxiv.org/abs/2401.15877",
        "title": "3DPFIX: Improving Remote Novices' 3D Printing Troubleshooting through Human-AI Collaboration",
        "authors": [
            "Nahyun Kwon",
            "Tong Sun",
            "Yuyang Gao",
            "Liang Zhao",
            "Xu Wang",
            "Jeeeun Kim",
            "Sungsoo Ray Hong"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "The widespread consumer-grade 3D printers and learning resources onlineenable novices to self-train in remote settings. While troubleshooting plays anessential part of 3D printing, the process remains challenging for many remotenovices even with the help of well-developed online sources, such as onlinetroubleshooting archives and online community help. We conducted a formativestudy with 76 active 3D printing users to learn how remote novices leverageonline resources in troubleshooting and their challenges. We found that remotenovices cannot fully utilize online resources. For example, the online archivesstatically provide general information, making it hard to search and relatetheir unique cases with existing descriptions. Online communities canpotentially ease their struggles by providing more targeted suggestions, but ahelper who can provide custom help is rather scarce, making it hard to obtaintimely assistance. We propose 3DPFIX, an interactive 3D troubleshooting systempowered by the pipeline to facilitate Human-AI Collaboration, designed toimprove novices' 3D printing experiences and thus help them easily accumulatetheir domain knowledge. We built 3DPFIX that supports automated diagnosis andsolution-seeking. 3DPFIX was built upon shared dialogues about failure casesfrom Q\\&A discourses accumulated in online communities. We leverage socialannotations (i.e., comments) to build an annotated failure image dataset for AIclassifiers and extract a solution pool. Our summative study revealed thatusing 3DPFIX helped participants spend significantly less effort in diagnosingfailures and finding a more accurate solution than relying on their commonpractice. We also found that 3DPFIX users learn about 3D printingdomain-specific knowledge. We discuss the implications of leveragingcommunity-driven data in developing future Human-AI Collaboration designs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15878",
        "title": "Decoding the MITRE Engenuity ATT&CK Enterprise Evaluation: An Analysis of EDR Performance in Real-World Environments",
        "authors": [
            "Xiangmin Shen",
            "Zhenyuan Li",
            "Graham Burleigh",
            "Lingzhi Wang",
            "Yan Chen"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Endpoint detection and response (EDR) systems have emerged as a criticalcomponent of enterprise security solutions, effectively combating endpointthreats like APT attacks with extended lifecycles. In light of the growingsignificance of endpoint detection and response (EDR) systems, manycybersecurity providers have developed their own proprietary EDR solutions.It's crucial for users to assess the capabilities of these detection engines tomake informed decisions about which products to choose. This is especiallyurgent given the market's size, which is expected to reach around 3.7 billiondollars by 2023 and is still expanding. MITRE is a leading organization incyber threat analysis. In 2018, MITRE started to conduct annual APT emulationsthat cover major EDR vendors worldwide. Indicators include telemetry, detectionand blocking capability, etc. Nevertheless, the evaluation results published byMITRE don't contain any further interpretations or suggestions.In this paper, we thoroughly analyzed MITRE evaluation results to gainfurther insights into real-world EDR systems under test. Specifically, wedesigned a whole-graph analysis method, which utilizes additional control flowand data flow information to measure the performance of EDR systems. Besides,we analyze MITRE evaluation's results over multiple years from various aspects,including detection coverage, detection confidence, detection modifier, datasource, compatibility, etc. Through the above studies, we have compiled athorough summary of our findings and gained valuable insights from theevaluation results. We believe these summaries and insights can assistresearchers, practitioners, and vendors in better understanding the strengthsand limitations of mainstream EDR products."
    },
    {
        "link": "https://arxiv.org/abs/2401.15879",
        "title": "lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap",
        "authors": [
            "Tzu-Hsien Tsai",
            "Yun-Da Tsai",
            "Shou-De Lin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Good arm identification (GAI) is a pure-exploration bandit problem in which asingle learner outputs an arm as soon as it is identified as a good arm. A goodarm is defined as an arm with an expected reward greater than or equal to agiven threshold. This paper focuses on the GAI problem under a small thresholdgap, which refers to the distance between the expected rewards of arms and thegiven threshold. We propose a new algorithm called lil'HDoC to significantlyimprove the total sample complexity of the HDoC algorithm. We demonstrate thatthe sample complexity of the first \u03bb output arm in lil'HDoC is boundedby the original HDoC algorithm, except for one negligible term, when thedistance between the expected reward and threshold is small. Extensiveexperiments confirm that our algorithm outperforms the state-of-the-artalgorithms in both synthetic and real-world datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.15883",
        "title": "TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability",
        "authors": [
            "Hao Wang",
            "Tao Xiang",
            "Shangwei Guo",
            "Jialing He",
            "Hangcheng Liu",
            "Tianwei Zhang"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Pre-trained models (PTMs) are extensively utilized in various downstreamtasks. Adopting untrusted PTMs may suffer from backdoor attacks, where theadversary can compromise the downstream models by injecting backdoors into thePTM. However, existing backdoor attacks to PTMs can only achieve partiallytask-agnostic and the embedded backdoors are easily erased during thefine-tuning process. In this paper, we propose a novel transferable backdoorattack, TransTroj, to simultaneously meet functionality-preserving, durable,and task-agnostic. In particular, we first formalize transferable backdoorattacks as the indistinguishability problem between poisoned and clean samplesin the embedding space. We decompose the embedding indistinguishability intopre- and post-indistinguishability, representing the similarity of the poisonedand reference embeddings before and after the attack. Then, we propose atwo-stage optimization that separately optimizes triggers and victim PTMs toachieve embedding indistinguishability. We evaluate TransTroj on four PTMs andsix downstream tasks. Experimental results show that TransTroj significantlyoutperforms SOTA task-agnostic backdoor attacks (18%\u223c99%, 68% on average)and exhibits superior performance under various system settings. The code isavailable at https://github.com/haowang-cqu/TransTroj ."
    },
    {
        "link": "https://arxiv.org/abs/2401.15884",
        "title": "Corrective Retrieval Augmented Generation",
        "authors": [
            "Shi-Qi Yan",
            "Jia-Chen Gu",
            "Yun Zhu",
            "Zhen-Hua Ling"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since theaccuracy of generated texts cannot be secured solely by the parametricknowledge they encapsulate. Although retrieval-augmented generation (RAG) is apracticable complement to LLMs, it relies heavily on the relevance of retrieveddocuments, raising concerns about how the model behaves if retrieval goeswrong. To this end, we propose the Corrective Retrieval Augmented Generation(CRAG) to improve the robustness of generation. Specifically, a lightweightretrieval evaluator is designed to assess the overall quality of retrieveddocuments for a query, returning a confidence degree based on which differentknowledge retrieval actions can be triggered. Since retrieval from static andlimited corpora can only return sub-optimal documents, large-scale web searchesare utilized as an extension for augmenting the retrieval results. Besides, adecompose-then-recompose algorithm is designed for retrieved documents toselectively focus on key information and filter out irrelevant information inthem. CRAG is plug-and-play and can be seamlessly coupled with variousRAG-based approaches. Experiments on four datasets covering short- andlong-form generation tasks show that CRAG can significantly improve theperformance of RAG-based approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15885",
        "title": "Rectify the Regression Bias in Long-Tailed Object Detection",
        "authors": [
            "Ke Zhu",
            "Minghao Fu",
            "Jie Shao",
            "Tianyu Liu",
            "Jianxin Wu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Long-tailed object detection faces great challenges because of its extremelyimbalanced class distribution. Recent methods mainly focus on theclassification bias and its loss function design, while ignoring the subtleinfluence of the regression branch. This paper shows that the regression biasexists and does adversely and seriously impact the detection accuracy. Whileexisting methods fail to handle the regression bias, the class-specificregression head for rare classes is hypothesized to be the main cause of it inthis paper. As a result, three kinds of viable solutions to cater for the rarecategories are proposed, including adding a class-agnostic branch, clusteringheads and merging heads. The proposed methods brings in consistent andsignificant improvements over existing long-tailed detection methods,especially in rare and common classes. The proposed method achievesstate-of-the-art performance in the large vocabulary LVIS dataset withdifferent backbones and architectures. It generalizes well to more difficultevaluation metrics, relatively balanced datasets, and the mask branch. This isthe first attempt to reveal and explore rectifying of the regression bias inlong-tailed object detection."
    },
    {
        "link": "https://arxiv.org/abs/2401.15886",
        "title": "Grey Level Texture Features for Segmentation of Chromogenic Dye RNAscope From Breast Cancer Tissue",
        "authors": [
            "Andrew Davidson",
            "Arthur Morley-Bunker",
            "George Wiggins",
            "Logan Walker",
            "Gavin Harris",
            "Ramakrishnan Mukundan",
            "kConFab Investigators"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Chromogenic RNAscope dye and haematoxylin staining of cancer tissuefacilitates diagnosis of the cancer type and subsequent treatment, and fitswell into existing pathology workflows. However, manual quantification of theRNAscope transcripts (dots), which signify gene expression, is prohibitivelytime consuming. In addition, there is a lack of verified supporting methods forquantification and analysis. This paper investigates the usefulness of graylevel texture features for automatically segmenting and classifying thepositions of RNAscope transcripts from breast cancer tissue. Feature analysisshowed that a small set of gray level features, including Gray Level DependenceMatrix and Neighbouring Gray Tone Difference Matrix features, were well suitedfor the task. The automated method performed similarly to expert annotators atidentifying the positions of RNAscope transcripts, with an F1-score of 0.571compared to the expert inter-rater F1-score of 0.596. These results demonstratethe potential of gray level texture features for automated quantification ofRNAscope in the pathology workflow."
    },
    {
        "link": "https://arxiv.org/abs/2401.15893",
        "title": "Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation",
        "authors": [
            "Dongheon Lee",
            "Seungmyong Jeong",
            "Youngmin Ro"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Numerical models have long been used to understand geoscientific phenomena,including tidal currents, crucial for renewable energy production and coastalengineering. However, their computational cost hinders generating data ofvarying resolutions. As an alternative, deep learning-based downscaling methodshave gained traction due to their faster inference speeds. But most of them arelimited to only inference fixed scale and overlook important characteristics oftarget geoscientific data. In this paper, we propose a novel downscalingframework for tidal current data, addressing its unique characteristics, whichare dissimilar to images: heterogeneity and local dependency. Moreover, ourframework can generate any arbitrary-scale output utilizing a continuousrepresentation model. Our proposed framework demonstrates significantlyimproved flow velocity predictions by 93.21% (MSE) and 63.85% (MAE) compared tothe Baseline model while achieving a remarkable 33.2% reduction in FLOPs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15894",
        "title": "A Gated MLP Architecture for Learning Topological Dependencies in Spatio-Temporal Graphs",
        "authors": [
            "Yun Young Choi",
            "Minho Lee",
            "Sun Woo Park",
            "Seunghwan Lee",
            "Joohwan Ko"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) and Transformer have been increasingly adoptedto learn the complex vector representations of spatio-temporal graphs,capturing intricate spatio-temporal dependencies crucial for applications suchas traffic datasets. Although many existing methods utilize multi-headattention mechanisms and message-passing neural networks (MPNNs) to captureboth spatial and temporal relations, these approaches encode temporal andspatial relations independently, and reflect the graph's topologicalcharacteristics in a limited manner. In this work, we introduce the Cycle toMixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivialinvariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP).The Cy2Mixer is composed of three blocks based on MLPs: A message-passing blockfor encapsulating spatial information, a cycle message-passing block forenriching topological information through cyclic subgraphs, and a temporalblock for capturing temporal properties. We bolster the effectiveness ofCy2Mixer with mathematical evidence emphasizing that our cycle message-passingblock is capable of offering differentiated information to the deep learningmodel compared to the message-passing block. Furthermore, empirical evaluationssubstantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-artperformances across various traffic benchmark datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.15896",
        "title": "M",
        "authors": [
            "Qingpei Guo",
            "Furong Xu",
            "Hanxiao Zhang",
            "Wang Ren",
            "Ziping Ma",
            "Lin Ju",
            "Jian Wang",
            "Jingdong Chen",
            "Ming Yang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-language foundation models like CLIP have revolutionized the field ofartificial intelligence. Nevertheless, VLM models supporting multi-language,e.g., in both Chinese and English, have lagged due to the relative scarcity oflarge-scale pretraining datasets. Toward this end, we introduce a comprehensivebilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs,aimed at enhancing multimodal foundation models to well understand images inboth languages. To handle such a scale of dataset, we propose a novel groupedaggregation approach for image-text contrastive loss computation, which reducesthe communication overhead and GPU memory demands significantly, facilitating a60% increase in training speed. We pretrain a series of bilingual image-textfoundation models with an enhanced fine-grained understanding ability on BM-6B,the resulting models, dubbed as M2-Encoders (pronounced \"M-Square\"), set newbenchmarks in both languages for multimodal retrieval and classification tasks.Notably, Our largest M2-Encoder-10B model has achieved top-1 accuracies of88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classificationsetting, surpassing previously reported SoTA methods by 2.2% and 21.1%,respectively. The M2-Encoder series represents one of the most comprehensivebilingual image-text foundation models to date, so we are making it availableto the research community for further exploration and development."
    },
    {
        "link": "https://arxiv.org/abs/2401.15897",
        "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?",
        "authors": [
            "Michael Feffer",
            "Anusha Sinha",
            "Zachary C. Lipton",
            "Hoda Heidari"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "In response to rising concerns surrounding the safety, security, andtrustworthiness of Generative AI (GenAI) models, practitioners and regulatorsalike have pointed to AI red-teaming as a key component of their strategies foridentifying and mitigating these risks. However, despite AI red-teaming'scentral role in policy discussions and corporate messaging, significantquestions remain about what precisely it means, what role it can play inregulation, and how precisely it relates to conventional red-teaming practicesas originally conceived in the field of cybersecurity. In this work, weidentify recent cases of red-teaming activities in the AI industry and conductan extensive survey of the relevant research literature to characterize thescope, structure, and criteria for AI red-teaming practices. Our analysisreveals that prior methods and practices of AI red-teaming diverge alongseveral axes, including the purpose of the activity (which is often vague), theartifact under evaluation, the setting in which the activity is conducted(e.g., actors, resources, and methods), and the resulting decisions it informs(e.g., reporting, disclosure, and mitigation). In light of our findings, weargue that while red-teaming may be a valuable big-tent idea for characterizinga broad set of activities and attitudes aimed at improving the behavior ofGenAI models, gestures towards red-teaming as a panacea for every possible riskverge on security theater. To move toward a more robust toolbox of evaluationsfor generative AI, we synthesize our recommendations into a question bank meantto guide and scaffold future AI red-teaming practices."
    },
    {
        "link": "https://arxiv.org/abs/2401.15900",
        "title": "MV2MAE: Multi-View Video Masked Autoencoders",
        "authors": [
            "Ketul Shah",
            "Robert Crandall",
            "Jie Xu",
            "Peng Zhou",
            "Marian George",
            "Mayank Bansal",
            "Rama Chellappa"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Videos captured from multiple viewpoints can help in perceiving the 3Dstructure of the world and benefit computer vision tasks such as actionrecognition, tracking, etc. In this paper, we present a method forself-supervised learning from synchronized multi-view videos. We use across-view reconstruction task to inject geometry information in the model. Ourapproach is based on the masked autoencoder (MAE) framework. In addition to thesame-view decoder, we introduce a separate cross-view decoder which leveragescross-attention mechanism to reconstruct a target viewpoint video using a videofrom source viewpoint, to help representations robust to viewpoint changes. Forvideos, static regions can be reconstructed trivially which hinders learningmeaningful representations. To tackle this, we introduce a motion-weightedreconstruction loss which improves temporal modeling. We reportstate-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well asin the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets,demonstrating the robustness of our approach. Code will be made available."
    },
    {
        "link": "https://arxiv.org/abs/2401.15902",
        "title": "A Concise but Effective Network for Image Guided Depth Completion in Autonomous Driving",
        "authors": [
            "Moyun Liu",
            "Youping Chen",
            "Jingming Xie",
            "Lei Yao",
            "Yang Zhang",
            "Joey Tianyi Zhou"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Depth completion is a crucial task in autonomous driving, aiming to convert asparse depth map into a dense depth prediction. Due to its potentially richsemantic information, RGB image is commonly fused to enhance the completioneffect. Image-guided depth completion involves three key challenges: 1) how toeffectively fuse the two modalities; 2) how to better recover depthinformation; and 3) how to achieve real-time prediction for practicalautonomous driving. To solve the above problems, we propose a concise buteffective network, named CENet, to achieve high-performance depth completionwith a simple and elegant structure. Firstly, we use a fast guidance module tofuse the two sensor features, utilizing abundant auxiliary features extractedfrom the color space. Unlike other commonly used complicated guidance modules,our approach is intuitive and low-cost. In addition, we find and analyze theoptimization inconsistency problem for observed and unobserved positions, and adecoupled depth prediction head is proposed to alleviate the issue. Theproposed decoupled head can better output the depth of valid and invalidpositions with very few extra inference time. Based on the simple structure ofdual-encoder and single-decoder, our CENet can achieve superior balance betweenaccuracy and efficiency. In the KITTI depth completion benchmark, our CENetattains competitive performance and inference speed compared with thestate-of-the-art methods. To validate the generalization of our method, we alsoevaluate on indoor NYUv2 dataset, and our CENet still achieve impressiveresults. The code of this work will be available athttps://github.com/lmomoy/CENet."
    },
    {
        "link": "https://arxiv.org/abs/2401.15903",
        "title": "Toward the Identifiability of Comparative Deep Generative Models",
        "authors": [
            "Romain Lopez",
            "Jan-Christian Huetter",
            "Ehsan Hajiramezanali",
            "Jonathan Pritchard",
            "Aviv Regev"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Deep Generative Models (DGMs) are versatile tools for learning datarepresentations while adequately incorporating domain knowledge such as thespecification of conditional probability distributions. Recently proposed DGMstackle the important task of comparing data sets from different sources. Onesuch example is the setting of contrastive analysis that focuses on describingpatterns that are enriched in a target data set compared to a background dataset. The practical deployment of those models often assumes that DGMs naturallyinfer interpretable and modular latent representations, which is known to be anissue in practice. Consequently, existing methods often rely on ad-hocregularization schemes, although without any theoretical grounding. Here, wepropose a theory of identifiability for comparative DGMs by extending recentadvances in the field of non-linear independent component analysis. We showthat, while these models lack identifiability across a general class of mixingfunctions, they surprisingly become identifiable when the mixing function ispiece-wise affine (e.g., parameterized by a ReLU neural network). We alsoinvestigate the impact of model misspecification, and empirically show thatpreviously proposed regularization techniques for fitting comparative DGMs helpwith identifiability when the number of latent variables is not known inadvance. Finally, we introduce a novel methodology for fitting comparative DGMsthat improves the treatment of multiple data sources via multi-objectiveoptimization and that helps adjust the hyperparameter for the regularization inan interpretable manner, using constrained optimization. We empiricallyvalidate our theory and new methodology using simulated data as well as arecent data set of genetic perturbations in cells profiled via single-cell RNAsequencing."
    },
    {
        "link": "https://arxiv.org/abs/2401.15906",
        "title": "Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets",
        "authors": [
            "V. Arvind Rameshwar",
            "Anshoo Tandon",
            "Prajjwal Gupta",
            "Novoneel Chakraborty",
            "Abhay Sharma"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "This paper considers the problem of the private release of sample means ofspeed values from traffic datasets. Our key contribution is the development ofuser-level differentially private algorithms that incorporate carefully chosenparameter values to ensure low estimation errors on real-world datasets, whileensuring privacy. We test our algorithms on ITMS (Intelligent TrafficManagement System) data from an Indian city, where the speeds of differentbuses are drawn in a potentially non-i.i.d. manner from an unknowndistribution, and where the number of speed samples contributed by differentbuses is potentially different. We then apply our algorithms to a syntheticdataset, generated based on the ITMS data, having either a large number ofusers or a large number of samples per user. Here, we provide recommendationsfor the choices of parameters and algorithm subroutines that result in lowestimation errors, while guaranteeing user-level privacy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15910",
        "title": "Correction to \"Private Information Retrieval Over Gaussian MAC\"",
        "authors": [
            "Or Elimelech",
            "Ori Shmuel",
            "Asaf Cohen"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In the above article \\cite{shmuel2021private}, the authors introduced a PIRscheme for the Additive White Gaussian Noise (AWGN) Multiple Access Channel(MAC), both with and without fading. The authors utilized the additive natureof the channel and leveraged the linear properties and structure of latticecodes to retrieve the desired message without the servers acquiring anyknowledge on the retrieved message's index.Theorems 3 and 4 in \\cite{shmuel2021private} contain an error arising fromthe incorrect usage of the modulo operator. Moreover, the proofs assume aone-to-one mapping function, \u03d5(\u22c5), between a messageWj\u2208FLp and the elements of $\\cC$, mistakenly suggesting thatthe user possesses all the required information in advance. However, this isnot the case. Herein, we present the corrected versions of these theorems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15911",
        "title": "Distribution-consistency Structural Causal Models",
        "authors": [
            "Heyang Gong",
            "Chaochao Lu",
            "Yu Zhang"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "In the field of causal modeling, potential outcomes (PO) and structuralcausal models (SCMs) stand as the predominant frameworks. However, theseframeworks face notable challenges in practically modeling counterfactuals,formalized as parameters of the joint distribution of potential outcomes.Counterfactual reasoning holds paramount importance in contemporarydecision-making processes, especially in scenarios that demand personalizedincentives based on the joint values of (Y(0),Y(1)). This paper begins withan investigation of the PO and SCM frameworks for modeling counterfactuals.Through the analysis, we identify an inherent model capacity limitation, termedas the ``degenerative counterfactual problem'', emerging from the consistencyrule that is the cornerstone of both frameworks. To address this limitation, weintroduce a novel \\textit{distribution-consistency} assumption, and inalignment with it, we propose the Distribution-consistency Structural CausalModels (DiscoSCMs) offering enhanced capabilities to model counterfactuals. Toconcretely reveal the enhanced model capacity, we introduce a new identifiablecausal parameter, \\textit{the probability of consistency}, which holdspractical significance within DiscoSCM alone, showcased with a personalizedincentive example. Furthermore, we provide a comprehensive set of theoreticalresults about the ``Ladder of Causation'' within the DiscoSCM framework. Wehope it opens new avenues for future research of counterfactual modeling,ultimately enhancing our understanding of causality and its real-worldapplications."
    },
    {
        "link": "https://arxiv.org/abs/2401.15912",
        "title": "An Efficient, High-Rate Scheme for Private Information Retrieval over the Gaussian MAC",
        "authors": [
            "Or Elimelech",
            "Asaf Cohen"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This paper addresses the challenge of the private information retrieval (PIR)problem wherein there are N replicated non-communicating databases containingthe same M messages and a user who wants to retrieve one of the messageswithout revealing the wanted message's index to the databases. In addition, weassume a block-fading additive white Gaussian noise multiple access channel(AWGN MAC) linking the user and the databases. Shmuel's contribution\\cite{shmuel2021private}, presenting a joint channel-PIR scheme utilizing theC\\&F protocol, has shown the potential of a joint channel-PIR scheme over aseparated scheme. In this paper, we propose an improved joint channel-PIRapproach tailored for the PIR problem with N databases over a block-fadingAWGN. Unlike the C\\&F protocol, our scheme offers reduced computationalcomplexity while improving the scaling laws governing the achievable rate. Ourachievable rate scales with the number of databases N and the power Psimilarly to the channel capacity without the privacy constraint andoutperforms the C\\&F-based approach. Furthermore, our analysis demonstratesthat our improved rate exhibits only a finite gap from the channel capacity ofone bit as N increases."
    },
    {
        "link": "https://arxiv.org/abs/2401.15914",
        "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization",
        "authors": [
            "Yuhang Zang",
            "Hanlin Goh",
            "Josh Susskind",
            "Chen Huang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing vision-language models exhibit strong generalization on a variety ofvisual domains and tasks. However, such models mainly perform zero-shotrecognition in a closed-set manner, and thus struggle to handle open-domainvisual concepts by design. There are recent finetuning methods, such as promptlearning, that not only study the discrimination between in-distribution (ID)and out-of-distribution (OOD) samples, but also show some improvements in bothID and OOD accuracies. In this paper, we first demonstrate that vision-languagemodels, after long enough finetuning but without proper regularization, tend tooverfit the known classes in the given dataset, with degraded performance onunknown classes. Then we propose a novel approach OGEN to address this pitfall,with the main focus on improving the OOD GENeralization of finetuned models.Specifically, a class-conditional feature generator is introduced to synthesizeOOD features using just the class name of any unknown class. Such synthesizedfeatures will provide useful knowledge about unknowns and help regularize thedecision boundary between ID and OOD data when optimized jointly. Equallyimportant is our adaptive self-distillation mechanism to regularize our featuregeneration model during joint optimization, i.e., adaptively transferringknowledge between model states to further prevent overfitting. Experimentsvalidate that our method yields convincing gains in OOD generalizationperformance in different settings."
    },
    {
        "link": "https://arxiv.org/abs/2401.15915",
        "title": "Unrestricted Error-Type Codebook Generation for Error Correction Code in DNA Storage Inspired by NLP",
        "authors": [
            "Yi Lu",
            "Yun Ma",
            "Chenghao Li",
            "Xin Zhang",
            "Guangxiang Si"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Recently, DNA storage has surfaced as a promising alternative for datastorage, presenting notable benefits in terms of storage capacity,cost-effectiveness in maintenance, and the capability for parallel replication.Mathematically, the DNA storage process can be conceptualized as an insertion,deletion, and substitution (IDS) channel. Due to the mathematical complexityassociated with the Levenshtein distance, creating a code that corrects for IDSremains a challenging task. In this paper, we propose a bottom-up generationapproach to grow the required codebook based on the computation of EditComputational Graph (ECG) which differs from the algebraic constructions byincorporating the Derivative-Free Optimization (DFO) method. Specifically, thisapproach is regardless of the type of errors. Compared the results with thework for 1-substitution-1-deletion and 2-deletion, the redundancy is reduced byabout 30-bit and 60-bit, respectively. As far as we know, our method is thefirst IDS-correcting code designed using classical Natural Language Process(NLP) techniques, marking a turning point in the field of error correction coderesearch. Based on the codebook generated by our method, there may besignificant breakthroughs in the complexity of encoding and decodingalgorithms."
    },
    {
        "link": "https://arxiv.org/abs/2401.15917",
        "title": "Blockchain-enabled Trustworthy Federated Unlearning",
        "authors": [
            "Yijing Lin",
            "Zhipeng Gao",
            "Hongyang Du",
            "Jinke Ren",
            "Zhiqiang Xie",
            "Dusit Niyato"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Federated unlearning is a promising paradigm for protecting the dataownership of distributed clients. It allows central servers to removehistorical data effects within the machine learning model as well as addressthe \"right to be forgotten\" issue in federated learning. However, existingworks require central servers to retain the historical model parameters fromdistributed clients, such that allows the central server to utilize theseparameters for further training even, after the clients exit the trainingprocess. To address this issue, this paper proposes a new blockchain-enabledtrustworthy federated unlearning framework. We first design a proof offederated unlearning protocol, which utilizes the Chameleon hash function toverify data removal and eliminate the data contributions stored in otherclients' models. Then, an adaptive contribution-based retraining mechanism isdeveloped to reduce the computational overhead and significantly improve thetraining efficiency. Extensive experiments demonstrate that the proposedframework can achieve a better data removal effect than the state-of-the-artframeworks, marking a significant stride towards trustworthy federatedunlearning."
    },
    {
        "link": "https://arxiv.org/abs/2401.15921",
        "title": "A New Framework to Predict and Visualize Technology Acceptance: A Case Study of Shared Autonomous Vehicles",
        "authors": [
            "Lirui Guo",
            "Michael G. Burke",
            "Wynita M. Griggs"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "The burgeoning field of Shared Autonomous Vehicles (SAVs) presentstransformative potential for the transport sector, subject to publicacceptance. Traditional acceptance models, primarily reliant on StructuralEquation Modelling (SEM), often fall short of capturing the complex, non-lineardynamics underlying this acceptance. To address these limitations, this paperproposes a Machine Learning (ML) approach to predict public acceptance of SAVsand employs a chord diagram to visualize the influence of different predictors.This approach reveals nuanced, non-linear relationships between factors at bothmacro and micro levels, and identifies attitude as the primary predictor of SAVusage intention, followed by perceived risk, perceived usefulness, trust, andperceived ease of use. The framework also uncovers divergent perceptions ofthese factors among SAV adopters and non-adopters, providing granular insightsfor strategic initiatives to enhance SAV acceptance. Using SAV acceptance as acase study, our findings contribute a novel, machine learning-based perspectiveto the discourse on technology acceptance, underscoring the importance ofnuanced, data-driven approaches in understanding and fostering publicacceptance of emerging transport technologies."
    },
    {
        "link": "https://arxiv.org/abs/2401.15924",
        "title": "Energy-Aware Service Offloading for Semantic Communications in Wireless Networks",
        "authors": [
            "Hassan Saadat",
            "Abdullatif Albaseer",
            "Mohamed Abdallah",
            "Amr Mohamed",
            "Aiman Erbad"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Today, wireless networks are becoming responsible for serving intelligentapplications, such as extended reality and metaverse, holographic telepresence,autonomous transportation, and collaborative robots. Although currentfifth-generation (5G) networks can provide high data rates in terms ofGigabytes/second, they cannot cope with the high demands of the aforementionedapplications, especially in terms of the size of the high-quality live videosand images that need to be communicated in real-time. Therefore, with the helpof artificial intelligence (AI)-based future sixth-generation (6G) networks,the semantic communication concept can provide the services demanded by theseapplications. Unlike Shannon's classical information theory, semanticcommunication urges the use of the semantics (meaningful contents) of the datain designing more efficient data communication schemes. Hence, in this paper,we model semantic communication as an energy minimization framework inheterogeneous wireless networks with respect to delay and quality-of-serviceconstraints. Then, we propose a sub-optimal solution to the NP-hardcombinatorial mixed-integer nonlinear programming problem (MINLP) by utilizingefficient techniques such as discrete optimization variables' relaxation. Inaddition, AI-based autoencoder and classifier are trained and deployed toperform semantic extraction, reconstruction, and classification services.Finally, we compare our proposed sub-optimal solution with differentstate-of-the-art methods, and the obtained results demonstrate its superiority."
    },
    {
        "link": "https://arxiv.org/abs/2401.15927",
        "title": "E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models",
        "authors": [
            "Jinchang Hou",
            "Chang Ao",
            "Haihong Wu",
            "Xiangtao Kong",
            "Zhigang Zheng",
            "Daijia Tang",
            "Chengming Li",
            "Xiping Hu",
            "Ruifeng Xu",
            "Shiwen Ni",
            "Min Yang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "With the accelerating development of Large Language Models (LLMs), many LLMsare beginning to be used in the Chinese K-12 education domain. The integrationof LLMs and education is getting closer and closer, however, there is currentlyno benchmark for evaluating LLMs that focuses on the Chinese K-12 educationdomain. Therefore, there is an urgent need for a comprehensive natural languageprocessing benchmark to accurately assess the capabilities of various LLMs inthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,the first comprehensive evaluation benchmark specifically designed for theChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choicequestions at the primary, middle, and high school levels across a wide range ofsubjects, including Chinese, English, Politics, History, Ethics, Physics,Chemistry, Mathematics, and Geography. We conducted a comprehensive evaluationof E-EVAL on advanced LLMs, including both English-dominant andChinese-dominant models. Findings show that Chinese-dominant models performwell compared to English-dominant models, with many scoring even above the GPT4.0. However, almost all models perform poorly in complex subjects such asmathematics. We also found that most Chinese-dominant LLMs did not achievehigher scores at the primary school level compared to the middle school level.We observe that the mastery of higher-order knowledge by the model does notnecessarily imply the mastery of lower-order knowledge as well. Additionally,the experimental results indicate that the Chain of Thought (CoT) technique iseffective only for the challenging science subjects, while Few-shot promptingis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyzethe strengths and limitations of LLMs in educational applications, and tocontribute to the progress and development of Chinese K-12 education and LLMs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15931",
        "title": "EmoDM: A Diffusion Model for Evolutionary Multi-objective Optimization",
        "authors": [
            "Xueming Yan",
            "Yaochu Jin"
        ],
        "primary_subject": "Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Evolutionary algorithms have been successful in solving multi-objectiveoptimization problems (MOPs). However, as a class of population-based searchmethodology, evolutionary algorithms require a large number of evaluations ofthe objective functions, preventing them from being applied to a wide range ofexpensive MOPs. To tackle the above challenge, this work proposes for the firsttime a diffusion model that can learn to perform evolutionary multi-objectivesearch, called EmoDM. This is achieved by treating the reversed convergenceprocess of evolutionary search as the forward diffusion and learn the noisedistributions from previously solved evolutionary optimization tasks. Thepre-trained EmoDM can then generate a set of non-dominated solutions for a newMOP by means of its reverse diffusion without further evolutionary search,thereby significantly reducing the required function evaluations. To enhancethe scalability of EmoDM, a mutual entropy-based attention mechanism isintroduced to capture the decision variables that are most important for theobjectives. Experimental results demonstrate the competitiveness of EmoDM interms of both the search performance and computational efficiency compared withstate-of-the-art evolutionary algorithms in solving MOPs having up to 5000decision variables. The pre-trained EmoDM is shown to generalize well to unseenproblems, revealing its strong potential as a general and efficient MOP solver."
    },
    {
        "link": "https://arxiv.org/abs/2401.15934",
        "title": "HICH Image/Text (HICH-IT): Comprehensive Text and Image Datasets for Hypertensive Intracerebral Hemorrhage Research",
        "authors": [
            "Jie Li",
            "Yulong Xia",
            "Tongxin Yang",
            "Fenglin Cai",
            "Miao Wei",
            "Zhiwei Zhang",
            "Li Jiang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this paper, we introduce a new multimodal dataset in the medical field ofhypertensive intracerebral hemorrhage(HICH), called as HICH-IT, which includesboth textual information and head CT images. This dataset is designed toenhance the accuracy of artificial intelligence in the diagnosis and treatmentof HICH. This dataset, built upon the foundation of standard text and imagedata, incorporates specific annotations within the text data, extracting keycontent from the text information, and categorizes the annotation content ofimaging data into four types: brain midline, hematoma, left cerebral ventricle,and right cerebral ventricle. HICH-IT aims to be a foundational dataset forfeature learning in image segmentation tasks and named entity recognition. Tofurther understand the dataset, we have trained deep learning algorithms toobserve the performance. The pretrained models have been released at bothwww.daip.club and github.com/Deep-AI-Application-DAIP. The dataset has beenuploaded to https://github.com/CYBUS123456/HICH-IT-Datasets.Index Terms-HICH, Deep learning, Intraparenchymal hemorrhage, named entityrecognition, novel dataset"
    },
    {
        "link": "https://arxiv.org/abs/2401.15935",
        "title": "Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning",
        "authors": [
            "Viktor Moskvoretskii",
            "Dmitry Osin",
            "Egor Shvetsov",
            "Igor Udovichenko",
            "Maxim Zhelnin",
            "Andrey Dukhovny",
            "Anna Zhimerikina",
            "Albert Efimov",
            "Evgeny Burnaev"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "This study investigates self-supervised learning techniques to obtainrepresentations of Event Sequences. It is a key modality in variousapplications, including but not limited to banking, e-commerce, and healthcare.We perform a comprehensive study of generative and contrastive approaches inself-supervised learning, applying them both independently. We find that thereis no single supreme method. Consequently, we explore the potential benefits ofcombining these approaches. To achieve this goal, we introduce a novel methodthat aligns generative and contrastive embeddings as distinct modalities,drawing inspiration from contemporary multimodal research.Generative and contrastive approaches are often treated as mutuallyexclusive, leaving a gap for their combined exploration. Our resultsdemonstrate that this aligned model performs at least on par with, and mostlysurpasses, existing methods and is more universal across a variety of tasks.Furthermore, we demonstrate that self-supervised methods consistentlyoutperform the supervised approach on our datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.15938",
        "title": "Motion-induced error reduction for high-speed dynamic digital fringe projection system",
        "authors": [
            "Sanghoon Jeon",
            "Hyo-Geon Lee",
            "Jae-Sung Lee",
            "Bo-Min Kang",
            "Byung-Wook Jeon",
            "Jun Young Yoon",
            "Jae-Sang Hyun"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In phase-shifting profilometry (PSP), any motion during the acquisition offringe patterns can introduce errors because it assumes both the object andmeasurement system are stationary. Therefore, we propose a method to pixel-wisereduce the errors when the measurement system is in motion due to a motorizedlinear stage. The proposed method introduces motion-induced error reductionalgorithm, which leverages the motor's encoder and pinhole model of the cameraand projector. 3D shape measurement is possible with only three fringe patternsby applying geometric constraints of the digital fringe projection system. Weaddress the mismatch problem due to the motion-induced camera pixel disparitiesand reduce phase-shift errors. These processes are easy to implement andrequire low computational cost. Experimental results demonstrate that thepresented method effectively reduces the errors even in non-uniform motion."
    },
    {
        "link": "https://arxiv.org/abs/2401.15939",
        "title": "Correcting a Single Deletion in Reads from a Nanopore Sequencer",
        "authors": [
            "Anisha Banerjee",
            "Yonatan Yehezkeally",
            "Antonia Wachter-Zeh",
            "Eitan Yaakobi"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Owing to its several merits over other DNA sequencing technologies, nanoporesequencers hold an immense potential to revolutionize the efficiency of DNAstorage systems. However, their higher error rates necessitate further researchto devise practical and efficient coding schemes that would allow accurateretrieval of the data stored. Our work takes a step in this direction byadopting a simplified model of the nanopore sequencer inspired by Mao \\emph{etal.}, which incorporates some of its physical aspects. This channel model canbe viewed as a sliding window of length \u2113 that passes over the incominginput sequence and produces the L1-weight of the enclosed \u2113 bits, whileshifting by one position at each time step. The resulting (\u2113+1)-aryvector, referred to as the \u2113-\\emph{read vector}, is susceptible todeletion errors due to imperfections inherent in the sequencing process. Weestablish that at least logn\u2212\u2113 bits of redundancy are needed tocorrect a single deletion. An error-correcting code that is optimal up to anadditive constant, is also proposed. Furthermore, we find that for \u2113\u22652, reconstruction from two distinct noisy \u2113-read vectors can beaccomplished without any redundancy, and provide a suitable reconstructionalgorithm to this effect."
    },
    {
        "link": "https://arxiv.org/abs/2401.15940",
        "title": "Knowledge-Aware Code Generation with Large Language Models",
        "authors": [
            "Tao Huang",
            "Zhihong Sun",
            "Zhi Jin",
            "Ge Li",
            "Chen Lyu"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Large Language Models (LLMs) perform well on basic programming problems.However, they encounter challenges when dealing with complex tasks involvingthe use of diverse algorithmic and data structure skills, particularlyprogramming competition-level problems. Notably, ChatGPT exhibits proficientperformance on problems it has encountered during its pre-training phase, butthis performance deteriorates when faced with novel problems. Consequently,enhancing the ability of LLMs to address unfamiliar problems has emerged as apivotal research focus. The problem-solving process of LLMs mirrors humanprogrammers' approach to a certain extent. When confronted with new programmingtasks, human programmers engage in task planning and code writing with thepreviously acquired knowledge about algorithms and data structures. Despitehaving learned such knowledge, LLMs struggle to effectively apply it when facedwith specific new problems. To address this issue, we constructed a noveldataset, CodeF, which contains a portion of programming problems that ChatGPThas not previously encountered. Furthermore, we developed a Knowledge Librarytailored for Python programming contest problems and introduced the concept ofKnowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models'understanding and problem-solving capabilities by integrating prompt andknowledge from the library into the LLMs' code generation reasoning process,especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets,KareCoder demonstrated outstanding performance in handling novel problemspreviously unencountered by LLMs. In contrast with the code directly generatedby ChatGPT, KareCoder achieved a relative improvement of 23.3% on the Pass@1metric on the CodeF post2021-9 dataset. Additionally, it performs well comparedto other methods when dealing with problems that LLMs have previouslyencountered."
    },
    {
        "link": "https://arxiv.org/abs/2401.15941",
        "title": "High order conservative LDG-IMEX methods for the degenerate nonlinear non-equilibrium radiation diffusion problems",
        "authors": [
            "Shaoqin Zheng",
            "Min Tang",
            "Qiang Zhang",
            "Tao Xiong"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we develop a class of high-order conservative methods forsimulating non-equilibrium radiation diffusion problems. Numerically, thissystem poses significant challenges due to strong nonlinearity within the stiffsource terms and the degeneracy of nonlinear diffusion terms. Explicit methodsrequire impractically small time steps, while implicit methods, which offerstability, come with the challenge to guarantee the convergence of nonlineariterative solvers. To overcome these challenges, we propose apredictor-corrector approach and design proper implicit-explicit timediscretizations. In the predictor step, the system is reformulated into anonconservative form and linear diffusion terms are introduced as apenalization to mitigate strong nonlinearities. We then employ a Picarditeration to secure convergence in handling the nonlinear aspects. Thecorrector step guarantees the conservation of total energy, which is vital foraccurately simulating the speeds of propagating sharp fronts in this system.For spatial approximations, we utilize local discontinuous Galerkin finiteelement methods, coupled with positive-preserving and TVB limiters. We validatethe orders of accuracy, conservation properties, and suitability of using largetime steps for our proposed methods, through numerical experiments conducted onone- and two-dimensional spatial problems. In both homogeneous andheterogeneous non-equilibrium radiation diffusion problems, we attain a timestability condition comparable to that of a fully implicit time discretization.Such an approach is also applicable to many other reaction-diffusion systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.15942",
        "title": "Generating Multi-Center Classifier via Conditional Gaussian Distribution",
        "authors": [
            "Zhemin Zhang",
            "Xun Gong"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The linear classifier is widely used in various image classification tasks.It works by optimizing the distance between a sample and its correspondingclass center. However, in real-world data, one class can contain several localclusters, e.g., birds of different poses. To address this complexity, wepropose a novel multi-center classifier. Different from the vanilla linearclassifier, our proposal is established on the assumption that the deepfeatures of the training set follow a Gaussian Mixture distribution.Specifically, we create a conditional Gaussian distribution for each class andthen sample multiple sub-centers from that distribution to extend the linearclassifier. This approach allows the model to capture intra-class localstructures more efficiently. In addition, at test time we set the mean of theconditional Gaussian distribution as the class center of the linear classifierand follow the vanilla linear classifier outputs, thus requiring no additionalparameters or computational overhead. Extensive experiments on imageclassification show that the proposed multi-center classifier is a powerfulalternative to widely used linear classifiers. Code available athttps://github.com/ZheminZhang1/MultiCenter-Classifier."
    },
    {
        "link": "https://arxiv.org/abs/2401.15944",
        "title": "Bridging the Domain Gap: A Simple Domain Matching Method for Reference-based Image Super-Resolution in Remote Sensing",
        "authors": [
            "Jeongho Min",
            "Yejun Lee",
            "Dongyoung Kim",
            "Jaejun Yoo"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, reference-based image super-resolution (RefSR) has shown excellentperformance in image super-resolution (SR) tasks. The main idea of RefSR is toutilize additional information from the reference (Ref) image to recover thehigh-frequency components in low-resolution (LR) images. By transferringrelevant textures through feature matching, RefSR models outperform existingsingle image super-resolution (SISR) models. However, their performancesignificantly declines when a domain gap between Ref and LR images exists,which often occurs in real-world scenarios, such as satellite imaging. In thisletter, we introduce a Domain Matching (DM) module that can be seamlesslyintegrated with existing RefSR models to enhance their performance in aplug-and-play manner. To the best of our knowledge, we are the first to exploreDomain Matching-based RefSR in remote sensing image processing. Our analysisreveals that their domain gaps often occur in different satellites, and ourmodel effectively addresses these challenges, whereas existing models struggle.Our experiments demonstrate that the proposed DM module improves SR performanceboth qualitatively and quantitatively for remote sensing super-resolutiontasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15945",
        "title": "Regularization of linear inverse problems with irregular noise using embedding operators",
        "authors": [
            "Xinyan Li",
            "Simon Hubmer",
            "Shuai Lu",
            "Ronny Ramlau"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we investigate regularization of linear inverse problems withirregular noise. In particular, we consider the case that the noise can bepreprocessed by certain adjoint embedding operators. By introducing theconsequent preprocessed problem, we provide convergence analysis for generalregularization schemes under standard assumptions. Furthermore, for a specialcase of Tikhonov regularization in Computerized Tomography, we show that ourapproach leads to a novel (Fourier-based) filtered backprojection algorithm.Numerical examples with different parameter choice rules verify the efficiencyof our proposed algorithm."
    },
    {
        "link": "https://arxiv.org/abs/2401.15946",
        "title": "Approaching Maximum Likelihood Decoding Performance via Reshuffling ORBGRAND",
        "authors": [
            "Li Wan",
            "Wenyi Zhang"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Guessing random additive noise decoding (GRAND) is a recently proposeddecoding paradigm particularly suitable for codes with short length and highrate. Among its variants, ordered reliability bits GRAND (ORBGRAND) exploitssoft information in a simple and effective fashion to schedule its queries,thereby allowing efficient hardware implementation. Compared with maximumlikelihood (ML) decoding, however, ORBGRAND still exhibits noticeableperformance gap in terms of block error rate (BLER). In order to improve theperformance of ORBGRAND while still retaining its amenability to hardwareimplementation, a new variant of ORBGRAND termed RS-ORBGRAND is proposed, whosebasic idea is to reshuffle the queries of ORBGRAND so that the expected numberof queries is minimized. Numerical simulations show that RS-ORBGRAND leads tonoticeable gains compared with ORBGRAND and its existing variants, and is only0.1dB away from ML decoding, for BLER as low as 10\u22126."
    },
    {
        "link": "https://arxiv.org/abs/2401.15947",
        "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
        "authors": [
            "Bin Lin",
            "Zhenyu Tang",
            "Yang Ye",
            "Jiaxi Cui",
            "Bin Zhu",
            "Peng Jin",
            "Junwu Zhang",
            "Munan Ning",
            "Li Yuan"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "For Large Vision-Language Models (LVLMs), scaling the model can effectivelyimprove performance. However, expanding model parameters significantlyincreases the training and inferring costs, as all model parameters areactivated for each token in the calculation. In this work, we propose a noveltraining strategy MoE-tuning for LVLMs, which can constructing a sparse modelwith an outrageous number of parameter but a constant computational cost, andeffectively addresses the performance degradation typically associated withmulti-modal learning and model sparsity. Furthermore, we present the MoE-LLaVAframework, a MoE-based sparse LVLM architecture. This framework uniquelyactivates only the top-k experts through routers during deployment, keeping theremaining experts inactive. Our extensive experiments highlight the excellentcapabilities of MoE-LLaVA in visual understanding and its potential to reducehallucinations in model outputs. Remarkably, with just 3 billion sparselyactivated parameters, MoE-LLaVA demonstrates performance comparable to theLLaVA-1.5-7B on various visual understanding datasets and even surpasses theLLaVA-1.5-13B in object hallucination benchmarks. Through MoE-LLaVA, we aim toestablish a baseline for sparse LVLMs and provide valuable insights for futureresearch in developing more efficient and effective multi-modal learningsystems. Code is released at \\url{https://github.com/PKU-YuanGroup/MoE-LLaVA}."
    },
    {
        "link": "https://arxiv.org/abs/2401.15948",
        "title": "AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using Adversarial Learning",
        "authors": [
            "Vikas Kanaujia",
            "Mathias S. Scheurer",
            "Vipul Arora"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Deep generative models complement Markov-chain-Monte-Carlo methods forefficiently sampling from high-dimensional distributions. Among these methods,explicit generators, such as Normalising Flows (NFs), in combination with theMetropolis Hastings algorithm have been extensively applied to get unbiasedsamples from target distributions. We systematically study central problems inconditional NFs, such as high variance, mode collapse and data efficiency. Wepropose adversarial training for NFs to ameliorate these problems. Experimentsare conducted with low-dimensional synthetic datasets and XY spin models in twospatial dimensions."
    },
    {
        "link": "https://arxiv.org/abs/2401.15949",
        "title": "TFDMNet: A Novel Network Structure Combines the Time Domain and Frequency Domain Features",
        "authors": [
            "Hengyue Pan",
            "Yixin Chen",
            "Zhiliang Tian",
            "Peng Qiao",
            "Linbo Qiao",
            "Dongsheng Li"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Convolutional neural network (CNN) has achieved impressive success incomputer vision during the past few decades. The image convolution operationhelps CNNs to get good performance on image-related tasks. However, it also hashigh computation complexity and hard to be parallelized. This paper proposes anovel Element-wise Multiplication Layer (EML) to replace convolution layers,which can be trained in the frequency domain. Theoretical analyses show thatEMLs lower the computation complexity and easier to be parallelized. Moreover,we introduce a Weight Fixation mechanism to alleviate the problem ofover-fitting, and analyze the working behavior of Batch Normalization andDropout in the frequency domain. To get the balance between the computationcomplexity and memory usage, we propose a new network structure, namelyTime-Frequency Domain Mixture Network (TFDMNet), which combines the advantagesof both convolution layers and EMLs. Experimental results imply that TFDMNetachieves good performance on MNIST, CIFAR-10 and ImageNet databases with lessnumber of operations comparing with corresponding CNNs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15952",
        "title": "A Class-aware Optimal Transport Approach with Higher-Order Moment Matching for Unsupervised Domain Adaptation",
        "authors": [
            "Tuan Nguyen",
            "Van Nguyen",
            "Trung Le",
            "He Zhao",
            "Quan Hung Tran",
            "Dinh Phung"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge from alabeled source domain to an unlabeled target domain. In this paper, weintroduce a novel approach called class-aware optimal transport (OT), whichmeasures the OT distance between a distribution over the sourceclass-conditional distributions and a mixture of source and target datadistribution. Our class-aware OT leverages a cost function that determines thematching extent between a given data example and a source class-conditionaldistribution. By optimizing this cost function, we find the optimal matchingbetween target examples and source class-conditional distributions, effectivelyaddressing the data and label shifts that occur between the two domains. Tohandle the class-aware OT efficiently, we propose an amortization solution thatemploys deep neural networks to formulate the transportation probabilities andthe cost function. Additionally, we propose minimizing class-aware Higher-orderMoment Matching (HMM) to align the corresponding class regions on the sourceand target domains. The class-aware HMM component offers an economicalcomputational approach for accurately evaluating the HMM distance between thetwo distributions. Extensive experiments on benchmark datasets demonstrate thatour proposed method significantly outperforms existing state-of-the-artbaselines."
    },
    {
        "link": "https://arxiv.org/abs/2401.15953",
        "title": "Masked Audio Modeling with CLAP and Multi-Objective Learning",
        "authors": [
            "Yifei Xin",
            "Xiulian Peng",
            "Yan Lu"
        ],
        "primary_subject": "Sound (cs.SD)",
        "abstract": "Most existing masked audio modeling (MAM) methods learn audio representationsby masking and reconstructing local spectrogram patches. However, thereconstruction loss mainly accounts for the signal-level quality of thereconstructed spectrogram and is still limited in extracting high-level audiosemantics. In this paper, we propose to enhance the semantic modeling of MAM bydistilling cross-modality knowledge from contrastive language-audio pretraining(CLAP) representations for both masked and unmasked regions (MAM-CLAP) andleveraging a multi-objective learning strategy with a supervised classificationbranch (SupMAM), thereby providing more semantic knowledge for MAM and enablingit to effectively learn global features from labels. Experiments show that ourmethods significantly improve the performance on multiple downstream tasks.Furthermore, by combining our MAM-CLAP with SupMAM, we can achieve newstate-of-the-art results on various audio and speech classification tasks,exceeding previous self-supervised learning and supervised pretraining methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.15954",
        "title": "A supervised learning scheme for computing Hamilton-Jacobi equation via density coupling",
        "authors": [
            "Jianbo Cui",
            "Shu Liu",
            "Haomin Zhou"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We propose a supervised learning scheme for the first order Hamilton-JacobiPDEs in high dimensions. The scheme is designed by using the geometricstructure of Wasserstein Hamiltonian flows via a density coupling strategy. Itis equivalently posed as a regression problem using the Bregman divergence,which provides the loss function in learning while the data is generatedthrough the particle formulation of Wasserstein Hamiltonian flow. We prove aposterior estimate on L1 residual of the proposed scheme based on thecoupling density. Furthermore, the proposed scheme can be used to describe thebehaviors of Hamilton-Jacobi PDEs beyond the singularity formations on thesupport of coupling density.Several numerical examples with differentHamiltonians are provided to support our findings."
    },
    {
        "link": "https://arxiv.org/abs/2401.15956",
        "title": "MobFuzz: Adaptive Multi-objective Optimization in Gray-box Fuzzing",
        "authors": [
            "Gen Zhang",
            "Pengfei Wang",
            "Tai Yue",
            "Xiangdong Kong",
            "Shan Huang",
            "Xu Zhou",
            "Kai Lu"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Coverage-guided gray-box fuzzing (CGF) is an efficient software testingtechnique. There are usually multiple objectives to optimize in CGF. However,existing CGF meth- ods cannot successfully find the optimal values for multipleobjectives simultaneously. In this paper, we propose a gray-box fuzzer formulti-objective optimization (MOO) called MobFuzz. We model the multi-objectiveoptimization process as a multi- player multi-armed bandit (MPMAB). First, itadaptively selects the objective combination that contains the most appropriateobjectives for the current situation. Second, our model deals with the powerschedule, which adaptively allocates energy to the seeds under the chosenobjective combination. In MobFuzz, we propose an evolutionary algorithm calledNIC to optimize our chosen objectives simultaneously without incurringadditional performance overhead. To prove the effectiveness of MobFuzz, weconduct experiments on 12 real-world programs and the MAGMA data set.Experiment results show that multi-objective optimization in MobFuzzoutperforms single-objective fuzzing in the baseline fuzzers. In contrast tothem, MobFuzz can select the optimal objective combination and increase thevalues of multiple objectives up to 107%, with at most a 55% reduction in theenergy consumption. Moreover, MobFuzz has up to 6% more program coverage andfinds 3x more unique bugs than the baseline fuzzers. The NIC algorithm has atleast a 2x improvement with a performance overhead of approximately 3%."
    },
    {
        "link": "https://arxiv.org/abs/2401.15957",
        "title": "Scalable Federated Unlearning via Isolated and Coded Sharding",
        "authors": [
            "Yijing Lin",
            "Zhipeng Gao",
            "Hongyang Du",
            "Dusit Niyato",
            "Gui Gui",
            "Shuguang Cui",
            "Jinke Ren"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Federated unlearning has emerged as a promising paradigm to erase theclient-level data effect without affecting the performance of collaborativelearning models. However, the federated unlearning process often introducesextensive storage overhead and consumes substantial computational resources,thus hindering its implementation in practice. To address this issue, thispaper proposes a scalable federated unlearning framework based on isolatedsharding and coded computing. We first divide distributed clients into multipleisolated shards across stages to reduce the number of clients being affected.Then, to reduce the storage overhead of the central server, we develop a codedcomputing mechanism by compressing the model parameters across differentshards. In addition, we provide the theoretical analysis of time efficiency andstorage effectiveness for the isolated and coded sharding. Finally, extensiveexperiments on two typical learning tasks, i.e., classification and generation,demonstrate that our proposed framework can achieve better performance thanthree state-of-the-art frameworks in terms of accuracy, retraining time,storage overhead, and F1 scores for resisting membership inference attacks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15960",
        "title": "EchoPFL: Asynchronous Personalized Federated Learning on Mobile Devices with On-Demand Staleness Control",
        "authors": [
            "Xiaochen Li",
            "Sicong Liu",
            "Zimu Zhou",
            "Bin Guo",
            "Yuan Xu",
            "Zhiwen Yu"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "The rise of mobile devices with abundant sensory data and local computingcapabilities has driven the trend of federated learning (FL) on these devices.And personalized FL (PFL) emerges to train specific deep models for each mobiledevice to address data heterogeneity and varying performance preferences.However, mobile training times vary significantly, resulting in either delay(when waiting for slower devices for aggregation) or accuracy decline (whenaggregation proceeds without waiting). In response, we propose a shift towardsasynchronous PFL, where the server aggregates updates as soon as they areavailable. Nevertheless, existing asynchronous protocols are unfit for PFLbecause they are devised for federated training of a single global model. Theysuffer from slow convergence and decreased accuracy when confronted with severedata heterogeneity prevalent in PFL. Furthermore, they often exclude slowerdevices for staleness control, which notably compromises accuracy when thesedevices possess critical personalized data. Therefore, we propose EchoPFL, acoordination mechanism for asynchronous PFL. Central to EchoPFL is to includeupdates from all mobile devices regardless of their latency. To cope with theinevitable staleness from slow devices, EchoPFL revisits model broadcasting. Itintelligently converts the unscalable broadcast to on-demand broadcast,leveraging the asymmetrical bandwidth in wireless networks and the dynamicclustering-based PFL. Experiments show that compared to status quo approaches,EchoPFL achieves a reduction of up to 88.2% in convergence time, an improvementof up to 46% in accuracy, and a decrease of 37% in communication costs"
    },
    {
        "link": "https://arxiv.org/abs/2401.15962",
        "title": "An implicit staggered algorithm for CPFEM-based analysis of aluminum",
        "authors": [
            "Pedro Areias",
            "Charles dos Santos",
            "Rui Melicio",
            "Nuno Silvestre"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we introduce an implicit staggered algorithm for crystalplasticity finite element method (CPFEM) which makes use of dynamic relaxationat the constitutive integration level. An uncoupled version of the constitutivesystem consists of a multi-surface flow law complemented by an evolution lawfor the hardening variables. Since a saturation law is adopted for hardening, asequence of nonlinear iteration followed by a linear system is feasible. To tiethe constitutive unknowns, the dynamic relaxation method is adopted. AGreen-Nagdhi plasticity model is adopted based on the Hencky strain calculatedusing a [2/2] Pad\\'e approximation. For the incompressible case, theapproximation error is calculated exactly. A enhanced-assumed strain (EAS)element technology is adopted, which was found to be especially suited tolocalization problems such as the ones resulting from crystal plasticity planeslipping. Analysis of the results shows significant reduction of drift and welldefined localization without spurious modes or hourglassing."
    },
    {
        "link": "https://arxiv.org/abs/2401.15963",
        "title": "NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness",
        "authors": [
            "Manav Singhal",
            "Tushar Aggarwal",
            "Abhijeet Awasthi",
            "Nagarajan Natarajan",
            "Aditya Kanade"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Existing evaluation benchmarks of language models of code (code LMs) focusalmost exclusively on whether the LMs can generate functionally-correct code.In real-world software engineering, developers think beyond functionalcorrectness. They have requirements on \"how\" a functionality should beimplemented to meet overall system design objectives like efficiency, security,and maintainability. They would also trust the code LMs more if the LMsdemonstrate robust understanding of requirements and code semantics.We propose a new benchmark NoFunEval to evaluate code LMs on non-functionalrequirements and simple classification instances for both functional andnon-functional requirements. We propose a prompting method, Coding Concepts(CoCo), as a way for a developer to communicate the domain knowledge to theLMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding isthat they generally falter when tested on our benchmark, hinting at fundamentalblindspots in their training setups. Surprisingly, even the classificationaccuracy on functional-correctness instances derived from the popular HumanEvalbenchmark is low, calling in question the depth of their comprehension and thesource of their success in generating functionally-correct code in the firstplace. We will release our benchmark and evaluation scripts publicly athttps://aka.ms/NoFunEval."
    },
    {
        "link": "https://arxiv.org/abs/2401.15964",
        "title": "Spatio-Temporal Attention Graph Neural Network for Remaining Useful Life Prediction",
        "authors": [
            "Zhixin Huang",
            "Yujiang He",
            "Bernhard Sick"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Remaining useful life prediction plays a crucial role in the healthmanagement of industrial systems. Given the increasing complexity of systems,data-driven predictive models have attracted significant research interest.Upon reviewing the existing literature, it appears that many studies either donot fully integrate both spatial and temporal features or employ only a singleattention mechanism. Furthermore, there seems to be inconsistency in the choiceof data normalization methods, particularly concerning operating conditions,which might influence predictive performance. To bridge these observations,this study presents the Spatio-Temporal Attention Graph Neural Network. Ourmodel combines graph neural networks and temporal convolutional neural networksfor spatial and temporal feature extraction, respectively. The cascade of theseextractors, combined with multi-head attention mechanisms for bothspatio-temporal dimensions, aims to improve predictive precision and refinemodel explainability. Comprehensive experiments were conducted on the C-MAPSSdataset to evaluate the impact of unified versus clustering normalization. Thefindings suggest that our model performs state-of-the-art results using onlythe unified normalization. Additionally, when dealing with datasets withmultiple operating conditions, cluster normalization enhances the performanceof our proposed model by up to 27%."
    },
    {
        "link": "https://arxiv.org/abs/2401.15966",
        "title": "Response Generation for Cognitive Behavioral Therapy with Large Language Models: Comparative Study with Socratic Questioning",
        "authors": [
            "Kenta Izumi",
            "Hiroki Tanaka",
            "Kazuhiro Shidara",
            "Hiroyoshi Adachi",
            "Daisuke Kanayama",
            "Takashi Kudo",
            "Satoshi Nakamura"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Dialogue systems controlled by predefined or rule-based scenarios derivedfrom counseling techniques, such as cognitive behavioral therapy (CBT), play animportant role in mental health apps. Despite the need for responsibleresponses, it is conceivable that using the newly emerging LLMs to generatecontextually relevant utterances will enhance these apps. In this study, weconstruct dialogue modules based on a CBT scenario focused on conventionalSocratic questioning using two kinds of LLMs: a Transformer-based dialoguemodel further trained with a social media empathetic counseling dataset,provided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLMcreated by OpenAI. By comparing systems that use LLM-generated responses withthose that do not, we investigate the impact of generated responses onsubjective evaluations such as mood change, cognitive change, and dialoguequality (e.g., empathy). As a result, no notable improvements are observed whenusing the OsakaED model. When using GPT-4, the amount of mood change, empathy,and other dialogue qualities improve significantly. Results suggest that GPT-4possesses a high counseling ability. However, they also indicate that even whenusing a dialogue model trained with a human counseling dataset, it does notnecessarily yield better outcomes compared to scenario-based dialogues. Whilepresenting LLM-generated responses, including GPT-4, and having them interactdirectly with users in real-life mental health care services may raise ethicalissues, it is still possible for human professionals to produce exampleresponses or response templates using LLMs in advance in systems that userules, scenarios, or example responses."
    },
    {
        "link": "https://arxiv.org/abs/2401.15967",
        "title": "INSTILLER: Towards Efficient and Realistic RTL Fuzzing",
        "authors": [
            "Gen Zhang",
            "Pengfei Wang",
            "Tai Yue",
            "Danjun Liu",
            "Yubei Guo",
            "Kai Lu"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Bugs exist in hardware, such as CPU. Unlike soft- ware bugs, these hardwarebugs need to be detected before deployment. Previous fuzzing work in CPU bugdetection has several disadvantages, e.g., the length of RTL input instructionskeeps growing, and longer inputs are ineffective for fuzzing. In this paper, wepropose INSTILLER (Instruction Distiller), an RTL fuzzer based on ant colonyoptimization (ACO). First, to keep the input instruction length short andefficient in fuzzing, it distills input instructions with a variant of ACO(VACO). Next, related work cannot simulate realistic interruptions well infuzzing, and INSTILLER solves the problem of inserting interruptions andexceptions in generating the inputs. Third, to further improve the fuzzingperformance of INSTILLER, we propose hardware-based seed selection and mutationstrategies. We implement a prototype and conduct extensive experiments againststate-of-the-art fuzzing work in real-world target CPU cores. In experiments,INSTILLER has 29.4% more coverage than DiFuzzRTL. In addition, 17.0% moremismatches are detected by INSTILLER. With the VACO algorithm, INSTILLERgenerates 79.3% shorter input instructions than DiFuzzRTL, demonstrating itseffectiveness in distilling the input instructions. In addition, thedistillation leads to a 6.7% increase in execution speed on average."
    },
    {
        "link": "https://arxiv.org/abs/2401.15969",
        "title": "Routers in Vision Mixture of Experts: An Empirical Study",
        "authors": [
            "Tianlin Liu",
            "Mathieu Blondel",
            "Carlos Riquelme",
            "Joan Puigcerver"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Mixture-of-Experts (MoE) models are a promising way to scale up modelcapacity without significantly increasing computational cost. A key componentof MoEs is the router, which decides which subset of parameters (experts)process which feature embeddings (tokens). In this paper, we present acomprehensive study of routers in MoEs for computer vision tasks. We introducea unified MoE formulation that subsumes different MoEs with two parametricrouting tensors. This formulation covers both sparse MoE, which uses a binaryor hard assignment between experts and tokens, and soft MoE, which uses a softassignment between experts and weighted combinations of tokens. Routers forsparse MoEs can be further grouped into two variants: Token Choice, whichmatches experts to each token, and Expert Choice, which matches tokens to eachexpert. We conduct head-to-head experiments with 6 different routers, includingexisting routers from prior work and new ones we introduce. We show that (i)many routers originally developed for language modeling can be adapted toperform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routersgenerally outperform Token Choice routers, and (iii) soft MoEs generallyoutperform sparse MoEs with a fixed compute budget. These results provide newinsights regarding the crucial role of routers in vision MoE models."
    },
    {
        "link": "https://arxiv.org/abs/2401.15970",
        "title": "HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference",
        "authors": [
            "Tianshi Xu",
            "Meng Li",
            "Runsheng Wang"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Secure two-party computation with homomorphic encryption (HE) protects dataprivacy with a formal security guarantee but suffers from high communicationoverhead. While previous works, e.g., Cheetah, Iron, etc, have proposedefficient HE-based protocols for different neural network (NN) operations, theystill assume high precision, e.g., fixed point 37 bit, for the NN operationsand ignore NNs' native robustness against quantization error. In this paper, wepropose HEQuant, which features low-precision-quantization-aware optimizationfor the HE-based protocols. We observe the benefit of a naive combination ofquantization and HE quickly saturates as bit precision goes down. Hence, tofurther improve communication efficiency, we propose a series of optimizations,including an intra-coefficient packing algorithm and a quantization-awaretiling algorithm, to simultaneously reduce the number and precision of thetransferred data. Compared with prior-art HE-based protocols, e.g., CrypTFlow2,Cheetah, Iron, etc, HEQuant achieves 3.5\u223c23.4\u00d7 communicationreduction and 3.0\u223c9.3\u00d7 latency reduction. Meanwhile, when comparedwith prior-art network optimization frameworks, e.g., SENet, SNL, etc, HEQuantalso achieves 3.1\u223c3.6\u00d7 communication reduction."
    },
    {
        "link": "https://arxiv.org/abs/2401.15973",
        "title": "Sample Weight Estimation Using Meta-Updates for Online Continual Learning",
        "authors": [
            "Hamed Hemati",
            "Damian Borth"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The loss function plays an important role in optimizing the performance of alearning system. A crucial aspect of the loss function is the assignment ofsample weights within a mini-batch during loss computation. In the context ofcontinual learning (CL), most existing strategies uniformly treat samples whencalculating the loss value, thereby assigning equal weights to each sample.While this approach can be effective in certain standard benchmarks, itsoptimal effectiveness, particularly in more complex scenarios, remainsunderexplored. This is particularly pertinent in training \"in the wild,\" suchas with self-training, where labeling is automated using a reference model.This paper introduces the Online Meta-learning for Sample Importance (OMSI)strategy that approximates sample weights for a mini-batch in an online CLstream using an inner- and meta-update mechanism. This is done by firstestimating sample weight parameters for each sample in the mini-batch, then,updating the model with the adapted sample weights. We evaluate OMSI in twodistinct experimental settings. First, we show that OMSI enhances both learningand retained accuracy in a controlled noisy-labeled data stream. Then, we testthe strategy in three standard benchmarks and compare it with other popularreplay-based strategies. This research aims to foster the ongoing explorationin the area of self-adaptive CL."
    },
    {
        "link": "https://arxiv.org/abs/2401.15975",
        "title": "StableIdentity: Inserting Anybody into Anywhere at First Sight",
        "authors": [
            "Qinghe Wang",
            "Xu Jia",
            "Xiaomin Li",
            "Taiqing Li",
            "Liqian Ma",
            "Yunzhi Zhuge",
            "Huchuan Lu"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in large pretrained text-to-image models have shownunprecedented capabilities for high-quality human-centric generation, however,customizing face identity is still an intractable problem. Existing methodscannot ensure stable identity preservation and flexible editability, even withseveral images for each subject during training. In this work, we proposeStableIdentity, which allows identity-consistent recontextualization with justone face image. More specifically, we employ a face encoder with an identityprior to encode the input face, and then land the face representation into aspace with an editable prior, which is constructed from celeb names. Byincorporating identity prior and editability prior, the learned identity can beinjected anywhere with various contexts. In addition, we design a maskedtwo-phase diffusion loss to boost the pixel-level perception of the input faceand maintain the diversity of generation. Extensive experiments demonstrate ourmethod outperforms previous customization methods. In addition, the learnedidentity can be flexibly combined with the off-the-shelf modules such asControlNet. Notably, to the best knowledge, we are the first to directly injectthe identity learned from a single image into video/3D generation withoutfinetuning. We believe that the proposed StableIdentity is an important step tounify image, video, and 3D customized generation models."
    },
    {
        "link": "https://arxiv.org/abs/2401.15976",
        "title": "A Multi-Period Topology and Design Optimization Approach for District Heating Networks",
        "authors": [
            "Yannick Wack",
            "Martin Sollich",
            "Robbe Salenbien",
            "Jan Diriken",
            "Martine Baelmans",
            "Maarten Blommaert"
        ],
        "primary_subject": "Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "The transition to 4th generation district heating creates a growing need forscalable, automated design tools that accurately capture the spatial andtemporal details of heating network operation. This paper presents an automateddesign approach for the optimal design of district heating networks thatcombines scalable density-based topology optimization with a multi-periodapproach. In this way, temporal variations in demand, supply, and heat lossescan be taken into account while optimizing the network design based on anonlinear physics model. The transition of the automated design approach fromworst-case to multi-period shows a design progression from separate branchednetworks to a single integrated meshed network topology connecting allproducers. These integrated topologies emerge without imposing such structuresa priori. They increase network connectivity, and allow for more flexibleshifting of heat loads between different producers and heat consumers,resulting in more cost-effective use of heat. In a case study, this integrateddesign resulted in an increase in waste heat share of 42.8 % and a subsequentreduction in project cost of 17.9 %. We show how producer unavailability can beaccounted for in the automated design at the cost of a 3.1 % increase in thecost of backup capacity. The resulting optimized network designs of thisapproach connect multiple low temperature heat sources in a single integratednetwork achieving high waste heat utilization and redundancy, highlighting theapplicability of the approach to next-generation district heating networks."
    },
    {
        "link": "https://arxiv.org/abs/2401.15977",
        "title": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling",
        "authors": [
            "Xiaoyu Shi",
            "Zhaoyang Huang",
            "Fu-Yun Wang",
            "Weikang Bian",
            "Dasong Li",
            "Yi Zhang",
            "Manyuan Zhang",
            "Ka Chun Cheung",
            "Simon See",
            "Hongwei Qin",
            "Jifeng Da",
            "Hongsheng Li"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce Motion-I2V, a novel framework for consistent and controllableimage-to-video generation (I2V). In contrast to previous methods that directlylearn the complicated image-to-video mapping, Motion-I2V factorizes I2V intotwo stages with explicit motion modeling. For the first stage, we propose adiffusion-based motion field predictor, which focuses on deducing thetrajectories of the reference image's pixels. For the second stage, we proposemotion-augmented temporal attention to enhance the limited 1-D temporalattention in video latent diffusion models. This module can effectivelypropagate reference image's feature to synthesized frames with the guidance ofpredicted trajectories from the first stage. Compared with existing methods,Motion-I2V can generate more consistent videos even at the presence of largemotion and viewpoint variation. By training a sparse trajectory ControlNet forthe first stage, Motion-I2V can support users to precisely control motiontrajectories and motion regions with sparse trajectory and region annotations.This offers more controllability of the I2V process than solely relying ontextual instructions. Additionally, Motion-I2V's second stage naturallysupports zero-shot video-to-video translation. Both qualitative andquantitative comparisons demonstrate the advantages of Motion-I2V over priorapproaches in consistent and controllable image-to-video generation."
    },
    {
        "link": "https://arxiv.org/abs/2401.15978",
        "title": "Multilevel Markov Chain Monte Carlo with likelihood scaling for high-resolution data assimilation",
        "authors": [
            "Pieter Vanmechelen",
            "Geert Lombaert",
            "Giovanni Samaey"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We propose a multilevel Markov chain Monte Carlo (MCMC) method for theBayesian inference of random field parameters in PDEs using high-resolutiondata. Compared to existing multilevel MCMC methods, we additionally considerlevel-dependent data resolution and introduce a suitable likelihood scaling toenable consistent cross-level comparisons. We theoretically show that thisapproach attains the same convergence rates as when using level-independenttreatment of data, but at significantly reduced computational cost.Additionally, we show that assumptions of exponential covariance andlog-normality of random fields, widely held in multilevel Monte Carloliterature, can be extended to a wide range of covariance structures and randomfields. These results are illustrated using numerical experiments for a 2Dplane stress problem, where the Young's modulus is estimated fromdiscretisations of the displacement field."
    },
    {
        "link": "https://arxiv.org/abs/2401.15985",
        "title": "Dissecting the software-based measurement of CPU energy consumption: a comparative analysis",
        "authors": [
            "Guillaume Raffin",
            "Denis Trystram"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Every day, we experience the effects of the global warming: extreme weatherevents, major forest fires, storms, global warming, etc. The scientificcommunity acknowledges that this crisis is a consequence of human activitieswhere Information and Communications Technologies (ICT) are an increasinglyimportant contributor. Computer scientists need tools for measuring thefootprint of the code they produce. Running Average Power Limit (RAPL) is alow-level interface designed by Intel that provides a measure of the energyconsumption of a CPU (and more) without the need for additional hardware. Since2017, it is available on most computing devices, including non-Intel devicessuch as AMD processors. More and more people are using RAPL for energymeasurement, mostly like a black box without deep knowledge of its behaviour.In this paper, we propose to come back to the basic mechanisms that allow touse RAPL measurements and present a critical analysis of their operations. Foreach mechanism, we release a reference implementation in Rust that avoids thepitfalls we detected in existing tools, improving correctness, timing accuracyand performance. In addition to long-established methods, we explore thesuitability of the recent eBPF technology for working with RAPL. We alsoprovide an experimental study with multiple benchmarks and processor models inorder to evaluate the efficiency of the various mechanisms and their impact onparallel software. Our experiments show that no mechanism provides asignificant performance advantage over the others. However, they differsignificantly in terms of ease-of-use and resiliency. We believe that this workwill help the community to develop correct, resilient and lightweightmeasurement tools, based on the mechanism that suits their needs."
    },
    {
        "link": "https://arxiv.org/abs/2401.15987",
        "title": "Hand-Centric Motion Refinement for 3D Hand-Object Interaction via Hierarchical Spatial-Temporal Modeling",
        "authors": [
            "Yuze Hao",
            "Jianrong Zhang",
            "Tao Zhuo",
            "Fuan Wen",
            "Hehe Fan"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hands are the main medium when people interact with the world. Generatingproper 3D motion for hand-object interaction is vital for applications such asvirtual reality and robotics. Although grasp tracking or object manipulationsynthesis can produce coarse hand motion, this kind of motion is inevitablynoisy and full of jitter. To address this problem, we propose a data-drivenmethod for coarse motion refinement. First, we design a hand-centricrepresentation to describe the dynamic spatial-temporal relation between handsand objects. Compared to the object-centric representation, our hand-centricrepresentation is straightforward and does not require an ambiguous projectionprocess that converts object-based prediction into hand motion. Second, tocapture the dynamic clues of hand-object interaction, we propose a newarchitecture that models the spatial and temporal structure in a hierarchicalmanner. Extensive experiments demonstrate that our method outperforms previousmethods by a noticeable margin."
    },
    {
        "link": "https://arxiv.org/abs/2401.15989",
        "title": "Deep Embedding Clustering Driven by Sample Stability",
        "authors": [
            "Zhanwen Cheng",
            "Feijiang Li",
            "Jieting Wang",
            "Yuhua Qian"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Deep clustering methods improve the performance of clustering tasks byjointly optimizing deep representation learning and clustering. While numerousdeep clustering algorithms have been proposed, most of them rely onartificially constructed pseudo targets for performing clustering. Thisconstruction process requires some prior knowledge, and it is challenging todetermine a suitable pseudo target for clustering. To address this issue, wepropose a deep embedding clustering algorithm driven by sample stability(DECS), which eliminates the requirement of pseudo targets. Specifically, westart by constructing the initial feature space with an autoencoder and thenlearn the cluster-oriented embedding feature constrained by sample stability.The sample stability aims to explore the deterministic relationship betweensamples and all cluster centroids, pulling samples to their respective clustersand keeping them away from other clusters with high determinacy. We analyzedthe convergence of the loss using Lipschitz continuity in theory, whichverifies the validity of the model. The experimental results on five datasetsillustrate that the proposed method achieves superior performance compared tostate-of-the-art clustering approaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.15993",
        "title": "Continuous Target Speech Extraction: Enhancing Personalized Diarization and Extraction on Complex Recordings",
        "authors": [
            "He Zhao",
            "Hangting Chen",
            "Jianwei Yu",
            "Yuehai Wang"
        ],
        "primary_subject": "Sound (cs.SD)",
        "abstract": "Target speaker extraction (TSE) aims to extract the target speaker's voicefrom the input mixture. Previous studies have concentrated on high-overlappingscenarios. However, real-world applications usually meet more complex scenarioslike variable speaker overlapping and target speaker absence. In this paper, weintroduces a framework to perform continuous TSE (C-TSE), comprising a targetspeaker voice activation detection (TSVAD) and a TSE model. This frameworksignificantly improves TSE performance on similar speakers and enhancespersonalization, which is lacking in traditional diarization methods. Indetail, unlike conventional TSVAD deployed to refine the diarization results,the proposed Attention-target speaker voice activation detection (A-TSVAD)directly generates timestamps of the target speaker. We also explore somedifferent integration methods of A-TSVAD and TSE by comparing the cascaded andparallel methods. The framework's effectiveness is assessed using a range ofmetrics, including diarization and enhancement metrics. Our experimentsdemonstrate that A-TSVAD outperforms conventional methods in reducingdiarization errors. Furthermore, the integration of A-TSVAD and TSE in asequential cascaded manner further enhances extraction accuracy."
    },
    {
        "link": "https://arxiv.org/abs/2401.15994",
        "title": "Extracting and visualizing a new classification system for Colombia's National Administrative Department of Statistics. A visual analytics framework case study",
        "authors": [
            "Pierre Raimbaud",
            "Jaime Camilo Espitia Castillo",
            "John Guerra-Gomez"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "In a world filled with data, it is expected for a nation to take decisionsinformed by data. However, countries need to first collect and publish suchdata in a way meaningful for both citizens and policy makers. A good thematicclassification could be instrumental in helping users navigate and find theright resources on a rich data repository as the one collected by Colombia'sNational Administrative Department of Statistics (DANE). The Visual AnalyticsFramework is a methodology for conducting visual analysis developed by T.Munzner et al. [T. Munzner, Visualization Analysis and Design, A K PetersVisualization Series, 1, 2014] that could help with this task. This paperpresents a case study applying such framework conducted to help the DANE bettervisualize their data repository, and present a more understandableclassification of it. It describes three main analysis tasks identified, theproposed solutions and the collection of insights generated from them."
    },
    {
        "link": "https://arxiv.org/abs/2401.15996",
        "title": "AccessLens: Auto-detecting Inaccessibility of Everyday Objects",
        "authors": [
            "Nahyun Kwon",
            "Qian Lu",
            "Muhammad Hasham Qazi",
            "Joanne Liu",
            "Changhoon Oh",
            "Shu Kong",
            "Jeeeun Kim"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In our increasingly diverse society, everyday physical interfaces oftenpresent barriers, impacting individuals across various contexts. Thisoversight, from small cabinet knobs to identical wall switches that can posedifferent contextual challenges, highlights an imperative need for solutions.Leveraging low-cost 3D-printed augmentations such as knob magnifiers andtactile labels seems promising, yet the process of discovering unrecognizedbarriers remains challenging because disability is context-dependent. Weintroduce AccessLens, an end-to-end system designed to identify inaccessibleinterfaces in daily objects, and recommend 3D-printable augmentations foraccessibility enhancement. Our approach involves training a detector using thenovel AccessDB dataset designed to automatically recognize 21 distinctInaccessibility Classes (e.g., bar-small and round-rotate) within 6 commonobject categories (e.g., handle and knob). AccessMeta serves as a robust way tobuild a comprehensive dictionary linking these accessibility classes toopen-source 3D augmentation designs. Experiments demonstrate our detector'sperformance in detecting inaccessible objects."
    },
    {
        "link": "https://arxiv.org/abs/2401.16001",
        "title": "LESSON: Multi-Label Adversarial False Data Injection Attack for Deep Learning Locational Detection",
        "authors": [
            "Jiwei Tian",
            "Chao Shen",
            "Buhong Wang",
            "Xiaofang Xia",
            "Meng Zhang",
            "Chenhao Lin",
            "Qian Li"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Deep learning methods can not only detect false data injection attacks (FDIA)but also locate attacks of FDIA. Although adversarial false data injectionattacks (AFDIA) based on deep learning vulnerabilities have been studied in thefield of single-label FDIA detection, the adversarial attack and defenseagainst multi-label FDIA locational detection are still not involved. To bridgethis gap, this paper first explores the multi-label adversarial example attacksagainst multi-label FDIA locational detectors and proposes a generalmulti-label adversarial attack framework, namely muLti-labEl adverSarial falSedata injectiON attack (LESSON). The proposed LESSON attack framework includesthree key designs, namely Perturbing State Variables, Tailored Loss FunctionDesign, and Change of Variables, which can help find suitable multi-labeladversarial perturbations within the physical constraints to circumvent bothBad Data Detection (BDD) and Neural Attack Location (NAL). Four typical LESSONattacks based on the proposed framework and two dimensions of attack objectivesare examined, and the experimental results demonstrate the effectiveness of theproposed attack framework, posing serious and pressing security concerns insmart grids."
    },
    {
        "link": "https://arxiv.org/abs/2401.16004",
        "title": "Model predictive control of wakes for wind farm power tracking",
        "authors": [
            "Arnold Sterle",
            "Christian A. Hans",
            "J\u00f6rg Raisch"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "In this paper, a model predictive control scheme for wind farms is presented.Our approach considers wake dynamics including their influence on local windconditions and allows to track a given power reference. In detail, a Gaussianwake model is used in combination with observation points that carry windcondition information. This allows to estimate the rotor effective wind speedsat downstream turbines based on which we deduce their power output. Throughdifferent approximation methods, the associated finite horizon nonlinearoptimization problem is reformulated in a mixed-integerquadratically-constrained quadratic program fashion. By solving thereformulated problem online, optimal yaw angles and axial induction factors arefound. Closed-loop simulations indicate good power tracking capabilities over awide range of power setpoints while distributing wind turbine infeed evenlyamong all units. Additionally, the simulation results underline real timecapabilities of our approach."
    },
    {
        "link": "https://arxiv.org/abs/2401.16009",
        "title": "SpectroGLY: A Low-Cost IoT-Based Ecosystem for the Detection of Glyphosate Residues in Waters",
        "authors": [
            "Javier Aira",
            "Teresa Olivares",
            "Francisco M. Delicado"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Glyphosate contamination in waters is becoming a major health problem thatneeds to be urgently addressed, as accidental spraying, drift or leakage ofthis highly water-soluble herbicide can impact aquatic ecosystems. Researchersare increasingly concerned about exposure to glyphosate and the risks its posesto human health, since it may cause substantial damage, even in small doses.The detection of glyphosate residues in waters is not a simple task, as itrequires complex and expensive equipment and qualified personnel. Newtechnological tools need to be designed and developed, based on proven, butalso cost-efficient, agile and user-friendly, analytical techniques, which canbe used in the field and in the lab, enabled by connectivity and multi-platformsoftware applications. This paper presents the design, development and testingof an innovative low-cost VIS-NIR (Visible and Near-Infrared) spectrometer(called SpectroGLY), based on IoT (Internet of Things) technologies, whichallows potential glyphosate contamination in waters to be detected. SpectroGLYcombines the functional concept of a traditional lab spectrometer with the IoTtechnological concept, enabling the integration of several connectivity optionsfor rural and urban settings and digital visualization and monitoring platforms(Mobile App and Dashboard Web). Thanks to its portability, it can be used inany context and provides results in 10 minutes. Additionally, it is unnecessaryto transfer the sample to a laboratory (optimizing time, costs and the capacityfor corrective actions by the authorities). In short, this paper proposes aninnovative, low-cost, agile and highly promising solution to avoid potentialintoxications that may occur due to ingestion of water contaminated by thisherbicide."
    },
    {
        "link": "https://arxiv.org/abs/2401.16011",
        "title": "GPS: Graph Contrastive Learning via Multi-scale Augmented Views from Adversarial Pooling",
        "authors": [
            "Wei Ju",
            "Yiyang Gu",
            "Zhengyang Mao",
            "Ziyue Qiao",
            "Yifang Qin",
            "Xiao Luo",
            "Hui Xiong",
            "Ming Zhang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Self-supervised graph representation learning has recently shown considerablepromise in a range of fields, including bioinformatics and social networks. Alarge number of graph contrastive learning approaches have shown promisingperformance for representation learning on graphs, which train models bymaximizing agreement between original graphs and their augmented views (i.e.,positive views). Unfortunately, these methods usually involve pre-definedaugmentation strategies based on the knowledge of human experts. Moreover,these strategies may fail to generate challenging positive views to providesufficient supervision signals. In this paper, we present a novel approachnamed Graph Pooling ContraSt (GPS) to address these issues. Motivated by thefact that graph pooling can adaptively coarsen the graph with the removal ofredundancy, we rethink graph pooling and leverage it to automatically generatemulti-scale positive views with varying emphasis on providing challengingpositives and preserving semantics, i.e., strongly-augmented view andweakly-augmented view. Then, we incorporate both views into a joint contrastivelearning framework with similarity learning and consistency learning, where ourpooling module is adversarially trained with respect to the encoder foradversarial robustness. Experiments on twelve datasets on both graphclassification and transfer learning tasks verify the superiority of theproposed method over its counterparts."
    },
    {
        "link": "https://arxiv.org/abs/2401.16012",
        "title": "Finding Challenging Metaphors that Confuse Pretrained Language Models",
        "authors": [
            "Yucheng Li",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Metaphors are considered to pose challenges for a wide spectrum of NLP tasks.This gives rise to the area of computational metaphor processing. However, itremains unclear what types of metaphors challenge current state-of-the-artmodels. In this paper, we test various NLP models on the VUA metaphor datasetand quantify to what extent metaphors affect models' performance on variousdownstream tasks. Analysis reveals that VUA includes a large number ofmetaphors that pose little difficulty to downstream tasks. We would like toshift the attention of researchers away from these metaphors to instead focuson challenging metaphors. To identify hard metaphors, we propose an automaticpipeline that identifies metaphors that challenge a particular model. Ouranalysis demonstrates that our detected hard metaphors contrast significantlywith VUA and reduce the accuracy of machine translation by 16\\%, QA performanceby 4\\%, NLI by 7\\%, and metaphor identification recall by over 14\\% for variouspopular NLP systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.16013",
        "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning",
        "authors": [
            "Jianlan Luo",
            "Zheyuan Hu",
            "Charles Xu",
            "You Liang Tan",
            "Jacob Berg",
            "Archit Sharma",
            "Stefan Schaal",
            "Chelsea Finn",
            "Abhishek Gupta",
            "Sergey Levine"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "In recent years, significant progress has been made in the field of roboticreinforcement learning (RL), enabling methods that handle complex imageobservations, train in the real world, and incorporate auxiliary data, such asdemonstrations and prior experience. However, despite these advances, roboticRL remains hard to use. It is acknowledged among practitioners that theparticular implementation details of these algorithms are often just asimportant (if not more so) for performance as the choice of algorithm. We positthat a significant challenge to widespread adoption of robotic RL, as well asfurther development of robotic RL methods, is the comparative inaccessibilityof such methods. To address this challenge, we developed a carefullyimplemented library containing a sample efficient off-policy deep RL method,together with methods for computing rewards and resetting the environment, ahigh-quality controller for a widely-adopted robot, and a number of challengingexample tasks. We provide this library as a resource for the community,describe its design choices, and present experimental results. Perhapssurprisingly, we find that our implementation can achieve very efficientlearning, acquiring policies for PCB board assembly, cable routing, and objectrelocation between 25 to 50 minutes of training per policy on average,improving over state-of-the-art results reported for similar tasks in theliterature. These policies achieve perfect or near-perfect success rates,extreme robustness even under perturbations, and exhibit emergent recovery andcorrection behaviors. We hope that these promising results and our high-qualityopen-source implementation will provide a tool for the robotics community tofacilitate further developments in robotic RL. Our code, documentation, andvideos can be found at https://serl-robot.github.io/"
    },
    {
        "link": "https://arxiv.org/abs/2401.16015",
        "title": "Querying Fault and Attack Trees: Property Specification on a Water Network",
        "authors": [
            "Stefano M. Nicoletti",
            "Milan Lopuha\u00e4-Zwakenberg",
            "E. Moritz Hahn",
            "Mari\u00eblle Stoelinga"
        ],
        "primary_subject": "Logic in Computer Science (cs.LO)",
        "abstract": "We provide an overview of three different query languages whose objective isto specify properties on the highly popular formalisms of fault trees (FTs) andattack trees (ATs). These are BFL, a Boolean Logic for FTs, PFL, aprobabilistic extension of BFL and ATM, a logic for security metrics on ATs. Wevalidate the framework composed by these three logics by applying them to thecase study of a water distribution network. We extend the FT for this network -found in the literature - and we propose to model the system under analysiswith the Fault Trees/Attack Trees (FT/ATs) formalism, combining both FTs andATs in a unique model. Furthermore, we propose a novel combination of theshowcased logics to account for queries that jointly consider both the FT andthe AT of the model, integrating influences of attacks on failure probabilitiesof different components. Finally, we extend the domain specific language forPFL with novel constructs to capture the interplay between metrics of attacks -e.g., \"cost\", success probabilities - and failure probabilities in the system."
    },
    {
        "link": "https://arxiv.org/abs/2401.16024",
        "title": "Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules in Vector-symbolic Architectures",
        "authors": [
            "Michael Hersche",
            "Francesco di Stefano",
            "Thomas Hofmann",
            "Abu Sebastian",
            "Abbas Rahimi"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Abstract reasoning is a cornerstone of human intelligence, and replicating itwith artificial intelligence (AI) presents an ongoing challenge. This studyfocuses on efficiently solving Raven's progressive matrices (RPM), a visualtest for assessing abstract reasoning abilities, by using distributedcomputation and operators provided by vector-symbolic architectures (VSA).Instead of hard-coding the rule formulations associated with RPMs, our approachcan learn the VSA rule formulations (hence the name Learn-VRF) with just onepass through the training data. Yet, our approach, with compact parameters,remains transparent and interpretable. Learn-VRF yields accurate predictions onI-RAVEN's in-distribution data, and exhibits strong out-of-distributioncapabilities concerning unseen attribute-rule pairs, significantlyoutperforming pure connectionist baselines including large language models. Ourcode is available athttps://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations."
    },
    {
        "link": "https://arxiv.org/abs/2401.16025",
        "title": "Simple Policy Optimization",
        "authors": [
            "Zhengpeng Xie"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "PPO (Proximal Policy Optimization) algorithm has demonstrated excellentperformance in many fields, and it is considered as a simple version of TRPO(Trust Region Policy Optimization) algorithm. However, the ratio clippingoperation in PPO may not always effectively enforce the trust regionconstraints, this can be a potential factor affecting the stability of thealgorithm. In this paper, we propose SPO (Simple Policy Optimization)algorithm, which introduces a novel clipping method for KL divergence betweenthe old and current policies. SPO can effectively enforce the trust regionconstraints in almost all environments, while still maintaining the simplicityof a first-order algorithm. Comparative experiments in Atari 2600 environmentsshow that SPO sometimes provides stronger performance than PPO. Code isavailable at https://github.com/MyRepositories-hub/Simple-Policy-Optimization."
    },
    {
        "link": "https://arxiv.org/abs/2401.16027",
        "title": "Domain adaptation strategies for 3D reconstruction of the lumbar spine using real fluoroscopy data",
        "authors": [
            "Sascha Jecklin",
            "Youyang Shen",
            "Amandine Gout",
            "Daniel Suter",
            "Lilian Calvet",
            "Lukas Zingg",
            "Jennifer Straub",
            "Nicola Alessandro Cavalcanti",
            "Mazda Farshad",
            "Philipp F\u00fcrnstahl",
            "Hooman Esfandiari"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This study tackles key obstacles in adopting surgical navigation inorthopedic surgeries, including time, cost, radiation, and workflow integrationchallenges. Recently, our work X23D showed an approach for generating 3Danatomical models of the spine from only a few intraoperative fluoroscopicimages. This negates the need for conventional registration-based surgicalnavigation by creating a direct intraoperative 3D reconstruction of theanatomy. Despite these strides, the practical application of X23D has beenlimited by a domain gap between synthetic training data and real intraoperativeimages.In response, we devised a novel data collection protocol for a paired datasetconsisting of synthetic and real fluoroscopic images from the sameperspectives. Utilizing this dataset, we refined our deep learning model viatransfer learning, effectively bridging the domain gap between synthetic andreal X-ray data. A novel style transfer mechanism also allows us to convertreal X-rays to mirror the synthetic domain, enabling our in-silico-trained X23Dmodel to achieve high accuracy in real-world settings.Our results demonstrated that the refined model can rapidly generate accurate3D reconstructions of the entire lumbar spine from as few as threeintraoperative fluoroscopic shots. It achieved an 84% F1 score, matching theaccuracy of our previous synthetic data-based research. Additionally, with acomputational time of only 81.1 ms, our approach provides real-timecapabilities essential for surgery integration.Through examining ideal imaging setups and view angle dependencies, we'vefurther confirmed our system's practicality and dependability in clinicalsettings. Our research marks a significant step forward in intraoperative 3Dreconstruction, offering enhancements to surgical planning, navigation, androbotics."
    },
    {
        "link": "https://arxiv.org/abs/2401.16028",
        "title": "A blockchain-based e-goverment service for Quantity Surveyors",
        "authors": [
            "\u00c1ngel F. Alcaide",
            "Carlos N\u00fanez-G\u00f3mez",
            "Francisco M. Delicado",
            "Carmen Carri\u00f3n",
            "M. Blanca Caminero"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "In Spain, quantity surveyors are entitled to carry out official cadastralsurveys, attestations, and certificate issuing according to a well-definedprofessional code. Official Associations of Quantity Surveyors and TechnicalArchitects (COAAT) are responsible for endorsing the documentation related toactions performed on buildings. An e-platform that enables immutability,traceability, and a unique property record among all the Spanish COAATs, withan affordable cost, is essential to streamline the involved processes.The blockchain technology and smart contracts have recently emerged aspromising solutions for e-government services due to the inherent featuresprovided by the technology. In this paper, we identify the design goals andpropose a blockchain-based e-government system for the electronic management ofthe documentation generated, submitted, and validated by the Spanish COAATs,namely, the COAATChain. The proposal has been deployed and evaluated on theBinance testnet blockchain, in order to assess its affordability."
    },
    {
        "link": "https://arxiv.org/abs/2401.16035",
        "title": "Second Order Kinematic Surface Fitting in Anatomical Structures",
        "authors": [
            "Wilhelm Wimmer",
            "Herv\u00e9 Delingette"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Symmetry detection and morphological classification of anatomical structuresplay pivotal roles in medical image analysis. The application of kinematicsurface fitting, a method for characterizing shapes through parametricstationary velocity fields, has shown promising results in computer vision andcomputer-aided design. However, existing research has predominantly focused onfirst order rotational velocity fields, which may not adequately capture theintricate curved and twisted nature of anatomical structures. To address thislimitation, we propose an innovative approach utilizing a second order velocityfield for kinematic surface fitting. This advancement accommodates higherrotational shape complexity and improves the accuracy of symmetry detection inanatomical structures. We introduce a robust fitting technique and validate itsperformance through testing on synthetic shapes and real anatomical structures.Our method not only enables the detection of curved rotational symmetries (corelines) but also facilitates morphological classification by deriving intrinsicshape parameters related to curvature and torsion. We illustrate the usefulnessof our technique by categorizing the shape of human cochleae in terms of theintrinsic velocity field parameters. The results showcase the potential of ourmethod as a valuable tool for medical image analysis, contributing to theassessment of complex anatomical shapes."
    },
    {
        "link": "https://arxiv.org/abs/2401.16045",
        "title": "Type-based Neural Link Prediction Adapter for Complex Query Answering",
        "authors": [
            "Lingning Song",
            "Yi Zu",
            "Shan Lu",
            "Jieyue He"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Answering complex logical queries on incomplete knowledge graphs (KGs) is afundamental and challenging task in multi-hop reasoning. Recent work definesthis task as an end-to-end optimization problem, which significantly reducesthe training cost and enhances the generalization of the model by a pretrainedlink predictors for query answering. However, most existing proposals ignorethe critical semantic knowledge inherently available in KGs, such as typeinformation, which could help answer complex logical queries. To this end, wepropose TypE-based Neural Link Prediction Adapter (TENLPA), a novel model thatconstructs type-based entity-relation graphs to discover the latentrelationships between entities and relations by leveraging type information inKGs. Meanwhile, in order to effectively combine type information with complexlogical queries, an adaptive learning mechanism is introduced, which is trainedby back-propagating during the complex query answering process to achieveadaptive adjustment of neural link predictors. Experiments on 3 standarddatasets show that TENLPA model achieves state-of-the-art performance oncomplex query answering with good generalization and robustness."
    },
    {
        "link": "https://arxiv.org/abs/2401.16051",
        "title": "Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation",
        "authors": [
            "Jie Liu",
            "Wenzhe Yin",
            "Haochen Wang",
            "Yunlu CHen",
            "Jan-Jakob Sonke",
            "Efstratios Gavves"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Few-shot point cloud segmentation seeks to generate per-point masks forpreviously unseen categories, using only a minimal set of annotated pointclouds as reference. Existing prototype-based methods rely on supportprototypes to guide the segmentation of query point clouds, but they encounterchallenges when significant object variations exist between the supportprototypes and query features. In this work, we present dynamic prototypeadaptation (DPA), which explicitly learns task-specific prototypes for eachquery point cloud to tackle the object variation problem. DPA achieves theadaptation through prototype rectification, aligning vanilla prototypes fromsupport with the query feature distribution, and prototype-to-query attention,extracting task-specific context from query point clouds. Furthermore, weintroduce a prototype distillation regularization term, enabling knowledgetransfer between early-stage prototypes and their deeper counterparts duringadaption. By iteratively applying these adaptations, we generate task-specificprototypes for accurate mask predictions on query point clouds. Extensiveexperiments on two popular benchmarks show that DPA surpasses state-of-the-artmethods by a significant margin, e.g., 7.43\\% and 6.39\\% under the 2-way 1-shotsetting on S3DIS and ScanNet, respectively. Code is available athttps://github.com/jliu4ai/DPA."
    },
    {
        "link": "https://arxiv.org/abs/2401.16055",
        "title": "Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing",
        "authors": [
            "Vil\u00e9m Zouhar"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "In learning-based functionality stealing, the attacker is trying to build alocal model based on the victim's outputs. The attacker has to make choicesregarding the local model's architecture, optimization method and, specificallyfor NLP models, subword vocabulary, such as BPE. On the machine translationtask, we explore (1) whether the choice of the vocabulary plays a role in modelstealing scenarios and (2) if it is possible to extract the victim'svocabulary. We find that the vocabulary itself does not have a large effect onthe local model's performance. Given gray-box model access, it is possible tocollect the victim's vocabulary by collecting the outputs (detokenized subwordson the output). The results of the minimum effect of vocabulary choice areimportant more broadly for black-box knowledge distillation."
    },
    {
        "link": "https://arxiv.org/abs/2401.16058",
        "title": "Neuromorphic Valence and Arousal Estimation",
        "authors": [
            "Lorenzo Berlincioni",
            "Luca Cultrera",
            "Federico Becattini",
            "Alberto Del Bimbo"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recognizing faces and their underlying emotions is an important aspect ofbiometrics. In fact, estimating emotional states from faces has been tackledfrom several angles in the literature. In this paper, we follow the novel routeof using neuromorphic data to predict valence and arousal values from faces.Due to the difficulty of gathering event-based annotated videos, we leverage anevent camera simulator to create the neuromorphic counterpart of an existingRGB dataset. We demonstrate that not only training models on simulated data canstill yield state-of-the-art results in valence-arousal estimation, but alsothat our trained models can be directly applied to real data without furthertraining to address the downstream task of emotion recognition. In the paper wepropose several alternative models to solve the task, both frame-based andvideo-based."
    },
    {
        "link": "https://arxiv.org/abs/2401.16063",
        "title": "Shannon Capacity of Channels with Markov Insertions, Deletions and Substitutions",
        "authors": [
            "Ruslan Morozov",
            "Tolga M. Duman"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We consider channels with synchronization errors modeled as insertions anddeletions. A classical result for such channels is the information stability ofsuch channels, hence the existence of the Shannon capacity, when thesynchronization errors are memoryless. In this paper, we extend this result tothe case where the insertions and deletions have memory. Specifically, weassume that the synchronization errors are governed by a stationary and ergodicfinite state Markov chain, and prove that mutual information capacity of suchchannels exist, and it is equal to its coding capacity, showing that thereexists a coding scheme which achieves this limit."
    },
    {
        "link": "https://arxiv.org/abs/2401.16072",
        "title": "A symmetric silicon microring resonator optical crossbar array for accelerated inference and training in deep learning",
        "authors": [
            "Rui Tang",
            "Shuhei Ohno",
            "Ken Tanizawa",
            "Kazuhiro Ikeda",
            "Makoto Okano",
            "Kasidit Toprasertpong",
            "Shinichi Takagi",
            "Mitsuru Takenaka"
        ],
        "primary_subject": "Emerging Technologies (cs.ET)",
        "abstract": "Photonic integrated circuits are emerging as a promising platform foraccelerating matrix multiplications in deep learning, leveraging the inherentparallel nature of light. Although various schemes have been proposed anddemonstrated to realize such photonic matrix accelerators, the in-situ trainingof artificial neural networks using photonic accelerators remains challengingdue to the difficulty of direct on-chip backpropagation on a photonic chip. Inthis work, we propose a silicon microring resonator (MRR) optical crossbararray with a symmetric structure that allows for simple on-chipbackpropagation, potentially enabling the acceleration of both the inferenceand training phases of deep learning. We demonstrate a 4 \u00d7 4 circuit ona Si-on-insulator (SOI) platform and use it to perform inference tasks of asimple neural network for classifying Iris flowers, achieving a classificationaccuracy of 93.3%. Furthermore, we train the neural network using simulatedon-chip backpropagation and achieve an accuracy of 91.1% in the same inferencetask after training. This work contributes to the realization of compact andenergy-efficient photonic accelerators for deep learning."
    },
    {
        "link": "https://arxiv.org/abs/2401.16076",
        "title": "Find the Cliffhanger: Multi-Modal Trailerness in Soap Operas",
        "authors": [
            "Carlo Bretti",
            "Pascal Mettes",
            "Hendrik Vincent Koops",
            "Daan Odijk",
            "Nanne van Noord"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Creating a trailer requires carefully picking out and piecing together briefenticing moments out of a longer video, making it a chal- lenging andtime-consuming task. This requires selecting moments based on both visual anddialogue information. We introduce a multi-modal method for predicting thetrailerness to assist editors in selecting trailer- worthy moments fromlong-form videos. We present results on a newly introduced soap opera dataset,demonstrating that predicting trailerness is a challenging task that benefitsfrom multi-modal information. Code is available athttps://github.com/carlobretti/cliffhanger"
    },
    {
        "link": "https://arxiv.org/abs/2401.16078",
        "title": "Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation",
        "authors": [
            "V\u00edctor M. S\u00e1nchez-Cartagena",
            "Juan Antonio P\u00e9rez-Ortiz",
            "Felipe S\u00e1nchez-Mart\u00ednez"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This paper studies the effects of word-level linguistic annotations inunder-resourced neural machine translation, for which there is incompleteevidence in the literature. The study covers eight language pairs, differenttraining corpus sizes, two architectures, and three types of annotation: dummytags (with no linguistic information at all), part-of-speech tags, andmorpho-syntactic description tags, which consist of part of speech andmorphological features. These linguistic annotations are interleaved in theinput or output streams as a single tag placed before each word. In order tomeasure the performance under each scenario, we use automatic evaluationmetrics and perform automatic error classification. Our experiments show that,in general, source-language annotations are helpful and morpho-syntacticdescriptions outperform part of speech for some language pairs. On thecontrary, when words are annotated in the target language, part-of-speech tagssystematically outperform morpho-syntactic description tags in terms ofautomatic evaluation metrics, even though the use of morpho-syntacticdescription tags improves the grammaticality of the output. We provide adetailed analysis of the reasons behind this result."
    },
    {
        "link": "https://arxiv.org/abs/2401.16086",
        "title": "Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation",
        "authors": [
            "V\u00edctor M. S\u00e1nchez-Cartagena",
            "Miquel Espl\u00e0-Gomis",
            "Juan Antonio P\u00e9rez-Ortiz",
            "Felipe S\u00e1nchez-Mart\u00ednez"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "When the amount of parallel sentences available to train a neural machinetranslation is scarce, a common practice is to generate new synthetic trainingsamples from them. A number of approaches have been proposed to producesynthetic parallel sentences that are similar to those in the parallel dataavailable. These approaches work under the assumption that non-fluenttarget-side synthetic training samples can be harmful and may deterioratetranslation performance. Even so, in this paper we demonstrate that synthetictraining samples with non-fluent target sentences can improve translationperformance if they are used in a multilingual machine translation framework asif they were sentences in another language. We conducted experiments on tenlow-resource and four high-resource translation tasks and found out that thissimple approach consistently improves translation performance as compared tostate-of-the-art methods for generating synthetic training samples similar tothose found in corpora. Furthermore, this improvement is independent of thesize of the original training corpus, the resulting systems are much morerobust against domain shift and produce less hallucinations."
    },
    {
        "link": "https://arxiv.org/abs/2401.16087",
        "title": "High Resolution Image Quality Database",
        "authors": [
            "Huang Huang",
            "Qiang Wan",
            "Jari Korhonen"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With technology for digital photography and high resolution displays rapidlyevolving and gaining popularity, there is a growing demand for blind imagequality assessment (BIQA) models for high resolution images. Unfortunately, thepublicly available large scale image quality databases used for training BIQAmodels contain mostly low or general resolution images. Since image resizingaffects image quality, we assume that the accuracy of BIQA models trained onlow resolution images would not be optimal for high resolution images.Therefore, we created a new high resolution image quality database (HRIQ),consisting of 1120 images with resolution of 2880x2160 pixels. We conducted asubjective study to collect the subjective quality ratings for HRIQ in acontrolled laboratory setting, resulting in accurate MOS at high resolution. Todemonstrate the importance of a high resolution image quality database fortraining BIQA models to predict mean opinion scores (MOS) of high resolutionimages accurately, we trained and tested several traditional and deep learningbased BIQA methods on different resolution versions of our database. Thedatabase is publicly available in https://github.com/jarikorhonen/hriq."
    },
    {
        "link": "https://arxiv.org/abs/2401.16088",
        "title": "Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
        "authors": [
            "Andrew Bell",
            "Joao Fonseca",
            "Carlo Abrate",
            "Francesco Bonchi",
            "Julia Stoyanovich"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Algorithmic recourse -- providing recommendations to those affectednegatively by the outcome of an algorithmic system on how they can take actionand change that outcome -- has gained attention as a means of giving personsagency in their interactions with artificial intelligence (AI) systems. Recentwork has shown that even if an AI decision-making classifier is ``fair''(according to some reasonable criteria), recourse itself may be unfair due todifferences in the initial circumstances of individuals, compoundingdisparities for marginalized populations and requiring them to exert moreeffort than others. There is a need to define more methods and metrics forevaluating fairness in recourse that span a range of normative views of theworld, and specifically those that take into account time. Time is a criticalelement in recourse because the longer it takes an individual to act, the morethe setting may change due to model or data drift.This paper seeks to close this research gap by proposing two notions offairness in recourse that are in normative alignment with substantive equalityof opportunity, and that consider time. The first considers the (oftenrepeated) effort individuals exert per successful recourse event, and thesecond considers time per successful recourse event. Building upon anagent-based framework for simulating recourse, this paper demonstrates how mucheffort is needed to overcome disparities in initial circumstances. We thenproposes an intervention to improve the fairness of recourse by rewardingeffort, and compare it to existing strategies."
    },
    {
        "link": "https://arxiv.org/abs/2401.16092",
        "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You",
        "authors": [
            "Felix Friedrich",
            "Katharina H\u00e4mmerl",
            "Patrick Schramowski",
            "Jindrich Libovicky",
            "Kristian Kersting",
            "Alexander Fraser"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Text-to-image generation models have recently achieved astonishing results inimage quality, flexibility, and text alignment and are consequently employed ina fast-growing number of applications. Through improvements in multilingualabilities, a larger community now has access to this kind of technology. Yet,as we will show, multilingual models suffer similarly from (gender) biases asmonolingual models. Furthermore, the natural expectation is that these modelswill provide similar results across languages, but this is not the case andthere are important differences between languages. Thus, we propose a novelbenchmark MAGBIG intending to foster research in multilingual models withoutgender bias. We investigate whether multilingual T2I models magnify gender biaswith MAGBIG. To this end, we use multilingual prompts requesting portraitimages of persons of a certain occupation or trait (using adjectives). Ourresults show not only that models deviate from the normative assumption thateach gender should be equally likely to be generated, but that there are alsobig differences across languages. Furthermore, we investigate promptengineering strategies, i.e. the use of indirect, neutral formulations, as apossible remedy for these biases. Unfortunately, they help only to a limitedextent and result in worse text-to-image alignment. Consequently, this workcalls for more research into diverse representations across languages in imagegenerators."
    },
    {
        "link": "https://arxiv.org/abs/2401.16094",
        "title": "Federated unsupervised random forest for privacy-preserving patient stratification",
        "authors": [
            "Bastian Pfeifer",
            "Christel Sirocchi",
            "Marcus D. Bloice",
            "Markus Kreuzthaler",
            "Martin Urschler"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In the realm of precision medicine, effective patient stratification anddisease subtyping demand innovative methodologies tailored for multi-omicsdata. Clustering techniques applied to multi-omics data have becomeinstrumental in identifying distinct subgroups of patients, enabling afiner-grained understanding of disease variability. This work establishes apowerful framework for advancing precision medicine through unsupervisedrandom-forest-based clustering and federated computing. We introduce a novelmulti-omics clustering approach utilizing unsupervised random-forests. Theunsupervised nature of the random forest enables the determination ofcluster-specific feature importance, unraveling key molecular contributors todistinct patient groups. Moreover, our methodology is designed for federatedexecution, a crucial aspect in the medical domain where privacy concerns areparamount. We have validated our approach on machine learning benchmark datasets as well as on cancer data from The Cancer Genome Atlas (TCGA). Our methodis competitive with the state-of-the-art in terms of disease subtyping, but atthe same time substantially improves the cluster interpretability. Experimentsindicate that local clustering performance can be improved through federatedcomputing."
    },
    {
        "link": "https://arxiv.org/abs/2401.16095",
        "title": "On the Separability Problem of VASS Reachability Languages",
        "authors": [
            "Eren Keskin",
            "Roland Meyer"
        ],
        "primary_subject": "Formal Languages and Automata Theory (cs.FL)",
        "abstract": "We show that the regular separability problem of VASS reachability languagesis decidable and F\u03c9-complete. At the heart of our decisionprocedure are doubly-marked graph transition sequences, a new proof object thattracks a suitable product of the VASS we wish to separate. We give adecomposition algorithm for DMGTS that not only achieves perfectness as knownfrom MGTS, but also a new property called faithfulness. Faithfulness allows usto construct, from a regular separator for the Z-versions of theVASS, a regular separator for the N-versions. Behind faithfulness isthe insight that, for separability, it is sufficient to track the counters ofone VASS modulo a large number that is determined by the decomposition."
    },
    {
        "link": "https://arxiv.org/abs/2401.16097",
        "title": "Pushing the Limits: Concurrency Detection in Acyclic, Live, and 1-Safe Free-Choice Nets in",
        "authors": [
            "Thomas M. Prinz",
            "Julien Klaus",
            "Nick R.T.P. van Beest"
        ],
        "primary_subject": "Data Structures and Algorithms (cs.DS)",
        "abstract": "Concurrency is an important aspect of (Petri) nets to describe and simulatethe behavior of complex systems. Knowing which places and transitions could beexecuted in parallel helps to understand nets and enables analysis techniquesand the computation of other properties, such as causality, exclusivity, etc..All techniques based on concurrency detection depend on the efficiency of thisdetection methodology. Kovalyov and Esparza have developed algorithms thatcompute all concurrent places in O((P+T)TP2) for live nets (whereP and T are the numbers of places and transitions) and inO(P(P+T)2) for live free-choice nets. Although these algorithms havea reasonably good computational complexity, large numbers of concurrent pairsof nodes may still lead to long computation times. Furthermore, both algorithmscannot be parallelized without additional effort. This paper complements thepalette of concurrency detection algorithms with the Concurrent Paths (CP)algorithm for safe, live, free-choice nets. The algorithm allowsparallelization and has a worst-case computational complexity ofO((P+T)2) for acyclic nets and of O(P3+PT2) for cyclicnets. Although the computational complexity of cyclic nets has not improved,the evaluation shows the benefits of CP, especially, if the net contains manynodes in concurrency relation."
    },
    {
        "link": "https://arxiv.org/abs/2401.16102",
        "title": "Flexible Parallel Neural Network Architecture Model for Early Prediction of Lithium Battery Life",
        "authors": [
            "Lidang Jiang",
            "Zhuoxiang Li",
            "Changyan Hu",
            "Qingsong Huang",
            "Ge He"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The early prediction of battery life (EPBL) is vital for enhancing theefficiency and extending the lifespan of lithium batteries. Traditional modelswith fixed architectures often encounter underfitting or overfitting issues dueto the diverse data distributions in different EPBL tasks. An interpretabledeep learning model of flexible parallel neural network (FPNN) is proposed,which includes an InceptionBlock, a 3D convolutional neural network (CNN), a 2DCNN, and a dual-stream network. The proposed model effectively extractselectrochemical features from video-like formatted data using the 3D CNN andachieves advanced multi-scale feature abstraction through the InceptionBlock.The FPNN can adaptively adjust the number of InceptionBlocks to flexibly handletasks of varying complexity in EPBL. The test on the MIT dataset shows that theFPNN model achieves outstanding predictive accuracy in EPBL tasks, with MAPEsof 2.47%, 1.29%, 1.08%, and 0.88% when the input cyclic data volumes are 10,20, 30, and 40, respectively. The interpretability of the FPNN is mainlyreflected in its flexible unit structure and parameter selection: its diversebranching structure enables the model to capture features at different scales,thus allowing the machine to learn informative features. The approach presentedherein provides an accurate, adaptable, and comprehensible solution for earlylife prediction of lithium batteries, opening new possibilities in the field ofbattery health monitoring."
    },
    {
        "link": "https://arxiv.org/abs/2401.16104",
        "title": "A 2D Sinogram-Based Approach to Defect Localization in Computed Tomography",
        "authors": [
            "Yuzhong Zhou",
            "Linda-Sophie Schneider",
            "Fuxin Fan",
            "Andreas Maier"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rise of deep learning has introduced a transformative era in the field ofimage processing, particularly in the context of computed tomography. Deeplearning has made a significant contribution to the field of industrialComputed Tomography. However, many defect detection algorithms are applieddirectly to the reconstructed domain, often disregarding the raw sensor data.This paper shifts the focus to the use of sinograms. Within this framework, wepresent a comprehensive three-step deep learning algorithm, designed toidentify and analyze defects within objects without resorting to imagereconstruction. These three steps are defect segmentation, mask isolation, anddefect analysis. We use a U-Net-based architecture for defect segmentation. Ourmethod achieves the Intersection over Union of 92.02% on our simulated data,with an average position error of 1.3 pixels for defect detection on a512-pixel-wide detector."
    },
    {
        "link": "https://arxiv.org/abs/2401.16107",
        "title": "Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis",
        "authors": [
            "Haochun Wang",
            "Sendong Zhao",
            "Zewen Qiang",
            "Nuwa Xi",
            "Bing Qin",
            "Ting Liu"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Automatic diagnosis is a significant application of AI in healthcare, wherediagnoses are generated based on the symptom description of patients. Previousworks have approached this task directly by modeling the relationship betweenthe normalized symptoms and all possible diseases. However, in the clinicaldiagnostic process, patients are initially consulted by a general practitionerand, if necessary, referred to specialists in specific domains for a morecomprehensive evaluation. The final diagnosis often emerges from acollaborative consultation among medical specialist groups. Recently, largelanguage models have shown impressive capabilities in natural languageunderstanding. In this study, we adopt tuning-free LLM-based agents as medicalpractitioners and propose the Agent-derived Multi-Specialist Consultation(AMSC) framework to model the diagnosis process in the real world by adaptivelyfusing probability distributions of agents over potential diseases.Experimental results demonstrate the superiority of our approach compared withbaselines. Notably, our approach requires significantly less parameter updatingand training time, enhancing efficiency and practical utility. Furthermore, wedelve into a novel perspective on the role of implicit symptoms within thecontext of automatic diagnosis."
    },
    {
        "link": "https://arxiv.org/abs/2401.16108",
        "title": "Future Impact Decomposition in Request-level Recommendations",
        "authors": [
            "Xiaobei Wang",
            "Shuchang Liu",
            "Xueliang Wang",
            "Qingpeng Cai",
            "Lantao Hu",
            "Han Li",
            "Peng Jiang",
            "Guangming Xie"
        ],
        "primary_subject": "Information Retrieval (cs.IR)",
        "abstract": "In recommender systems, reinforcement learning solutions have shown promisingresults in optimizing the interaction sequence between users and the systemover the long-term performance. For practical reasons, the policy's actions aretypically designed as recommending a list of items to handle users' frequentand continuous browsing requests more efficiently. In this list-wiserecommendation scenario, the user state is updated upon every request in thecorresponding MDP formulation. However, this request-level formulation isessentially inconsistent with the user's item-level behavior. In this study, wedemonstrate that an item-level optimization approach can better utilize itemcharacteristics and optimize the policy's performance even under therequest-level MDP. We support this claim by comparing the performance ofstandard request-level methods with the proposed item-level actor-criticframework in both simulation and online experiments. Furthermore, we found thatthe naive equal decomposition of future values may not effectively express theitem-wise utility in the long term. To address this issue, we propose a futuredecomposition strategy based on each item's immediate reward, and further showthat we can obtain more advanced settings of weight through adversariallearning."
    },
    {
        "link": "https://arxiv.org/abs/2401.16109",
        "title": "Minimalistic System Modelling: Behaviours, Interfaces, and Local Reasoning",
        "authors": [
            "Didier Galmiche",
            "Timo Lang",
            "David Pym"
        ],
        "primary_subject": "Logic in Computer Science (cs.LO)",
        "abstract": "The infrastructure upon which the functioning of society depends is composedof complex ecosystems of systems. Consequently, we must reason about theproperties of such ecosystems, which requires that we construct models of them.There are very many approaches to systems modelling, typically building oncomplex structural and dynamic frameworks. Our purpose here is to explore amodelling framework based on minimal assumptions, starting from a primitivenotion of behaviour, and to show that such an approach allows the recovery ofthe key ideas, including a generalized CAP theorem, required for effectivemodelling of and reasoning about ecosystems of systems. We establish a logic ofbehaviours and use it to express local reasoning principles for thecompositional structure of systems."
    },
    {
        "link": "https://arxiv.org/abs/2401.16110",
        "title": "Towards Scenario Generalization for Vision-based Roadside 3D Object Detection",
        "authors": [
            "Lei Yang",
            "Xinyu Zhang",
            "Jun Li",
            "Li Wang",
            "Chuang Zhang",
            "Li Ju",
            "Zhiwei Li",
            "Yang Shen"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Roadside perception can greatly increase the safety of autonomous vehicles byextending their perception ability beyond the visual range and addressing blindspots. However, current state-of-the-art vision-based roadside detectionmethods possess high accuracy on labeled scenes but have inferior performanceon new scenes. This is because roadside cameras remain stationary afterinstallation and can only collect data from a single scene, resulting in thealgorithm overfitting these roadside backgrounds and camera poses. To addressthis issue, in this paper, we propose an innovative Scenario GeneralizationFramework for Vision-based Roadside 3D Object Detection, dubbed SGV3D.Specifically, we employ a Background-suppressed Module (BSM) to mitigatebackground overfitting in vision-centric pipelines by attenuating backgroundfeatures during the 2D to bird's-eye-view projection. Furthermore, byintroducing the Semi-supervised Data Generation Pipeline (SSDG) using unlabeledimages from new scenes, diverse instance foregrounds with varying camera posesare generated, addressing the risk of overfitting specific camera poses. Weevaluate our method on two large-scale roadside benchmarks. Our methodsurpasses all previous methods by a significant margin in new scenes, including+42.57% for vehicle, +5.87% for pedestrian, and +14.89% for cyclist compared toBEVHeight on the DAIR-V2X-I heterologous benchmark. On the larger-scale Rope3Dheterologous benchmark, we achieve notable gains of 14.48% for car and 12.41%for large vehicle. We aspire to contribute insights on the exploration ofroadside perception techniques, emphasizing their capability for scenariogeneralization. The code will be available at {\\url{https://github.com/yanglei18/SGV3D}}"
    },
    {
        "link": "https://arxiv.org/abs/2401.16113",
        "title": "A parallel preconditioner for the all-at-once linear system from evolutionary PDEs with Crank-Nicolson discretization",
        "authors": [
            "Yong-Liang Zhao",
            "Xian-Ming Gu",
            "Cornelis W. Oosterlee"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "The Crank-Nicolson (CN) method is a well-known time integrator forevolutionary partial differential equations (PDEs) arising in many real-worldapplications. Since the solution at any time depends on the solution atprevious time steps, the CN method will be inherently difficult to parallelize.In this paper, we consider a parallel method for the solution of evolutionaryPDEs with the CN scheme. Using an all-at-once approach, we can solve for alltime steps simultaneously using a parallelizable over time preconditionerwithin a standard iterative method. Due to the diagonalization of the proposedpreconditioner, we can prove that most eigenvalues of preconditioned matricesare equal to 1 and the others lie in the set: {z\u2208C:1/(1+\u03b1)<|z|<1/(1\u2212\u03b1)\u00a0and\u00a0Re(z)>0}, where 0<\u03b1<1 is a free parameter. Meanwhile, the efficient implementation ofthis proposed preconditioner is described and a mesh-independent convergencerate of the preconditioned GMRES method is derived under certain conditions.Finally, we will verify our theoretical findings via numerical experiments onfinancial option pricing partial differential equations."
    },
    {
        "link": "https://arxiv.org/abs/2401.16119",
        "title": "Triple Disentangled Representation Learning for Multimodal Affective Analysis",
        "authors": [
            "Ying Zhou",
            "Xuefeng Liang",
            "Han Chen",
            "Yin Zhao"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal learning has exhibited a significant advantage in affectiveanalysis tasks owing to the comprehensive information of various modalities,particularly the complementary information. Thus, many emerging studies focuson disentangling the modality-invariant and modality-specific representationsfrom input data and then fusing them for prediction. However, our study showsthat modality-specific representations may contain information that isirrelevant or conflicting with the tasks, which downgrades the effectiveness oflearned multimodal representations. We revisit the disentanglement issue, andpropose a novel triple disentanglement approach, TriDiRA, which disentanglesthe modality-invariant, effective modality-specific and ineffectivemodality-specific representations from input data. By fusing only themodality-invariant and effective modality-specific representations, TriDiRA cansignificantly alleviate the impact of irrelevant and conflicting informationacross modalities during model training. Extensive experiments conducted onfour benchmark datasets demonstrate the effectiveness and generalization of ourtriple disentanglement, which outperforms SOTA methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.16122",
        "title": "DeFlow: Decoder of Scene Flow Network in Autonomous Driving",
        "authors": [
            "Qingwen Zhang",
            "Yi Yang",
            "Heng Fang",
            "Ruoyu Geng",
            "Patric Jensfelt"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Scene flow estimation determines a scene's 3D motion field, by predicting themotion of points in the scene, especially for aiding tasks in autonomousdriving. Many networks with large-scale point clouds as input use voxelizationto create a pseudo-image for real-time running. However, the voxelizationprocess often results in the loss of point-specific features. This gives riseto a challenge in recovering those features for scene flow tasks. Our paperintroduces DeFlow which enables a transition from voxel-based features to pointfeatures using Gated Recurrent Unit (GRU) refinement. To further enhance sceneflow estimation performance, we formulate a novel loss function that accountsfor the data imbalance between static and dynamic points. Evaluations on theArgoverse 2 scene flow task reveal that DeFlow achieves state-of-the-artresults on large-scale point cloud data, demonstrating that our network hasbetter performance and efficiency compared to others. The code is open-sourcedat https://github.com/KTH-RPL/deflow."
    },
    {
        "link": "https://arxiv.org/abs/2401.16123",
        "title": "Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers",
        "authors": [
            "Amr Gomaa",
            "Guillermo Reyes",
            "Michael Feld",
            "Antonio Kr\u00fcger"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "The rapid advancement of the automotive industry towards automated andsemi-automated vehicles has rendered traditional methods of vehicleinteraction, such as touch-based and voice command systems, inadequate for awidening range of non-driving related tasks, such as referencing objectsoutside of the vehicle. Consequently, research has shifted toward gesturalinput (e.g., hand, gaze, and head pose gestures) as a more suitable mode ofinteraction during driving. However, due to the dynamic nature of driving andindividual variation, there are significant differences in drivers' gesturalinput performance. While, in theory, this inherent variability could bemoderated by substantial data-driven machine learning models, prevalentmethodologies lean towards constrained, single-instance trained models forobject referencing. These models show a limited capacity to continuously adaptto the divergent behaviors of individual drivers and the variety of drivingscenarios. To address this, we propose \\textit{IcRegress}, a novelregression-based incremental learning approach that adapts to changing behaviorand the unique characteristics of drivers engaged in the dual task of drivingand referencing objects. We suggest a more personalized and adaptable solutionfor multimodal gestural interfaces, employing continuous lifelong learning toenhance driver experience, safety, and convenience. Our approach was evaluatedusing an outside-the-vehicle object referencing use case, highlighting thesuperiority of the incremental learning models adapted over a single trainedmodel across various driver traits such as handedness, driving experience, andnumerous driving conditions. Finally, to facilitate reproducibility, easedeployment, and promote further research, we offer our approach as anopen-source framework at \\url{https://github.com/amrgomaaelhady/IcRegress}."
    },
    {
        "link": "https://arxiv.org/abs/2401.16124",
        "title": "On the generalization of learned constraints for ASP solving in temporal domains",
        "authors": [
            "Javier Romero",
            "Torsten Schaub",
            "Klaus Strauch"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "The representation of a dynamic problem in ASP usually boils down to usingcopies of variables and constraints, one for each time stamp, no matter whetherit is directly encoded or via an action or temporal language. Themultiplication of variables and constraints is commonly done during groundingand the solver is completely ignorant about the temporal relationship among thedifferent instances. On the other hand, a key factor in the performance oftoday's ASP solvers is conflict-driven constraint learning. Our question is nowwhether a constraint learned for particular time steps can be generalized andreused at other time stamps, and ultimately whether this enhances the overallsolver performance on temporal problems. Knowing full well the domain of time,we study conditions under which learned dynamic constraints can be generalized.We propose a simple translation of the original logic program such that, forthe translated programs, the learned constraints can be generalized to othertime points. Additionally, we identify a property of temporal problems thatallows us to generalize all learned constraints to all time steps. It turns outthat this property is satisfied by many planning problems. Finally, weempirically evaluate the impact of adding the generalized constraints to an ASPsolver"
    },
    {
        "link": "https://arxiv.org/abs/2401.16130",
        "title": "A polytopal discrete de Rham complex on manifolds, with application to the Maxwell equations",
        "authors": [
            "J\u00e9r\u00f4me Droniou",
            "Marien Hanot",
            "Todd Oliynyk"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We design in this work a discrete de Rham complex on manifolds. This complex,written in the framework of exterior calculus, is applicable on meshes on themanifold with generic elements, and has the same cohomology as the continuousde Rham complex. Notions of local (full and trimmed) polynomial spaces aredeveloped, with compatibility requirements between polynomials on mesh entitiesof various dimensions. Explicit examples of polynomials spaces are presented.The discrete de Rham complex is then used to set up a scheme for the Maxwellequations on a 2D manifold without boundary, and we show that a naturaldiscrete version of the constraint linking the electric field and the electriccharge density is satisfied. Numerical examples are provided on the sphere andthe torus, based on a bespoke analytical solution and mesh design on eachmanifold."
    },
    {
        "link": "https://arxiv.org/abs/2401.16131",
        "title": "CIMIL-CRC: a clinically-informed multiple instance learning framework for patient-level colorectal cancer molecular subtypes classification from H\\&E stained images",
        "authors": [
            "Hadar Hezi",
            "Matan Gelber",
            "Alexander Balabanov",
            "Yosef E. Maruvka",
            "Moti Freiman"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Treatment approaches for colorectal cancer (CRC) are highly dependent on themolecular subtype, as immunotherapy has shown efficacy in cases withmicrosatellite instability (MSI) but is ineffective for the microsatellitestable (MSS) subtype. There is promising potential in utilizing deep neuralnetworks (DNNs) to automate the differentiation of CRC subtypes by analyzingHematoxylin and Eosin (H\\&E) stained whole-slide images (WSIs). Due to theextensive size of WSIs, Multiple Instance Learning (MIL) techniques aretypically explored. However, existing MIL methods focus on identifying the mostrepresentative image patches for classification, which may result in the lossof critical information. Additionally, these methods often overlook clinicallyrelevant information, like the tendency for MSI class tumors to predominantlyoccur on the proximal (right side) colon. We introduce `CIMIL-CRC', a DNNframework that: 1) solves the MSI/MSS MIL problem by efficiently combining apre-trained feature extraction model with principal component analysis (PCA) toaggregate information from all patches, and 2) integrates clinical priors,particularly the tumor location within the colon, into the model to enhancepatient-level classification accuracy. We assessed our CIMIL-CRC method usingthe average area under the curve (AUC) from a 5-fold cross-validationexperimental setup for model development on the TCGA-CRC-DX cohort, contrastingit with a baseline patch-level classification, MIL-only approach, andClinically-informed patch-level classification approach. Our CIMIL-CRCoutperformed all methods (AUROC: 0.92\u00b10.002 (95\\% CI 0.91-0.92), vs.0.79\u00b10.02 (95\\% CI 0.76-0.82), 0.86\u00b10.01 (95\\% CI 0.85-0.88), and0.87\u00b10.01 (95\\% CI 0.86-0.88), respectively). The improvement wasstatistically significant."
    },
    {
        "link": "https://arxiv.org/abs/2401.16132",
        "title": "Challenges in computing matrix functions",
        "authors": [
            "Massimiliano Fasi",
            "St\u00e9phane Gaudreault",
            "Kathryn Lund",
            "Marcel Schweitzer"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This manuscript summarizes the outcome of the focus groups at \"The f(A)bulousworkshop on matrix functions and exponential integrators\", held at the MaxPlanck Institute for Dynamics of Complex Technical Systems in Magdeburg,Germany, on 25-27 September 2023. There were three focus groups in total, eachwith a different theme: knowledge transfer, high-performance and energy-awarecomputing, and benchmarking. We collect insights, open issues, and perspectivesfrom each focus group, as well as from general discussions throughout theworkshop. Our primary aim is to highlight ripe research directions and continueto build on the momentum from a lively meeting."
    },
    {
        "link": "https://arxiv.org/abs/2401.16133",
        "title": "BooleanOCT: Optimal Classification Trees based on multivariate Boolean Rules",
        "authors": [
            "Jiancheng Tu",
            "Wenqi Fan",
            "Zhibin Wu"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The global optimization of classification trees has demonstrated considerablepromise, notably in enhancing accuracy, optimizing size, and thereby improvinghuman comprehensibility. While existing optimal classification treessubstantially enhance accuracy over greedy-based tree models like CART, theystill fall short when compared to the more complex black-box models, such asrandom forests. To bridge this gap, we introduce a new mixed-integerprogramming (MIP) formulation, grounded in multivariate Boolean rules, toderive the optimal classification tree. Our methodology integrates both linearmetrics, including accuracy, balanced accuracy, and cost-sensitive cost, aswell as nonlinear metrics such as the F1-score. The approach is implemented inan open-source Python package named BooleanOCT. We comprehensively benchmarkthese methods on the 36 datasets from the UCI machine learning repository. Theproposed models demonstrate practical solvability on real-world datasets,effectively handling sizes in the tens of thousands. Aiming to maximizeaccuracy, this model achieves an average absolute improvement of 3.1\\% and1.5\\% over random forests in small-scale and medium-sized datasets,respectively. Experiments targeting various objectives, including balancedaccuracy, cost-sensitive cost, and F1-score, demonstrate the framework's wideapplicability and its superiority over contemporary state-of-the-art optimalclassification tree methods in small to medium-scale datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.16136",
        "title": "Neural Network Training on Encrypted Data with TFHE",
        "authors": [
            "Luis Montero",
            "Jordan Frery",
            "Celia Kherfallah",
            "Roman Bredehoft",
            "Andrei Stoian"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "We present an approach to outsourcing of training neural networks whilepreserving data confidentiality from malicious parties. We use fullyhomomorphic encryption to build a unified training approach that works onencrypted data and learns quantized neural network models. The data can behorizontally or vertically split between multiple parties, enablingcollaboration on confidential data. We train logistic regression andmulti-layer perceptrons on several datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.16137",
        "title": "X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme Multi-Profile Scenarios",
        "authors": [
            "Namju Kwak",
            "Taesup Kim"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning,aim to fine-tune a pre-trained language model (PLM) using a minimal number ofparameters for a specific task or profile. Although adapter tuning providesincreased parameter efficiency compared to full-model fine-tuning, itintroduces a small set of additional parameters attached to a PLM for eachprofile. This can become problematic in practical applications with multipleprofiles, particularly when a significant increase in the number of profileslinearly boosts the total number of additional parameters. To mitigate thisissue, we introduce X-PEFT, a novel PEFT method that leverages a multitude ofgiven adapters by fine-tuning an extremely small set of compact tensors for anew profile, which serve as binary masks to adaptively select the givenadapters. To efficiently validate our proposed method, we implement it using alarge number of trained or untrained (random) adapters. We evaluate theperformance of X-PEFT through LaMP and GLUE tasks and demonstrate that iteither matches or surpasses the effectiveness of conventional adapter tuning,despite reducing the memory requirements per profile by a factor of 10,000compared to it."
    },
    {
        "link": "https://arxiv.org/abs/2401.16139",
        "title": "Improving device-aware Web services and their mobile clients through an aspect-oriented, model-driven approach",
        "authors": [
            "Guadalupe Ortiz",
            "Alfonso Garcia-de-Prado"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Context: Mobile devices have become an essential element in our daily lives,even for connecting to the Internet. Consequently, Web services have becomeextremely important when offering services through the Internet. However,current Web services are very inflexible as regards their invocation fromdifferent types of device, especially if we consider the need for them to beadaptable when being invoked from mobile devices. Objective: In this paper, weprovide an approach for the creation of flexible Web services which can beinvoked transparently from different device types and which return subsequentresponses, as well as providing the client's adaptation as a result of theparticular device characteristics and end-user preferences in a completelydecoupled way. Method: Aspect-Oriented Programming and model-driven developmenthave been used to reduce both the impact of service and client code adaptationfor multiple devices as well as to facilitate the developer's task. Results: Amodel-driven methodology can be followed from system models to code, providingthe Web service developer with the option of marking which services should beadapted to mobile devices in the UML models, and obtaining the decoupledadaptation code automatically from the models. Conclusion: We can conclude thatthe approach presented in this paper provides us with the possibility offollowing the development of mobile-aware Web services in an integratedplatform, benefiting from the use of aspect-oriented techniques not only formaintaining device-related code completely decoupled from the mainfunctionality one, but also allowing a modularized non-intrusive adaptation ofmobile clients to the specific device characteristics as well as to final userpreferences."
    },
    {
        "link": "https://arxiv.org/abs/2401.16144",
        "title": "Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields",
        "authors": [
            "Rongkai Ma",
            "Leo Lebrat",
            "Rodrigo Santa Cruz",
            "Gil Avraham",
            "Yan Zuo",
            "Clinton Fookes",
            "Olivier Salvado"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Neural radiance fields (NeRFs) have exhibited potential in synthesizinghigh-fidelity views of 3D scenes but the standard training paradigm of NeRFpresupposes an equal importance for each image in the training set. Thisassumption poses a significant challenge for rendering specific viewspresenting intricate geometries, thereby resulting in suboptimal performance.In this paper, we take a closer look at the implications of the currenttraining paradigm and redesign this for more superior rendering quality byNeRFs. Dividing input views into multiple groups based on their visualsimilarities and training individual models on each of these groups enableseach model to specialize on specific regions without sacrificing speed orefficiency. Subsequently, the knowledge of these specialized models isaggregated into a single entity via a teacher-student distillation paradigm,enabling spatial efficiency for online render-ing. Empirically, we evaluate ournovel training framework on two publicly available datasets, namely NeRFsynthetic and Tanks&Temples. Our evaluation demonstrates that our DaC trainingpipeline enhances the rendering quality of a state-of-the-art baseline modelwhile exhibiting convergence to a superior minimum."
    },
    {
        "link": "https://arxiv.org/abs/2401.16152",
        "title": "Agile Effort Estimation: Comparing the Accuracy and Efficiency of Planning Poker, Bucket System, and Affinity Estimation methods",
        "authors": [
            "Marko Po\u017eenel",
            "Luka F\u00fcrst",
            "Damjan Vavpoti\u010d",
            "Toma\u017e Hovelja"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Published studies on agile effort estimation predominantly focus oncomparisons of the accuracy of different estimation methods, while efficiencycomparisons, i.e. how much time the estimation methods consume was not in theforefront. However, for practical use in software development, the timerequired can be a very important cost factor for enterprises, especially whenthe accuracy of different agile effort estimations is similar. In this study,we thus try to advance the current standard accuracy comparison between methodsby introducing efficiency i.e. time it takes to use a method as an additionaldimension of comparison. We conduct this comparison between three agile effortestimation methods that were not yet compared in the literature, namelyPlanning Poker, Bucket System and Affinity Estimation. For the comparison, weused eight student teams with 29 students that had to use all the effortestimation methods during the course where they had to finish a programmingproject in 3 weeks. The results indicate that after the students get used tousing the different methods the accuracy between them is not statisticallysignificantly different, however, the efficiency is. On average Bucket Systemand Affinity Estimation methods take half as much time as Planning Poker."
    },
    {
        "link": "https://arxiv.org/abs/2401.16156",
        "title": "A fitted scheme for a Caputo initial-boundary value problem",
        "authors": [
            "J.L. Gracia",
            "E. O'Riordan",
            "M. Stynes"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper we consider an initial-boundary value problem with a Caputotime derivative of order \u03b1\u2208(0,1). The solution typically exhibits aweak singularity near the initial time and this causes a reduction in theorders of convergence of standard schemes. To deal with this singularity, thesolution is computed with a fitted difference scheme on a graded mesh. Theconvergence of this scheme is analysed using a discrete maximum principle andcarefully chosen barrier functions. Sharp error estimates are proved, whichshow an enhancement in the convergence rate compared with the standard L1approximation on uniform meshes, and also indicate an optimal choice for themesh grading. This optimal mesh grading is less severe than the optimal gradingfor the standard L1 scheme. Furthermore, the dependence of the error on thefinal time forms part of our error estimate. Numerical experiments arepresented which corroborate our theoretical results."
    },
    {
        "link": "https://arxiv.org/abs/2401.16157",
        "title": "Spatial-Aware Latent Initialization for Controllable Image Generation",
        "authors": [
            "Wenqiang Sun",
            "Teng Li",
            "Zehong Lin",
            "Jun Zhang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, text-to-image diffusion models have demonstrated impressive abilityto generate high-quality images conditioned on the textual input. However,these models struggle to accurately adhere to textual instructions regardingspatial layout information. While previous research has primarily focused onaligning cross-attention maps with layout conditions, they overlook the impactof the initialization noise on the layout guidance. To achieve better layoutcontrol, we propose leveraging a spatial-aware initialization noise during thedenoising process. Specifically, we find that the inverted reference image withfinite inversion steps contains valuable spatial awareness regarding theobject's position, resulting in similar layouts in the generated images. Basedon this observation, we develop an open-vocabulary framework to customize aspatial-aware initialization noise for each layout condition. Without modifyingother modules except the initialization noise, our approach can be seamlesslyintegrated as a plug-and-play module within other training-free layout guidanceframeworks. We evaluate our approach quantitatively and qualitatively on theavailable Stable Diffusion model and COCO dataset. Equipped with thespatial-aware latent initialization, our method significantly improves theeffectiveness of layout guidance while preserving high-quality content."
    },
    {
        "link": "https://arxiv.org/abs/2401.16158",
        "title": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Jiabo Ye",
            "Ming Yan",
            "Weizhou Shen",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Mobile device agent based on Multimodal Large Language Models (MLLM) isbecoming a popular application. In this paper, we introduce Mobile-Agent, anautonomous multi-modal mobile device agent. Mobile-Agent first leverages visualperception tools to accurately identify and locate both the visual and textualelements within the app's front-end interface. Based on the perceived visioncontext, it then autonomously plans and decomposes the complex operation task,and navigates the mobile Apps through operations step by step. Different fromprevious solutions that rely on XML files of Apps or mobile system metadata,Mobile-Agent allows for greater adaptability across diverse mobile operatingenvironments in a vision-centric way, thereby eliminating the necessity forsystem-specific customizations. To assess the performance of Mobile-Agent, weintroduced Mobile-Eval, a benchmark for evaluating mobile device operations.Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent.The experimental results indicate that Mobile-Agent achieved remarkableaccuracy and completion rates. Even with challenging instructions, such asmulti-app operations, Mobile-Agent can still complete the requirements. Codeand model will be open-sourced at https://github.com/X-PLUG/MobileAgent."
    },
    {
        "link": "https://arxiv.org/abs/2401.16160",
        "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs",
        "authors": [
            "Shaoxiang Chen",
            "Zequn Jie",
            "Lin Ma"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Instruction finetuning on a variety of image-text instruction data is the keyto obtaining a versatile Multimodal Large Language Model (MLLM), and differentconfigurations of the instruction data can lead to finetuned models withdifferent capabilities. However, we have discovered that data conflicts areinevitable when mixing instruction data from distinct domains, which can resultin performance drops for tasks of a specific domain. To address this issue, wepropose to apply a sparse mixture of LoRA experts for instruction finetuningMLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption(LoRA) method by creating a set of LoRA experts specifically for the MLP layer,and route each token to the top-1 expert based on a routing function, allowingadaptive choices for tokens from different domains. Since the LoRA experts aresparsely activated, the training and inference cost are kept roughly constantcompared to the original LoRA method. By replacing the plain-LoRA finetuing ofLLaVA-1.5, our final model is named LLaVA-MoLE. Extensive experiments provedthat LLaVA-MoLE effectively mitigates the data conflict issue when mixingmultiple distinct instruction datasets with various configurations, andachieves consistent performance gains over the strong plain-LoRA baselines.Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform theplain-LoRA baseline trained with twice the samples."
    },
    {
        "link": "https://arxiv.org/abs/2401.16164",
        "title": "Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm",
        "authors": [
            "Wei Yao",
            "Chengming Yu",
            "Shangzhi Zeng",
            "Jin Zhang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a new approach and algorithm for solving a class ofconstrained Bi-Level Optimization (BLO) problems in which the lower-levelproblem involves constraints coupling both upper-level and lower-levelvariables. Such problems have recently gained significant attention due totheir broad applicability in machine learning. However, conventionalgradient-based methods unavoidably rely on computationally intensivecalculations related to the Hessian matrix. To address this challenge, we beginby devising a smooth proximal Lagrangian value function to handle theconstrained lower-level problem. Utilizing this construct, we introduce asingle-level reformulation for constrained BLOs that transforms the originalBLO problem into an equivalent optimization problem with smooth constraints.Enabled by this reformulation, we develop a Hessian-free gradient-basedalgorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-levelAlgorithm (LV-HBA)-that is straightforward to implement in a single loopmanner. Consequently, LV-HBA is especially well-suited for machine learningapplications. Furthermore, we offer non-asymptotic convergence analysis forLV-HBA, eliminating the need for traditional strong convexity assumptions forthe lower-level problem while also being capable of accommodating non-singletonscenarios. Empirical results substantiate the algorithm's superior practicalperformance."
    },
    {
        "link": "https://arxiv.org/abs/2401.16167",
        "title": "\"You tell me\": A Dataset of GPT-4-Based Behaviour Change Support Conversations",
        "authors": [
            "Selina Meyer",
            "David Elsweiler"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Conversational agents are increasingly used to address emotional needs on topof information needs. One use case of increasing interest are counselling-stylemental health and behaviour change interventions, with large language model(LLM)-based approaches becoming more popular. Research in this context so farhas been largely system-focused, foregoing the aspect of user behaviour and theimpact this can have on LLM-generated texts. To address this issue, we share adataset containing text-based user interactions related to behaviour changewith two GPT-4-based conversational agents collected in a preregistered userstudy. This dataset includes conversation data, user language analysis,perception measures, and user feedback for LLM-generated turns, and can offervaluable insights to inform the design of such systems based on realinteractions."
    },
    {
        "link": "https://arxiv.org/abs/2401.16170",
        "title": "A Privacy-preserving key transmission protocol to distribute QRNG keys using zk-SNARKs",
        "authors": [
            "David Soler",
            "Carlos Dafonte",
            "Manuel Fern\u00e1ndez-Veiga",
            "Ana Fern\u00e1ndez Vilas",
            "Francisco J. N\u00f3voa"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "High-entropy random numbers are an essential part of cryptography, andQuantum Random Number Generators (QRNG) are an emergent technology that canprovide high-quality keys for cryptographic algorithms but unfortunately arecurrently difficult to access. Existing Entropy-as-a-Service solutions requireusers to trust the central authority distributing the key material, which isnot desirable in a high-privacy environment. In this paper, we present a novelkey transmission protocol that allows users to obtain cryptographic materialgenerated by a QRNG in such a way that the server is unable to identify whichuser is receiving each key. This is achieved with the inclusion of ZeroKnowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARK), acryptographic primitive that allow users to prove knowledge of some valuewithout needing to reveal it. The security analysis of the protocol proves thatit satisfies the properties of Anonymity, Unforgeability and Confidentiality,as defined in this document. We also provide an implementation of the protocoldemonstrating its functionality and performance, using NFC as the transmissionchannel for the QRNG key."
    },
    {
        "link": "https://arxiv.org/abs/2401.16173",
        "title": "Reconstructing Close Human Interactions from Multiple Views",
        "authors": [
            "Qing Shuai",
            "Zhiyuan Yu",
            "Zhize Zhou",
            "Lixin Fan",
            "Haijun Yang",
            "Can Yang",
            "Xiaowei Zhou"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper addresses the challenging task of reconstructing the poses ofmultiple individuals engaged in close interactions, captured by multiplecalibrated cameras. The difficulty arises from the noisy or false 2D keypointdetections due to inter-person occlusion, the heavy ambiguity in associatingkeypoints to individuals due to the close interactions, and the scarcity oftraining data as collecting and annotating motion data in crowded scenes isresource-intensive. We introduce a novel system to address these challenges.Our system integrates a learning-based pose estimation component and itscorresponding training and inference strategies. The pose estimation componenttakes multi-view 2D keypoint heatmaps as input and reconstructs the pose ofeach individual using a 3D conditional volumetric network. As the networkdoesn't need images as input, we can leverage known camera parameters from testscenes and a large quantity of existing motion capture data to synthesizemassive training data that mimics the real data distribution in test scenes.Extensive experiments demonstrate that our approach significantly surpassesprevious approaches in terms of pose accuracy and is generalizable acrossvarious camera setups and population sizes. The code is available on ourproject page: https://github.com/zju3dv/CloseMoCap."
    },
    {
        "link": "https://arxiv.org/abs/2401.16176",
        "title": "A Survey on Structure-Preserving Graph Transformers",
        "authors": [
            "Van Thuy Hoang",
            "O-Joun Lee"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The transformer architecture has shown remarkable success in various domains,such as natural language processing and computer vision. When it comes to graphlearning, transformers are required not only to capture the interactionsbetween pairs of nodes but also to preserve graph structures connoting theunderlying relations and proximity between them, showing the expressive powerto capture different graph structures. Accordingly, variousstructure-preserving graph transformers have been proposed and widely used forvarious tasks, such as graph-level tasks in bioinformatics andchemoinformatics. However, strategies related to graph structure preservationhave not been well organized and systematized in the literature. In this paper,we provide a comprehensive overview of structure-preserving graph transformersand generalize these methods from the perspective of their design objective.First, we divide strategies into four main groups: node feature modulation,context node sampling, graph rewriting, and transformer architectureimprovements. We then further divide the strategies according to the coverageand goals of graph structure preservation. Furthermore, we also discusschallenges and future directions for graph transformer models to preserve thegraph structure and understand the nature of graphs."
    },
    {
        "link": "https://arxiv.org/abs/2401.16178",
        "title": "B\u00e9zier curves and the Takagi function",
        "authors": [
            "Lenka Ptackova",
            "Franco Vivaldi"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We consider B\\'ezier curves with complex parameter, and we determineexplicitly the affine iterated function system (IFS) corresponding to the deCasteljau subdivision algorithm, together with the complex parametric domainover which such an IFS has a unique global connected attractor. For a specificfamily of complex parameter having vanishing imaginary part, we prove that theTakagi fractal curve is the attractor, under suitable scaling."
    },
    {
        "link": "https://arxiv.org/abs/2401.16181",
        "title": "On Decentralized Linearly Separable Computation With the Minimum Computation Cost",
        "authors": [
            "Haoning Chen",
            "Minquan Cheng",
            "Zhenhao Huang",
            "Youlong Wu"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "The distributed linearly separable computation problem finds extensiveapplications across domains such as distributed gradient coding, distributedlinear transform, real-time rendering, etc. In this paper, we investigate thisproblem in a fully decentralized scenario, where N workerscollaboratively perform the computation task without a central master. Eachworker aims to compute a linearly separable computation that can be manifestedas Kc linear combinations of K messages,where each message is a function of a distinct dataset. We require that eachworker successfully fulfill the task based on the transmissions from anyNr workers, such that the system can tolerate anyN\u2212Nr stragglers. We focus on the scenario wherethe computation cost (the number of uncoded datasets assigned to each worker)is minimum, and aim to minimize the communication cost (the number of symbolsthe fastest Nr workers transmit). We propose a noveldistributed computing scheme that is optimal under the widely used cyclic dataassignment. Interestingly, we demonstrate that the side information at eachworker is ineffective in reducing the communication cost whenKc\u2264KNr/N,while it helps reduce the communication cost as Kcincreases."
    },
    {
        "link": "https://arxiv.org/abs/2401.16182",
        "title": "LLaMandement: Large Language Models for Summarization of French Legislative Proposals",
        "authors": [
            "Joseph Gesnouin",
            "Yannis Tannier",
            "Christophe Gomes Da Silva",
            "Hatim Tapory",
            "Camille Brier",
            "Hugo Simon",
            "Raphael Rozenberg",
            "Hermann Woehrel",
            "Mehdi El Yakaabi",
            "Thomas Binder",
            "Guillaume Marie",
            "Emilie Caron",
            "Mathile Nogueira",
            "Thomas Fontas",
            "Laure Puydebois",
            "Marie Theophile",
            "Stephane Morandi",
            "Mael Petit",
            "David Creissac",
            "Pauline Ennouchy",
            "Elise Valetoux",
            "Celine Visade",
            "Severine Balloux",
            "Emmanuel Cortes",
            "Pierre-Etienne Devineau",
            "Ulrich Tan",
            "Esther Mac Namara",
            "Su Yang"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "This report introduces LLaMandement, a state-of-the-art Large Language Model,fine-tuned by the French government and designed to enhance the efficiency andefficacy of processing parliamentary sessions (including the production ofbench memoranda and documents required for interministerial meetings) bygenerating neutral summaries of legislative proposals. Addressing theadministrative challenges of manually processing a growing volume oflegislative amendments, LLaMandement stands as a significant legaltechnological milestone, providing a solution that exceeds the scalability oftraditional human efforts while matching the robustness of a specialized legaldrafter. We release all our fine-tuned models and training data to thecommunity."
    },
    {
        "link": "https://arxiv.org/abs/2401.16183",
        "title": "Scalable Reinforcement Learning for Linear-Quadratic Control of Networks",
        "authors": [
            "Johan Olsson",
            "Runyu Zhang",
            "Emma Tegling",
            "Na Li"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Distributed optimal control is known to be challenging and can becomeintractable even for linear-quadratic regulator problems. In this work, westudy a special class of such problems where distributed state feedbackcontrollers can give near-optimal performance. More specifically, we considernetworked linear-quadratic controllers with decoupled costs and spatiallyexponentially decaying dynamics. We aim to exploit the structure in the problemto design a scalable reinforcement learning algorithm for learning adistributed controller. Recent work has shown that the optimal controller canbe well approximated only using information from a \u03ba-neighborhood ofeach agent. Motivated by these results, we show that similar results hold forthe agents' individual value and Q-functions. We continue by designing analgorithm, based on the actor-critic framework, to learn distributedcontrollers only using local information. Specifically, the Q-function isestimated by modifying the Least Squares Temporal Difference for Q-functionsmethod to only use local information. The algorithm then updates the policyusing gradient descent. Finally, we evaluate the algorithm through simulationsthat indeed suggest near-optimal performance."
    },
    {
        "link": "https://arxiv.org/abs/2401.16184",
        "title": "On the Semantics of LM Latent Space: A Vocabulary-defined Approach",
        "authors": [
            "Jian Gu",
            "Chunyang Chen",
            "Aldeida Aleti"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "In the realm of deep learning, understanding the latent space of languagemodels (LMs) like transformers is crucial for refining their performance andinterpretability. However, existing analyses often fall short in providingabsolute and model-centric insights into LM semantics, and neglect essentialaspects of LM adaption. In response, we introduce a pioneering method calledvocabulary-defined semantics, which establishes a fixed reference frame withinthe LM latent space, ensuring absolute semantic analysis grounded in LMvocabulary. Our approach transcends prior relative analyses, leveraging LMvocabulary for model-centric insights. Furthermore, we propose a noveltechnique to compute logits, emphasizing differentiability and local isotropy,and introduce a neural clustering module for semantically calibrating datarepresentations during LM adaptation. Through extensive experiments acrossdiverse text understanding datasets, our approach surpasses state-of-the-artmethods of retrieval-augmented generation and parameters-efficient finetuning,showcasing its efficacy and broad applicability. Our findings not only shedlight on LM mechanics but also offer practical solutions for enhancing LMperformance and interpretability."
    },
    {
        "link": "https://arxiv.org/abs/2401.16185",
        "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "authors": [
            "Yuqiang Sun",
            "Daoyuan Wu",
            "Yue Xue",
            "Han Liu",
            "Wei Ma",
            "Lyuye Zhang",
            "Miaolei Shi",
            "Yang Liu"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Large language models (LLMs) have demonstrated significant poten- tial formany downstream tasks, including those requiring human- level intelligence,such as vulnerability detection. However, recent attempts to use LLMs forvulnerability detection are still prelim- inary, as they lack an in-depthunderstanding of a subject LLM's vulnerability reasoning capability - whetherit originates from the model itself or from external assistance, such asinvoking tool sup- port and retrieving vulnerability knowledge. In this paper,we aim to decouple LLMs' vulnerability reason- ing capability from their othercapabilities, including the ability to actively seek additional information(e.g., via function calling in SOTA models), adopt relevant vulnerabilityknowledge (e.g., via vector-based matching and retrieval), and followinstructions to out- put structured results. To this end, we propose a unifiedevaluation framework named LLM4Vuln, which separates LLMs' vulnerabilityreasoning from their other capabilities and evaluates how LLMs' vulnerabilityreasoning could be enhanced when combined with the enhancement of othercapabilities. To demonstrate the effectiveness of LLM4Vuln, we have designedcontrolled experiments using 75 ground-truth smart contract vulnerabilities,which were extensively audited as high-risk on Code4rena from August toNovember 2023, and tested them in 4,950 different scenarios across threerepresen- tative LLMs (GPT-4, Mixtral, and Code Llama). Our results not onlyreveal ten findings regarding the varying effects of knowledge en- hancement,context supplementation, prompt schemes, and models but also enable us toidentify 9 zero-day vulnerabilities in two pilot bug bounty programs with over1,000 USD being awarded."
    },
    {
        "link": "https://arxiv.org/abs/2401.16186",
        "title": "An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project",
        "authors": [
            "Sanka Rasnayaka",
            "Guanlin Wang",
            "Ridwan Shariffdeen",
            "Ganesh Neelakanta Iyer"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Large Language Models (LLMs) represent a leap in artificial intelligence,excelling in tasks using human language(s). Although the main focus ofgeneral-purpose LLMs is not code generation, they have shown promising resultsin the domain. However, the usefulness of LLMs in an academic softwareengineering project has not been fully explored yet. In this study, we explorethe usefulness of LLMs for 214 students working in teams consisting of up tosix members. Notably, in the academic course through which this study isconducted, students were encouraged to integrate LLMs into their developmenttool-chain, in contrast to most other academic courses that explicitly prohibitthe use of LLMs.In this paper, we analyze the AI-generated code, prompts used for codegeneration, and the human intervention levels to integrate the code into thecode base. We also conduct a perception study to gain insights into theperceived usefulness, influencing factors, and future outlook of LLM from acomputer science student's perspective. Our findings suggest that LLMs can playa crucial role in the early stages of software development, especially ingenerating foundational code structures, and helping with syntax and errordebugging. These insights provide us with a framework on how to effectivelyutilize LLMs as a tool to enhance the productivity of software engineeringstudents, and highlight the necessity of shifting the educational focus towardpreparing students for successful human-AI collaboration."
    },
    {
        "link": "https://arxiv.org/abs/2401.16187",
        "title": "Graph Neural Network-based Joint Equalization and Decoding",
        "authors": [
            "Jannis Clausius",
            "Marvin Geiselhart",
            "Daniel Tandler",
            "Stephan ten Brink"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This paper proposes to use graph neural networks (GNNs) for equalization,that can also be used to perform joint equalization and decoding (JED). Forequalization, the GNN is build upon the factor graph representations of thechannel, while for JED, the factor graph is expanded by the Tanner graph of theparity-check matrix (PCM) of the channel code, sharing the variable nodes(VNs). A particularly advantageous property of the GNN is the robustnessagainst cycles in the factor graphs which is the main problem for beliefpropagation (BP)-based equalization. As a result of having a fully deeplearning-based receiver, joint optimization instead of individual optimizationof the components is enabled, so-called end-to-end learning. Furthermore, wepropose a parallel flooding schedule that further reduces the latency, whichturns out to improve also the error correcting performance. The proposedapproach is analyzed and compared to state-of-the-art baselines in terms oferror correcting capability and latency. At a fixed low latency, the floodingGNN for JED demonstrates a gain of 2.25 dB in bit error rate (BER) compared toan iterative Bahl--Cock--Jelinek--Raviv (BCJR)-BP baseline."
    },
    {
        "link": "https://arxiv.org/abs/2401.16189",
        "title": "FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction",
        "authors": [
            "Sungmin Woo",
            "Minjung Kim",
            "Donghyeong Kim",
            "Sungjun Jang",
            "Sangyoun Lee"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multi-agent motion prediction is a crucial concern in autonomous driving, yetit remains a challenge owing to the ambiguous intentions of dynamic agents andtheir intricate interactions. Existing studies have attempted to captureinteractions between road entities by using the definite data in historytimesteps, as future information is not available and involves highuncertainty. However, without sufficient guidance for capturing future statesof interacting agents, they frequently produce unrealistic trajectory overlaps.In this work, we propose Future Interaction modeling for Motion Prediction(FIMP), which captures potential future interactions in an end-to-end manner.FIMP adopts a future decoder that implicitly extracts the potential futureinformation in an intermediate feature-level, and identifies the interactingentity pairs through future affinity learning and top-k filtering strategy.Experiments show that our future interaction modeling improves the performanceremarkably, leading to superior performance on the Argoverse motion forecastingbenchmark."
    },
    {
        "link": "https://arxiv.org/abs/2401.16191",
        "title": "From Tripods to Bipods: Reducing the Queue Number of Planar Graphs Costs Just One Leg",
        "authors": [
            "Henry F\u00f6rster"
        ],
        "primary_subject": "Discrete Mathematics (cs.DM)",
        "abstract": "As an alternative to previously existing planar graph product structuretheorems, we prove that every planar graph G is a subgraph of the strongproduct of K2, a path and a planar subgraph of a 4-tree. As anapplication, we show that the queue number of planar graphs is at most 38whereas the queue number of planar bipartite graphs is at most 25."
    },
    {
        "link": "https://arxiv.org/abs/2401.16193",
        "title": "Contributing Dimension Structure of Deep Feature for Coreset Selection",
        "authors": [
            "Zhijing Wan",
            "Zhixiang Wang",
            "Yuran Wang",
            "Zheng Wang",
            "Hongyuan Zhu",
            "Shin'ichi Satoh"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Coreset selection seeks to choose a subset of crucial training samples forefficient learning. It has gained traction in deep learning, particularly withthe surge in training dataset sizes. Sample selection hinges on two mainaspects: a sample's representation in enhancing performance and the role ofsample diversity in averting overfitting. Existing methods typically measureboth the representation and diversity of data based on similarity metrics, suchas L2-norm. They have capably tackled representation via distribution matchingguided by the similarities of features, gradients, or other information betweendata. However, the results of effectively diverse sample selection are mired insub-optimality. This is because the similarity metrics usually simply aggregatedimension similarities without acknowledging disparities among the dimensionsthat significantly contribute to the final similarity. As a result, they fallshort of adequately capturing diversity. To address this, we propose afeature-based diversity constraint, compelling the chosen subset to exhibitmaximum diversity. Our key lies in the introduction of a novel ContributingDimension Structure (CDS) metric. Different from similarity metrics thatmeasure the overall similarity of high-dimensional features, our CDS metricconsiders not only the reduction of redundancy in feature dimensions, but alsothe difference between dimensions that contribute significantly to the finalsimilarity. We reveal that existing methods tend to favor samples with similarCDS, leading to a reduced variety of CDS types within the coreset andsubsequently hindering model performance. In response, we enhance theperformance of five classical selection methods by integrating the CDSconstraint. Our experiments on three datasets demonstrate the generaleffectiveness of the proposed method in boosting existing methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.16195",
        "title": "Dot-depth three, return of the J-class",
        "authors": [
            "Thomas Place",
            "Marc Zeitoun"
        ],
        "primary_subject": "Formal Languages and Automata Theory (cs.FL)",
        "abstract": "We look at concatenation hierarchies of classes of regular languages. Eachsuch hierarchy is determined by a single class, its basis: level n is builtby applying the Boolean polynomial closure operator (BPol), n times to thebasis. A prominent and difficult open question in automata theory is to decidemembership of a regular language in a given level. For instance, for thehistorical dot-depth hierarchy, the decidability of membership is only known atlevels one and two.We give a generic algebraic characterization of the operator BPol. Thischaracterization implies that for any concatenation hierarchy, if n is atleast two, membership at level n reduces to a more complex problem, calledcovering, for the previous level, n\u22121. Combined with earlier results oncovering, this implies that membership is decidable for dot-depth three and forlevel two in most of the prominent hierarchies in the literature. For instance,we obtain that the levels two in both the modulo hierarchy and the grouphierarchy have decidable membership."
    },
    {
        "link": "https://arxiv.org/abs/2401.16197",
        "title": "Geospatial Disparities: A Case Study on Real Estate Prices in Paris",
        "authors": [
            "Agathe Fernandes Machado",
            "Fran\u00e7ois Hu",
            "Philipp Ratz",
            "Ewen Gallic",
            "Arthur Charpentier"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Driven by an increasing prevalence of trackers, ever more IoT sensors, andthe declining cost of computing power, geospatial information has come to playa pivotal role in contemporary predictive models. While enhancing prognosticperformance, geospatial data also has the potential to perpetuate manyhistorical socio-economic patterns, raising concerns about a resurgence ofbiases and exclusionary practices, with their disproportionate impacts onsociety. Addressing this, our paper emphasizes the crucial need to identify andrectify such biases and calibration errors in predictive models, particularlyas algorithms become more intricate and less interpretable. The increasinggranularity of geospatial information further introduces ethical concerns, aschoosing different geographical scales may exacerbate disparities akin toredlining and exclusionary zoning. To address these issues, we propose atoolkit for identifying and mitigating biases arising from geospatial data.Extending classical fairness definitions, we incorporate an ordinal regressioncase with spatial attributes, deviating from the binary classification focus.This extension allows us to gauge disparities stemming from data aggregationlevels and advocates for a less interfering correction approach. Illustratingour methodology using a Parisian real estate dataset, we showcase practicalapplications and scrutinize the implications of choosing geographicalaggregation levels for fairness and calibration measures."
    },
    {
        "link": "https://arxiv.org/abs/2401.16198",
        "title": "Contracting with a Learning Agent",
        "authors": [
            "Guru Guruganesh",
            "Yoav Kolumbus",
            "Jon Schneider",
            "Inbal Talgam-Cohen",
            "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
            "Joshua R. Wang",
            "S. Matthew Weinberg"
        ],
        "primary_subject": "Computer Science and Game Theory (cs.GT)",
        "abstract": "Many real-life contractual relations differ completely from the clean, staticmodel at the heart of principal-agent theory. Typically, they involve repeatedstrategic interactions of the principal and agent, taking place underuncertainty and over time. While appealing in theory, players seldom usecomplex dynamic strategies in practice, often preferring to circumventcomplexity and approach uncertainty through learning. We initiate the study ofrepeated contracts with a learning agent, focusing on agents who achieveno-regret outcomes.Optimizing against a no-regret agent is a known open problem in generalgames; we achieve an optimal solution to this problem for a canonical contractsetting, in which the agent's choice among multiple actions leads tosuccess/failure. The solution has a surprisingly simple structure: for some\u03b1>0, initially offer the agent a linear contract with scalar \u03b1,then switch to offering a linear contract with scalar 0. This switch causesthe agent to ``free-fall'' through their action space and during this timeprovides the principal with non-zero reward at zero cost. Despite apparentexploitation of the agent, this dynamic contract can leave \\emph{both} playersbetter off compared to the best static contract. Our results generalize beyondsuccess/failure, to arbitrary non-linear contracts which the principal rescalesdynamically.Finally, we quantify the dependence of our results on knowledge of the timehorizon, and are the first to address this consideration in the study ofstrategizing against learning agents."
    },
    {
        "link": "https://arxiv.org/abs/2401.16199",
        "title": "Optimal quadrature errors and sampling numbers for Sobolev spaces with logarithmic perturbation on spheres",
        "authors": [
            "Jiaxin Geng",
            "Yun Ling",
            "Jiansong Li",
            "Heping Wang"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we study optimal quadrature errors, approximation numbers, andsampling numbers in L2(Sd) for Sobolev spaces H\u03b1,\u03b2(Sd) with logarithmic perturbation on the unit sphereSd in Rd+1. First we obtain strong equivalences of theapproximation numbers for H\u03b1,\u03b2(Sd) with \u03b1>0,which gives a clue to Open problem 3 as posed by Krieg and Vyb\\'iral in\\cite{KV}. Second, for the optimal quadrature errors for H\u03b1,\u03b2(Sd), we use the \"fooling\" function technique to getlower bounds in the case \u03b1>d/2, and apply Hilbert space structure andVyb\\'iral's theorem about Schur product theory to obtain lower bounds in thecase \u03b1=d/2,\u03b2>1/2 of small smoothness, which confirms theconjecture as posed by Grabner and Stepanyukin in \\cite{GS} and solves Openproblem 2 in \\cite{KV}. Finally, we employ the weighted least squares operatorsand the least squares quadrature rules to obtain approximation theorems andquadrature errors for H\u03b1,\u03b2(Sd) with \u03b1>d/2 or\u03b1=d/2,\u03b2>1/2, which are order optimal."
    },
    {
        "link": "https://arxiv.org/abs/2401.16202",
        "title": "FPIA: Field-Programmable Ising Arrays with In-Memory Computing",
        "authors": [
            "George Higgins Hutchinson",
            "Ethan Sifferman",
            "Tinish Bhattacharya",
            "Dmitri B. Strukov"
        ],
        "primary_subject": "Hardware Architecture (cs.AR)",
        "abstract": "Ising Machine is a promising computing approach for solving combinatorialoptimization problems. It is naturally suited for energy-saving and compactin-memory computing implementations with emerging memories. A na\\\"ive in-memorycomputing implementation of a quadratic Ising Machine requires an array ofcoupling weights that grows quadratically with problem size. However, theresources in such an approach are used inefficiently due to sparsity inpractical optimization problems. We first show that this issue can be addressedby partitioning a coupling array into smaller sub-arrays. This technique,however, requires interconnecting subarrays; hence, we developed in-memorycomputing architecture for quadratic Ising Machines inspired by island-typefield programmable gate arrays, which is the main contribution of our paper. Weadapt open-source tools to optimize problem embedding and model routingoverhead. Modeling results of benchmark problems for the developed architectureshow up to 60x area improvement and faster operation than the baselineapproach. Finally, we discuss algorithm/circuit co-design techniques forfurther improvements."
    },
    {
        "link": "https://arxiv.org/abs/2401.16204",
        "title": "Computing High-Degree Polynomial Gradients in Memory",
        "authors": [
            "T. Bhattacharya",
            "G. H. Hutchinson",
            "G. Pedretti",
            "X. Sheng",
            "J. Ignowski",
            "T. Van Vaerenbergh",
            "R. Beausoleil",
            "J.P. Strachan",
            "D.B. Strukov"
        ],
        "primary_subject": "Emerging Technologies (cs.ET)",
        "abstract": "Specialized function gradient computing hardware could greatly improve theperformance of state-of-the-art optimization algorithms, e.g., based ongradient descent or conjugate gradient methods that are at the core of control,machine learning, and operations research applications. Prior work on suchhardware, performed in the context of the Ising Machines and related concepts,is limited to quadratic polynomials and not scalable to commonly usedhigher-order functions. Here, we propose a novel approach for massivelyparallel gradient calculations of high-degree polynomials, which is conduciveto efficient mixed-signal in-memory computing circuit implementations and whosearea complexity scales linearly with the number of variables and terms in thefunction and, most importantly, independent of its degree. Two flavors of suchan approach are proposed. The first is limited to binary-variable polynomialstypical in combinatorial optimization problems, while the second type isbroader at the cost of a more complex periphery. To validate the formerapproach, we experimentally demonstrated solving a small-scale third-orderBoolean satisfiability problem based on integrated metal-oxide memristorcrossbar circuits, one of the most prospective in-memory computing devicetechnologies, with a competitive heuristics algorithm. Simulation results forlarger-scale, more practical problems show orders of magnitude improvements inthe area, and related advantages in speed and energy efficiency compared to thestate-of-the-art. We discuss how our work could enable even higher-performancesystems after co-designing algorithms to exploit massively parallel gradientcomputation."
    },
    {
        "link": "https://arxiv.org/abs/2401.16205",
        "title": "CognitiveOS: Large Multimodal Model based System to Endow Any Type of Robot with Generative AI",
        "authors": [
            "Artem Lykov",
            "Mikhail Konenkov",
            "Koffivi Fid\u00e8le Gbagbe",
            "Mikhail Litvinov",
            "Robinroy Peter",
            "Denis Davletshin",
            "Aleksey Fedoseev",
            "Oleg Kobzarev",
            "Ali Alabbas",
            "Oussama Alyounes",
            "Miguel Altamirano Cabrera",
            "Dzmitry Tsetserukou"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "This paper introduces CognitiveOS, a disruptive system based on multipletransformer-based models, endowing robots of various types with cognitiveabilities not only for communication with humans but also for task resolutionthrough physical interaction with the environment. The system operates smoothlyon different robotic platforms without extra tuning. It autonomously makesdecisions for task execution by analyzing the environment and using informationfrom its long-term memory. The system underwent testing on various platforms,including quadruped robots and manipulator robots, showcasing its capability toformulate behavioral plans even for robots whose behavioral examples wereabsent in the training dataset.Experimental results revealed the system's high performance in advanced taskcomprehension and adaptability, emphasizing its potential for real-worldapplications. The chapters of this paper describe the key components of thesystem and the dataset structure. The dataset for fine-tuning step generationmodel is provided at the following link: link coming soon"
    },
    {
        "link": "https://arxiv.org/abs/2401.16209",
        "title": "MultiMUC: Multilingual Template Filling on MUC-4",
        "authors": [
            "William Gantt",
            "Shabnam Behzad",
            "Hannah YoungEun An",
            "Yunmo Chen",
            "Aaron Steven White",
            "Benjamin Van Durme",
            "Mahsa Yarmohammadi"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "We introduce MultiMUC, the first multilingual parallel corpus for templatefilling, comprising translations of the classic MUC-4 template fillingbenchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. Weobtain automatic translations from a strong multilingual machine translationsystem and manually project the original English annotations into each targetlanguage. For all languages, we also provide human translations for sentencesin the dev and test splits that contain annotated template arguments. Finally,we present baselines on MultiMUC both with state-of-the-art template fillingmodels and with ChatGPT."
    },
    {
        "link": "https://arxiv.org/abs/2401.16212",
        "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
        "authors": [
            "Lauren Martin",
            "Nick Whitehouse",
            "Stephanie Yiu",
            "Lizzie Catterson",
            "Rivindu Perera"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "This paper presents a groundbreaking comparison between Large Language Modelsand traditional legal contract reviewers, Junior Lawyers and Legal ProcessOutsourcers. We dissect whether LLMs can outperform humans in accuracy, speed,and cost efficiency during contract review. Our empirical analysis benchmarksLLMs against a ground truth set by Senior Lawyers, uncovering that advancedmodels match or exceed human accuracy in determining legal issues. In speed,LLMs complete reviews in mere seconds, eclipsing the hours required by theirhuman counterparts. Cost wise, LLMs operate at a fraction of the price,offering a staggering 99.97 percent reduction in cost over traditional methods.These results are not just statistics, they signal a seismic shift in legalpractice. LLMs stand poised to disrupt the legal industry, enhancingaccessibility and efficiency of legal services. Our research asserts that theera of LLM dominance in legal contract review is upon us, challenging thestatus quo and calling for a reimagined future of legal workflows."
    },
    {
        "link": "https://arxiv.org/abs/2401.16213",
        "title": "A Unified Study on Sequentiality in Universal Classification with Empirically Observed Statistics",
        "authors": [
            "Ching-Fang Li",
            "I-Hsiang Wang"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In hypothesis testing problems, taking samples sequentially and stoppingopportunistically to make the inference greatly enhances the reliability. Thedesign of the stopping and inference policy, however, critically relies on theknowledge of the underlying distribution of each hypothesis. When the knowledgeof distributions, say, P0 and P1 in the binary-hypothesis case, isreplaced by empirically observed statistics from the respective distributions,the gain of sequentiality is less understood when subject to universalityconstraints. In this work, the gap is mended by a unified study onsequentiality in the universal binary classification problem. We propose aunified framework where the universality constraints are set on the expectedstopping time as well as the type-I error exponent. The type-I error exponentis required to achieve a pre-set distribution-dependent constraint\u03bb(P0,P1) for all P0,P1. The framework is employed to investigatea semi-sequential and a fully-sequential setup, so that fair comparison can bemade with the fixed-length setup. The optimal type-II error exponents indifferent setups are characterized when the function \u03bb satisfies mildcontinuity conditions. The benefit of sequentiality is shown by comparing thesemi-sequential, the fully-sequential, and the fixed-length cases inrepresentative examples of \u03bb. Conditions under which sequentialityeradicates the trade-off between error exponents are also derived."
    },
    {
        "link": "https://arxiv.org/abs/2401.16215",
        "title": "Learning big logical rules by joining small rules",
        "authors": [
            "C\u00e9line Hocquette",
            "Andreas Niskanen",
            "Rolf Morel",
            "Matti J\u00e4rvisalo",
            "Andrew Cropper"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "A major challenge in inductive logic programming is learning big rules. Toaddress this challenge, we introduce an approach where we join small rules tolearn big rules. We implement our approach in a constraint-driven system anduse constraint solvers to efficiently join rules. Our experiments on manydomains, including game playing and drug design, show that our approach can (i)learn rules with more than 100 literals, and (ii) drastically outperformexisting approaches in terms of predictive accuracies."
    },
    {
        "link": "https://arxiv.org/abs/2401.16216",
        "title": "A mechanism for discovering semantic relationships among agent communication protocols",
        "authors": [
            "Idoia Berges",
            "Jes\u00fas Berm\u00fadez",
            "Alfredo Go\u00f1i",
            "Arantza Illarramendi"
        ],
        "primary_subject": "Multiagent Systems (cs.MA)",
        "abstract": "One relevant aspect in the development of the Semantic Web framework is theachievement of a real inter-agents communication capability at the semanticlevel. Agents should be able to communicate with each other freely usingdifferent communication protocols, constituted by communication acts. For thatscenario, we introduce in this paper an efficient mechanism presenting thefollowing main features: - It promotes the description of the communicationacts of protocols as classes that belong to a communication acts ontology, andassociates to those acts a social commitment semantics formalized throughpredicates in the Event Calculus. - It is sustained on the idea that differentprotocols can be compared semantically by looking to the set of fluentsassociated to each branch of the protocols. Those sets are generated usingSemantic Web technology rules. - It discovers the following types of protocolrelationships: equivalence, specialization, restriction, prefix, suffix, infixand complement_to_infix."
    },
    {
        "link": "https://arxiv.org/abs/2401.16221",
        "title": "Foundations of Work-Systems Modeling",
        "authors": [
            "Henderik Alex Proper"
        ],
        "primary_subject": "Other Computer Science (cs.OH)",
        "abstract": "In 2006, the course \"Modeling of Organizations\" is taught for the third time.This third time will be the second time we will use the new lecture notes \"WorkSystems Modelling\" from the DA VINCI series. These lecture notes, however, willbe evolved further hand-in-hand with the actual process of lecturing. In theacademic year 2005/2006, a second incarnation of these lecture notes will becreated, where the aim is to deliver these lecture notes in three increments.An important step that will be taken in this academic year is the integrationof the ICIS Work Systems Modelling lecture notes with the NICI course onOrganisational Dynamics. The first results of this integration will start toappear in the second and third trimester."
    },
    {
        "link": "https://arxiv.org/abs/2401.16222",
        "title": "Modelling Solar PV Adoption in Irish Dairy Farms using Agent-Based Modelling",
        "authors": [
            "Iias Faiud",
            "Michael Schukat",
            "Karl Mason"
        ],
        "primary_subject": "Multiagent Systems (cs.MA)",
        "abstract": "The agricultural sector is facing mounting demands to enhance energyefficiency within farm enterprises, concurrent with a steady escalation inelectricity costs. This paper focuses on modelling the adoption rate ofphotovoltaic (PV) energy within the dairy sector in Ireland. An agent-basedmodelling approach is introduced to estimate the adoption rate. The modelconsiders grid energy prices, revenue, costs, and maintenance expenses tocalculate the probability of PV adoption. The ABM outputs estimate that by year2022, 2.45% of dairy farmers have installed PV. This is a 0.45% difference tothe actual PV adoption rate in year 2022. This validates the proposed ABM. Thepaper demonstrates the increasing interest in PV systems as evidenced by therate of adoption, shedding light on the potential advantages of PV energyadoption in agriculture. This study possesses the potential to forecast futurerates of PV energy adoption among dairy farmers. It establishes a groundworkfor further research on predicting and understanding the factors influencingthe adoption of renewable energy."
    },
    {
        "link": "https://arxiv.org/abs/2401.16224",
        "title": "Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models",
        "authors": [
            "Zhongjie Duan",
            "Chengyu Wang",
            "Cen Chen",
            "Weining Qian",
            "Jun Huang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Toon shading is a type of non-photorealistic rendering task of animation. Itsprimary purpose is to render objects with a flat and stylized appearance. Asdiffusion models have ascended to the forefront of image synthesismethodologies, this paper delves into an innovative form of toon shading basedon diffusion models, aiming to directly render photorealistic videos into animestyles. In video stylization, extant methods encounter persistent challenges,notably in maintaining consistency and achieving high visual quality. In thispaper, we model the toon shading problem as four subproblems: stylization,consistency enhancement, structure guidance, and colorization. To address thechallenges in video stylization, we propose an effective toon shading approachcalled \\textit{Diffutoon}. Diffutoon is capable of rendering remarkablydetailed, high-resolution, and extended-duration videos in anime style. It canalso edit the content according to prompts via an additional branch. Theefficacy of Diffutoon is evaluated through quantitive metrics and humanevaluation. Notably, Diffutoon surpasses both open-source and closed-sourcebaseline approaches in our experiments. Our work is accompanied by the releaseof both the source code and example videos on Github (Project page:https://ecnu-cilab.github.io/DiffutoonProjectPage/)."
    },
    {
        "link": "https://arxiv.org/abs/2401.16227",
        "title": "A Volumetric Saliency Guided Image Summarization for RGB-D Indoor Scene Classification",
        "authors": [
            "Preeti Meena",
            "Himanshu Kumar",
            "Sandeep Yadav"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Image summary, an abridged version of the original visual content, can beused to represent the scene. Thus, tasks such as scene classification,identification, indexing, etc., can be performed efficiently using the uniquesummary. Saliency is the most commonly used technique for generating therelevant image summary. However, the definition of saliency is subjective innature and depends upon the application. Existing saliency detection methodsusing RGB-D data mainly focus on color, texture, and depth features.Consequently, the generated summary contains either foreground objects ornon-stationary objects. However, applications such as scene identificationrequire stationary characteristics of the scene, unlike state-of-the-artmethods. This paper proposes a novel volumetric saliency-guided framework forindoor scene classification. The results highlight the efficacy of the proposedmethod."
    },
    {
        "link": "https://arxiv.org/abs/2401.16228",
        "title": "On the Anatomy of Real-World R Code for Static Analysis",
        "authors": [
            "Florian Sihler",
            "Lukas Pietzschmann",
            "Raphael Straub",
            "Matthias Tichy",
            "Andor Diera",
            "Abdelhalim Dahou"
        ],
        "primary_subject": "Programming Languages (cs.PL)",
        "abstract": "CONTEXT The R programming language has a huge and active community,especially in the area of statistical computing. Its interpreted nature allowsfor several interesting constructs, like the manipulation of functions atrun-time, that hinder the static analysis of R programs. At the same time,there is a lack of existing research regarding how these features, or even theR language as a whole are used in practice. OBJECTIVE In this paper, we conducta large-scale, static analysis of more than 50 million lines of real-world Rprograms and packages to identify their characteristics and the features thatare actually used. Moreover, we compare the similarities and differencesbetween the scripts of R users and the implementations of package authors. Weprovide insights for static analysis tools like the lintr package as well aspotential interpreter optimizations and uncover areas for future research.METHOD We analyze 4230 R scripts submitted alongside publications and thesources of 19450 CRAN packages for over 350000 R files, collecting andsummarizing quantitative information for features of interest. RESULTS We finda high frequency of name-based indexing operations, assignments, and loops, buta low frequency for most of R's reflective functions. Furthermore, we findneither testing functions nor many calls to R's foreign function interface(FFI) in the publication submissions. CONCLUSION R scripts and package sourcesdiffer, for example, in their size, the way they include other packages, andtheir usage of R's reflective capabilities. We provide features that are usedfrequently and should be prioritized by static analysis tools, like operatorassignments, function calls, and certain reflective functions like load."
    },
    {
        "link": "https://arxiv.org/abs/2401.16230",
        "title": "Elementary first-order model checking for sparse graphs",
        "authors": [
            "Jakub Gajarsk\u00fd",
            "Micha\u0142 Pilipczuk",
            "Marek Soko\u0142owski",
            "Giannos Stamoulis",
            "Szymon Toru\u0144czyk"
        ],
        "primary_subject": "Logic in Computer Science (cs.LO)",
        "abstract": "It is known that for subgraph-closed graph classes the first-order modelchecking problem is fixed-parameter tractable if and only if the class isnowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014]. However, the dependencyon the formula size is non-elementary, and in fact, this is unavoidable evenfor the class of all trees [Frick and Grohe, LICS 2002]. On the other hand, itis known that the dependency is elementary for classes of bounded degree [Frickand Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis,ICALP 2023]. In this paper we generalise these results and almost completelycharacterise subgraph-closed graph classes for which the model checking problemis fixed-parameter tractable with an elementary dependency on the formula size.Those are the graph classes for which there exists a number d such that forevery r, some tree of depth d and size bounded by an elementary function ofr is avoided as an (\u2264r)-subdivision in all graphs in the class. Inparticular, this implies that if the class in question excludes a fixed tree asa topological minor, then first-order model checking for graphs in the class isfixed-parameter tractable with an elementary dependency on the formula size."
    },
    {
        "link": "https://arxiv.org/abs/2401.16231",
        "title": "Error Mitigation for Thermodynamic Computing",
        "authors": [
            "Maxwell Aifer",
            "Denis Melanson",
            "Kaelan Donatella",
            "Gavin Crooks",
            "Thomas Ahle",
            "Patrick J. Coles"
        ],
        "primary_subject": "Emerging Technologies (cs.ET)",
        "abstract": "While physics-based computing can offer speed and energy efficiency comparedto digital computing, it also is subject to errors that must be mitigated. Forexample, many error mitigation methods have been proposed for quantumcomputing. However this error mitigation framework has yet to be applied toother physics-based computing paradigms. In this work, we considerthermodynamic computing, which has recently captured attention due to itsrelevance to artificial intelligence (AI) applications, such as probabilisticAI and generative AI. A key source of errors in this paradigm is theimprecision of the analog hardware components. Here, we introduce a method thatreduces the overall error from a linear to a quadratic dependence (from\u03f5 to \u03f52) on the imprecision \u03f5, for Gaussiansampling and linear algebra applications. The method involves sampling from anensemble of imprecise distributions associated with various rounding events andthen merging these samples. We numerically demonstrate the scalability of thismethod for dimensions greater than 1000. Finally, we implement this method onan actual thermodynamic computer and show 20% error reduction for matrixinversion; the first thermodynamic error mitigation experiment."
    },
    {
        "link": "https://arxiv.org/abs/2401.16232",
        "title": "Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis",
        "authors": [
            "Oleksandr Kuznetsov",
            "Dmytro Zakharov",
            "Emanuele Frontoni",
            "Andrea Maranesi",
            "Serhii Bohucharskyi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In an era where biometric security serves as a keystone of modern identityverification systems, ensuring the authenticity of these biometric samples isparamount. Liveness detection, the capability to differentiate between genuineand spoofed biometric samples, stands at the forefront of this challenge. Thisresearch presents a comprehensive evaluation of liveness detection models, witha particular focus on their performance in cross-database scenarios, a testparadigm notorious for its complexity and real-world relevance. Our studycommenced by meticulously assessing models on individual datasets, revealingthe nuances in their performance metrics. Delving into metrics such as the HalfTotal Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthedinvaluable insights into the models' strengths and weaknesses. Crucially, ourexploration of cross-database testing provided a unique perspective,highlighting the chasm between training on one dataset and deploying onanother. Comparative analysis with extant methodologies, ranging fromconvolutional networks to more intricate strategies, enriched our understandingof the current landscape. The variance in performance, even amongstate-of-the-art models, underscored the inherent challenges in this domain. Inessence, this paper serves as both a repository of findings and a clarion callfor more nuanced, data-diverse, and adaptable approaches in biometric livenessdetection. In the dynamic dance between authenticity and deception, our workoffers a blueprint for navigating the evolving rhythms of biometric security."
    },
    {
        "link": "https://arxiv.org/abs/2401.16233",
        "title": "Incremental Proof Development in Dafny with Module-Based Induction",
        "authors": [
            "Son Ho",
            "Cl\u00e9ment Pit-Claudel"
        ],
        "primary_subject": "Programming Languages (cs.PL)",
        "abstract": "Highly automated theorem provers like Dafny allow users to prove simpleproperties with little effort, making it easy to quickly sketch proofs. Thedrawback is that such provers leave users with little control about the proofsearch, meaning that the small changes inherent to the iterative process ofwriting a proof often lead to unpredictable variations in verification time,and eventually hard-to-diagnose proof failures. This sometimes turns the boonof high automation into a curse, as instead of breaking early and showingunsolved goals to the user like in Coq, proofs tend to gradually becomeunstable until their verification time explodes. At this point, the absence ofa proof context to investigate often leaves the user to a painful debuggingsession. In this paper, we show how to use Dafny modules to encode Coq-likeinduction principles to dramatically improve the stability and maintainabilityof proofs about inductive data structures."
    },
    {
        "link": "https://arxiv.org/abs/2401.16234",
        "title": "DAEDALUS: Defense Against Firmware ROP Exploits Using Stochastic Software Diversity",
        "authors": [
            "Islam Obaidat",
            "Meera Sridhar",
            "Fatemeh Tavakoli"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "This paper presents DAEDALUS, a software diversity-based framework designedto resist ROP attacks on Linux-based IoT devices. DAEDALUS generates unique,semantically equivalent but syntactically different rewrites of IoT firmware,disrupting large-scale replication of ROP attacks. DAEDALUS employs STOKE, astochastic optimizer for x86 binaries, as its core diversity engine butintroduces significant extensions to address unique IoT firmware challenges.DAEDALUS's effectiveness is evaluated using DDoSim, a published botnet DDoSattack simulation testbed. Results demonstrate that DAEDALUS successfullyneutralizes ROP payloads by diversifying critical basic blocks in the firmware,preventing attackers from compromising multiple devices for DDoS attacks viamemory error vulnerabilities. The findings indicate that DAEDALUS not onlymitigates the impact of ROP attacks on individual IoT devices throughprobabilistic protection but also thwarts large-scale ROP attacks acrossmultiple devices."
    },
    {
        "link": "https://arxiv.org/abs/2401.16235",
        "title": "Player Pressure Map - A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts",
        "authors": [
            "Chaoyi Gu",
            "Jiaming Na",
            "Yisheng Pei",
            "Varuna De Silva"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In soccer, contextual player performance metrics are invaluable to coaches.For example, the ability to perform under pressure during matches distinguishesthe elite from the average. Appropriate pressure metric enables teams to assessplayers' performance accurately under pressure and design targeted trainingscenarios to address their weaknesses. The primary objective of this paper isto leverage both tracking and event data and game footage to capture thepressure experienced by the possession team in a soccer game scene. We proposea player pressure map to represent a given game scene, which lowers thedimension of raw data and still contains rich contextual information. Not onlydoes it serve as an effective tool for visualizing and evaluating the pressureon the team and each individual, but it can also be utilized as a backbone foraccessing players' performance. Overall, our model provides coaches andanalysts with a deeper understanding of players' performance under pressure sothat they make data-oriented tactical decisions."
    },
    {
        "link": "https://arxiv.org/abs/2401.16236",
        "title": "Effective Communication with Dynamic Feature Compression",
        "authors": [
            "Pietro Talli",
            "Francesco Pase",
            "Federico Chiariotti",
            "Andrea Zanella",
            "Michele Zorzi"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The remote wireless control of industrial systems is one of the major usecases for 5G and beyond systems: in these cases, the massive amounts of sensoryinformation that need to be shared over the wireless medium may overload evenhigh-capacity connections. Consequently, solving the effective communicationproblem by optimizing the transmission strategy to discard irrelevantinformation can provide a significant advantage, but is often a very complextask. In this work, we consider a prototypal system in which an observer mustcommunicate its sensory data to a robot controlling a task (e.g., a mobilerobot in a factory). We then model it as a remote Partially Observable MarkovDecision Process (POMDP), considering the effect of adopting semantic andeffective communication-oriented solutions on the overall system performance.We split the communication problem by considering an ensemble Vector QuantizedVariational Autoencoder (VQ-VAE) encoding, and train a Deep ReinforcementLearning (DRL) agent to dynamically adapt the quantization level, consideringboth the current state of the environment and the memory of past messages. Wetested the proposed approach on the well-known CartPole reference controlproblem, obtaining a significant performance increase over traditionalapproaches."
    },
    {
        "link": "https://arxiv.org/abs/2401.16238",
        "title": "Alternating Minimization for Wideband Multiuser IRS-aided MIMO Systems under Imperfect CSI",
        "authors": [
            "Darian P\u00e9rez-Ad\u00e1n",
            "Michael Joham",
            "\u00d3scar Fresnedo",
            "Jos\u00e9 P. Gonz\u00e1lez-Coma",
            "Luis Castedo",
            "Wolfgang Utschick"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This work focuses on wideband intelligent reflecting surface (IRS)-aidedmultiuser MIMO systems. One of the major challenges of this scenario is thejoint design of the frequency-dependent base station (BS) precoder and userfilters, and the IRS phase-shift matrix which is frequency flat and common toall the users. In addition, we consider that the channel state information(CSI) is imperfect at both the transmitter and the receivers. A statisticalmodel for the imperfect CSI is developed and exploited for the system design. Aminimum mean square error (MMSE) approach is followed to determine the IRSphase-shift matrix, the transmit precoders, and the receiving filters. Thebroadcast (BC)- multiple access channel (MAC) duality is used to solve theoptimization problem following an alternating minimization approach. Numericalresults show that the proposed approach leads to substantial performance gainswith respect to baseline strategies that neglect the inter-user interferenceand do not optimize the IRS phase-shift matrix. Further performance gains areobtained when incorporating into the system design the statistical informationof the channel estimation errors."
    },
    {
        "link": "https://arxiv.org/abs/2401.16240",
        "title": "Clinically meaningful timeline summarisation in social media for mental health monitoring",
        "authors": [
            "Jiayu Song",
            "Jenny Chim",
            "Adam Tsakalidis",
            "Julia Ive",
            "Dana Atzil-Slonim",
            "Maria Liakata"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "We introduce the new task of clinically meaningful summarisation of socialmedia user timelines, appropriate for mental health monitoring. We develop anovel approach for unsupervised abstractive summarisation that produces atwo-layer summary consisting of both high-level information, covering aspectsuseful to clinical experts, as well as accompanying time sensitive evidencefrom a user's social media timeline. A key methodological novelty comes fromthe timeline summarisation component based on a version of hierarchicalvariational autoencoder (VAE) adapted to represent long texts and guided byLLM-annotated key phrases. The resulting timeline summary is input into a LLM(LLaMA-2) to produce the final summary containing both the high levelinformation, obtained through instruction prompting, as well as correspondingevidence from the user's timeline. We assess the summaries generated by ournovel architecture via automatic evaluation against expert written summariesand via human evaluation with clinical experts, showing that timelinesummarisation by TH-VAE results in logically coherent summaries rich inclinical utility and superior to LLM-only approaches in capturing changes overtime."
    },
    {
        "link": "https://arxiv.org/abs/2401.16241",
        "title": "Channel Estimation and Hybrid Precoding for Frequency Selective Multiuser mmWave MIMO Systems",
        "authors": [
            "J. P. Gonz\u00e1lez-Coma",
            "J. Rodr\u00edguez-Fern\u00e1ndez",
            "N. Gonz\u00e1lez-Prelcic",
            "L. Castedo",
            "R. W. Heath"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "Configuring the hybrid precoders and combiners in a millimeter wave (mmWave)multiuser (MU) multiple-input multiple-output (MIMO) system is challenging infrequency selective channels. In this paper, we develop a system that usescompressive estimation on the uplink to configure precoders and combiners forthe downlink (DL). In the first step, the base station (BS) simultaneouslyestimates the channels from all the mobile stations (MSs) on each subcarrier.To reduce the number of measurements required, compressed sensing techniquesare developed that exploit common support on the different subcarriers. In thesecond step, exploiting reciprocity and the channel estimates, the base stationdesigns hybrid precoders and combiners. Two algorithms are developed for thispurpose, with different performance and complexity tradeoffs: 1) afactorization of the purely digital solution, and 2) an iterative hybriddesign. Extensive numerical experiments evaluate the proposed solutionscomparing to state-of-the-art strategies, and illustrating design tradeoffs inoverhead, complexity, and performance."
    },
    {
        "link": "https://arxiv.org/abs/2401.16244",
        "title": "Employing Iterative Feature Selection in Fuzzy Rule-Based Binary Classification",
        "authors": [
            "Haoning Li",
            "Cong Wang",
            "Qinghua Huang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The feature selection in a traditional binary classification algorithm isalways used in the stage of dataset preprocessing, which makes the obtainedfeatures not necessarily the best ones for the classification algorithm, thusaffecting the classification performance. For a traditional rule-based binaryclassification algorithm, classification rules are usually deterministic, whichresults in the fuzzy information contained in the rules being ignored. To doso, this paper employs iterative feature selection in fuzzy rule-based binaryclassification. The proposed algorithm combines feature selection based onfuzzy correlation family with rule mining based on biclustering. It firstconducts biclustering on the dataset after feature selection. Then it conductsfeature selection again for the biclusters according to the feedback ofbiclusters evaluation. In this way, an iterative feature selection framework isbuild. During the iteration process, it stops until the obtained biclustermeets the requirements. In addition, the rule membership function is introducedto extract vectorized fuzzy rules from the bicluster and construct weakclassifiers. The weak classifiers with good classification performance areselected by Adaptive Boosting and the strong classifier is constructed by\"weighted average\". Finally, we perform the proposed algorithm on differentdatasets and compare it with other peers. Experimental results show that itachieves good classification performance and outperforms its peers."
    },
    {
        "link": "https://arxiv.org/abs/2401.16247",
        "title": "Towards Red Teaming in Multimodal and Multilingual Translation",
        "authors": [
            "Christophe Ropers",
            "David Dale",
            "Prangthip Hansanti",
            "Gabriel Mejia Gonzalez",
            "Ivan Evtimov",
            "Corinne Wong",
            "Christophe Touret",
            "Kristina Pereyra",
            "Seohyun Sonia Kim",
            "Cristian Canton Ferrer",
            "Pierre Andrews",
            "Marta R. Costa-juss\u00e0"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Assessing performance in Natural Language Processing is becoming increasinglycomplex. One particular challenge is the potential for evaluation datasets tooverlap with training data, either directly or indirectly, which can lead toskewed results and overestimation of model performance. As a consequence, humanevaluation is gaining increasing interest as a means to assess the performanceand reliability of models. One such method is the red teaming approach, whichaims to generate edge cases where a model will produce critical errors. Whilethis methodology is becoming standard practice for generative AI, itsapplication to the realm of conditional AI remains largely unexplored. Thispaper presents the first study on human-based red teaming for MachineTranslation (MT), marking a significant step towards understanding andimproving the performance of translation models. We delve into both human-basedred teaming and a study on automation, reporting lessons learned and providingrecommendations for both translation models and red teaming drills. Thispioneering work opens up new avenues for research and development in the fieldof MT."
    },
    {
        "link": "https://arxiv.org/abs/2401.16250",
        "title": "Efficient Solution of ill-posed integral equations through averaging",
        "authors": [
            "Michael Griebel",
            "Tim Jahn"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "This paper discusses the error and cost aspects of ill-posed integralequations when given discrete noisy point evaluations on a fine grid. Standardsolution methods usually employ discretization schemes that are directlyinduced by the measurement points. Thus, they may scale unfavorably with thenumber of evaluation points, which can result in computational inefficiency. Toaddress this issue, we propose an algorithm that achieves the same level ofaccuracy while significantly reducing computational costs. Our approachinvolves an initial averaging procedure to sparsify the underlying grid. Tokeep the exposition simple, we focus only on one-dimensional ill-posed integralequations that have sufficient smoothness. However, the approach can begeneralized to more complicated two- and three-dimensional problems withappropriate modifications."
    },
    {
        "link": "https://arxiv.org/abs/2401.16251",
        "title": "Cross-silo Federated Learning with Record-level Personalized Differential Privacy",
        "authors": [
            "Junxu Liu",
            "Jian Lou",
            "Li Xiong",
            "Jinfei Liu",
            "Xiaofeng Meng"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Federated learning enhanced by differential privacy has emerged as a popularapproach to better safeguard the privacy of client-side data by protectingclients' contributions during the training process. Existing solutionstypically assume a uniform privacy budget for all records and provideone-size-fits-all solutions that may not be adequate to meet each record'sprivacy requirement. In this paper, we explore the uncharted territory ofcross-silo FL with record-level personalized differential privacy. We devise anovel framework named rPDP-FL, employing a two-stage hybrid sampling schemewith both client-level sampling and non-uniform record-level sampling toaccommodate varying privacy requirements. A critical and non-trivial problem isto select the ideal per-record sampling probability q given the personalizedprivacy budget {\\epsilon}. We introduce a versatile solution namedSimulation-CurveFitting, allowing us to uncover a significant insight into thenonlinear correlation between q and {\\epsilon} and derive an elegantmathematical model to tackle the problem. Our evaluation demonstrates that oursolution can provide significant performance gains over the baselines that donot consider personalized privacy preservation."
    },
    {
        "link": "https://arxiv.org/abs/2401.16258",
        "title": "MosquIoT: A System Based on IoT and Machine Learning for the Monitoring of Aedes aegypti (Diptera: Culicidae)",
        "authors": [
            "Javier Aira",
            "Teresa Olivares Montes",
            "Francisco M. Delicado",
            "Dar\u00eco Vezzani"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Millions of people around the world are infected with mosquito-borne diseaseseach year. One of the most dangerous species is Aedes aegypti, the main vectorof viruses such as dengue, yellow fever, chikungunya, and Zika, among others.Mosquito prevention and eradication campaigns are essential to avoid majorpublic health consequences. In this respect, entomological surveillance is animportant tool. At present, this traditional monitoring tool is executedmanually and requires digital transformation to help authorities make betterdecisions, improve their planning efforts, speed up execution, and bettermanage available resources. Therefore, new technological tools based on proventechniques need to be designed and developed. However, such tools should alsobe cost-effective, autonomous, reliable, and easy to implement, and should beenabled by connectivity and multi-platform software applications. This paperpresents the design, development, and testing of an innovative system namedMosquIoT. It is based on traditional ovitraps with embedded Internet of Things(IoT) and Tiny Machine Learning (TinyML) technologies, which enable thedetection and quantification of Ae. aegypti eggs. This innovative and promisingsolution may help dynamically understand the behavior of Ae. aegyptipopulations in cities, shifting from the current reactive entomologicalmonitoring model to a proactive and predictive digital one."
    },
    {
        "link": "https://arxiv.org/abs/2401.16261",
        "title": "Using multiple Dirac delta points to describe inhomogeneous flux density over a cell boundary in a single-cell diffusion model",
        "authors": [
            "Qiyao Peng",
            "Sander C. Hille"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Biological cells can release compounds into their direct environment,generally inhomogeneously over their cell membrane, after which the compoundsspread by diffusion. In mathematical modelling and simulation of a collectiveof such cells, it is theoretically and numerically advantageous to replacespatial extended cells with point sources, in particular when cell numbers arelarge, but still so small that a continuum density description cannot bejustified, or when cells are moving. We show that inhomogeneous flux densityover the cell boundary may be realized in a point source approach, thusmaintaining computational efficiency, by utilizing multiple, clustered pointsources (and sinks). In this report, we limit ourselves to a sinusoidalfunction as flux density in the spatial exclusion model, and we show how todetermine the amplitudes of the Dirac delta points in the point source model,such that the deviation between the point source model and the spatialexclusion model is small."
    },
    {
        "link": "https://arxiv.org/abs/2401.16263",
        "title": "Collaboration Petri Nets: Verification, Equivalence, and Discovery (Extended Version)",
        "authors": [
            "Janik-Vasily Benzin",
            "Stefanie Rinderle-Ma"
        ],
        "primary_subject": "Formal Languages and Automata Theory (cs.FL)",
        "abstract": "Process modeling and discovery techniques aim to construct sound and validprocess models for different types of processes, i.e., process orchestrationsand collaboration processes. Orchestrations represent behavior of cases withinone process. Collaboration processes represent behavior of collaborating caseswithin multiple process orchestrations that interact via collaboration conceptssuch as organizations, agents, objects, and services. The heterogeneity ofcollaboration concepts and types such as message exchange and resource sharinghas led to different representations and discovery techniques for collaborationprocess models, but a standard model class is lacking. We propose collaborationPetri nets (cPN) to achieve comparability between techniques, to enableapproach and property transfer, and to build a standardized collaborationmining pipeline similar to process mining. For cPN, we require desirablemodeling power, decision power, modeling convenience, and relations to existingmodel classes. We show the representation of collaboration types, structuralcharacterization as workflow nets, automatic verification of soundness,bisimulation equivalence to existing model classes, and application in ageneral discovery framework. As empirical evidence to discover cPN, we conducta comparative evaluation between three discovery techniques on a set ofexisting collaboration event logs."
    },
    {
        "link": "https://arxiv.org/abs/2401.16265",
        "title": "CO2: Efficient Distributed Training with Full Communication-Computation Overlap",
        "authors": [
            "Weigao Sun",
            "Zhen Qin",
            "Weixuan Sun",
            "Shidi Li",
            "Dong Li",
            "Xuyang Shen",
            "Yu Qiao",
            "Yiran Zhong"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "The fundamental success of large language models hinges upon the efficaciousimplementation of large-scale distributed training techniques. Nevertheless,building a vast, high-performance cluster featuring high-speed communicationinterconnectivity is prohibitively costly, and accessible only to prominententities. In this work, we aim to lower this barrier and democratizelarge-scale training with limited bandwidth clusters. We propose a new approachcalled CO2 that introduces local-updating and asynchronous communication to thedistributed data-parallel training, thereby facilitating the full overlap ofCOmunication with COmputation. CO2 is able to attain a high scalability even onextensive multi-node clusters constrained by very limited communicationbandwidth. We further propose the staleness gap penalty and outer momentumclipping techniques together with CO2 to bolster its convergence and trainingstability. Besides, CO2 exhibits seamless integration with well-establishedZeRO-series optimizers which mitigate memory consumption of model states withlarge model training. We also provide a mathematical proof of convergence,accompanied by the establishment of a stringent upper bound. Furthermore, wevalidate our findings through an extensive set of practical experimentsencompassing a wide range of tasks in the fields of computer vision and naturallanguage processing. These experiments serve to demonstrate the capabilities ofCO2 in terms of convergence, generalization, and scalability when deployedacross configurations comprising up to 128 A100 GPUs. The outcomes emphasizethe outstanding capacity of CO2 to hugely improve scalability, no matter onclusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections."
    },
    {
        "link": "https://arxiv.org/abs/2401.16268",
        "title": "A.I. In All The Wrong Places",
        "authors": [
            "Marc B\u00f6hlen",
            "Ruolin Chen",
            "Xiaoxu Dong",
            "Srikar Gopaladinne",
            "Hemanth Gorla",
            "Divya Kandukuri",
            "Sean Mansfield"
        ],
        "primary_subject": "Computers and Society (cs.CY)",
        "abstract": "This text describes experiences gained across a two-year test period duringwhich two generations of Generative Artificial Intelligence (A.I.) systems wereincorpo-rated into an interdisciplinary, university level course on A.I. forart and design practices. The text uses the results from the courses to reflecton new opportuni-ties for generative systems in art and design whileconsidering traps and limits."
    },
    {
        "link": "https://arxiv.org/abs/2401.16270",
        "title": "Capturing Knowledge Graphs and Rules with Octagon Embeddings",
        "authors": [
            "Victor Charpenay",
            "Steven Schockaert"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Region based knowledge graph embeddings represent relations as geometricregions. This has the advantage that the rules which are captured by the modelare made explicit, making it straightforward to incorporate prior knowledge andto inspect learned models. Unfortunately, existing approaches are severelyrestricted in their ability to model relational composition, and hence alsotheir ability to model rules, thus failing to deliver on the main promise ofregion based models. With the aim of addressing these limitations, weinvestigate regions which are composed of axis-aligned octagons. Such octagonsare particularly easy to work with, as intersections and compositions can bestraightforwardly computed, while they are still sufficiently expressive tomodel arbitrary knowledge graphs. Among others, we also show that our octagonembeddings can properly capture a non-trivial class of rule bases. Finally, weshow that our model achieves competitive experimental results."
    },
    {
        "link": "https://arxiv.org/abs/2401.16274",
        "title": "The HSF Conditions Database Reference Implementation",
        "authors": [
            "Ruslan Mashinistov",
            "Lino Gerlach",
            "Paul Laycock",
            "Andrea Formica",
            "Giacomo Govi",
            "Chris Pinkenburg"
        ],
        "primary_subject": "Databases (cs.DB)",
        "abstract": "Conditions data is the subset of non-event data that is necessary to processevent data. It poses a unique set of challenges, namely a heterogeneousstructure and high access rates by distributed computing. The HSF ConditionsDatabases activity is a forum for cross-experiment discussions inviting asbroad a participation as possible. It grew out of the HSF Community White Paperwork to study conditions data access, where experts from ATLAS, Belle II, andCMS converged on a common language and proposed a schema that represents bestpractice. Following discussions with a broader community, including NP as wellas HEP experiments, a core set of use cases, functionality and behaviour wasdefined with the aim to describe a core conditions database API. This paperwill describe the reference implementation of both the conditions databaseservice and the client which together encapsulate HSF best practice conditionsdata handling. Django was chosen for the service implementation, which uses anORM instead of the direct use of SQL for all but one method. The simplerelational database schema to organise conditions data is implemented inPostgreSQL. The task of storing conditions data payloads themselves isoutsourced to any POSIX- compliant filesystem, allowing for transparentrelocation and redundancy. Cru- cially this design provides a clear separationbetween retrieving the metadata describing which conditions data are needed fora data processing job, and retrieving the actual payloads from storage. Theservice deployment using Helm on OKD will be described together with scalingtests and operations experience from the sPHENIX experiment running more than25k cores at BNL."
    },
    {
        "link": "https://arxiv.org/abs/2401.16277",
        "title": "SECOMP: Formally Secure Compilation of Compartmentalized C Programs",
        "authors": [
            "J\u00e9r\u00e9my Thibault",
            "Roberto Blanco",
            "Dongjae Lee",
            "Sven Argo",
            "Arthur Azevedo de Amorim",
            "A\u00efna Linn Georges",
            "Catalin Hritcu",
            "Andrew Tolmach"
        ],
        "primary_subject": "Programming Languages (cs.PL)",
        "abstract": "Undefined behavior in C often causes devastating security vulnerabilities.One practical mitigation is compartmentalization, which allows developers tostructure large programs into mutually distrustful compartments with clearlyspecified privileges and interactions. In this paper we introduce SECOMP, acompiler for compartmentalized C code that comes with machine-checked proofsguaranteeing that the scope of undefined behavior is restricted to thecompartments that encounter it and become dynamically compromised. Theseguarantees are formalized as the preservation of safety properties againstadversarial contexts, a secure compilation criterion similar to fullabstraction, and this is the first time such a strong criterion is proven for amainstream programming language. To achieve this we extend the languages of theCompCert verified C compiler with isolated compartments that can only interactvia procedure calls and returns, as specified by cross-compartment interfaces.We adapt the passes and optimizations of CompCert as well as their correctnessproofs to this compartment-aware setting. We then use compiler correctness asan ingredient in a larger secure compilation proof that involves several proofengineering novelties, needed to scale formally secure compilation up to a Ccompiler."
    },
    {
        "link": "https://arxiv.org/abs/2401.16279",
        "title": "Rethinking the Producer-Consumer Relationship in Modern DRAM-Based Systems",
        "authors": [
            "Minesh Patel",
            "Taha Shahroodi",
            "Aditya Manglik",
            "Abdullah Giray Ya\u011fl\u0131k\u00e7\u0131",
            "Ataberk Olgun",
            "Haocong Luo",
            "Onur Mutlu"
        ],
        "primary_subject": "Hardware Architecture (cs.AR)",
        "abstract": "Generational improvements to commodity DRAM throughout half a century havelong solidified its prevalence as main memory across the computing industry.However, overcoming today's DRAM technology scaling challenges requires newsolutions driven by both DRAM producers and consumers. In this paper, weobserve that the separation of concerns between producers and consumersspecified by industry-wide DRAM standards is becoming a liability to progressin addressing scaling-related concerns.To understand the problem, we study four key directions for overcoming DRAMscaling challenges using system-memory cooperation: (i) improving memory accesslatencies; (ii) reducing DRAM refresh overheads; (iii) securely defendingagainst the RowHammer vulnerability; and (iv) addressing worsening memoryerrors. We find that the single most important barrier to advancement in allfour cases is the consumer's lack of insight into DRAM reliability. Based on ananalysis of DRAM reliability testing, we recommend revising the separation ofconcerns to incorporate limited information transparency between producers andconsumers. Finally, we propose adopting this revision in a two-step plan,starting with immediate information release through crowdsourcing andpublication and culminating in widespread modifications to DRAM standards."
    },
    {
        "link": "https://arxiv.org/abs/2401.16280",
        "title": "Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model",
        "authors": [
            "Till Grutschus",
            "Ola Karrar",
            "Emir Esenov",
            "Ekta Vats"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This work explores the performance of a large video understanding foundationmodel on the downstream task of human fall detection on untrimmed video andleverages a pretrained vision transformer for multi-class action detection,with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\". Amethod for temporal action localization that relies on a simple cutup ofuntrimmed videos is demonstrated. The methodology includes a preprocessingpipeline that converts datasets with timestamp action annotations into labeleddatasets of short action clips. Simple and effective clip-sampling strategiesare introduced. The effectiveness of the proposed method has been empiricallyevaluated on the publicly available High-Quality Fall Simulation Dataset(HQFSD). The experimental results validate the performance of the proposedpipeline. The results are promising for real-time application, and the fallsare detected on video level with a state-of-the-art 0.96 F1 score on the HQFSDdataset under the given experimental settings. The source code will be madeavailable on GitHub."
    },
    {
        "link": "https://arxiv.org/abs/2401.16282",
        "title": "MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification",
        "authors": [
            "Xia Zeng",
            "Arkaitz Zubiaga"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Claim verification is an essential step in the automated fact-checkingpipeline which assesses the veracity of a claim against a piece of evidence. Inthis work, we explore the potential of few-shot claim verification, where onlyvery limited data is available for supervision. We propose MAPLE (MicroAnalysis of Pairwise Language Evolution), a pioneering approach that exploresthe alignment between a claim and its evidence with a small seq2seq model and anovel semantic measure. Its innovative utilization of micro language evolutionpath leverages unlabelled pairwise data to facilitate claim verification whileimposing low demand on data annotations and computing resources. MAPLEdemonstrates significant performance improvements over SOTA baselines SEED, PETand LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, andSciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE"
    },
    {
        "link": "https://arxiv.org/abs/2401.16284",
        "title": "Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation",
        "authors": [
            "Jaewoo Park",
            "Jaeguk Kim",
            "Nam Ik Cho"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurately estimating the pose of an object is a crucial task in computervision and robotics. There are two main deep learning approaches for this:geometric representation regression and iterative refinement. However, thesemethods have some limitations that reduce their effectiveness. In this paper,we analyze these limitations and propose new strategies to overcome them. Totackle the issue of blurry geometric representation, we use positional encodingwith high-frequency components for the object's 3D coordinates. To address thelocal minimum problem in refinement methods, we introduce a normalized imageplane-based multi-reference refinement strategy that's independent of intrinsicmatrix constraints. Lastly, we utilize adaptive instance normalization and asimple occlusion augmentation method to help our model concentrate on thetarget object. Our experiments on Linemod, Linemod-Occlusion, and YCB-Videodatasets demonstrate that our approach outperforms existing methods. We willsoon release the code."
    },
    {
        "link": "https://arxiv.org/abs/2401.16285",
        "title": "Capturing Pertinent Symbolic Features for Enhanced Content-Based Misinformation Detection",
        "authors": [
            "Flavio Merenda",
            "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Preventing the spread of misinformation is challenging. The detection ofmisleading content presents a significant hurdle due to its extreme linguisticand domain variability. Content-based models have managed to identify deceptivelanguage by learning representations from textual data such as social mediaposts and web articles. However, aggregating representative samples of thisheterogeneous phenomenon and implementing effective real-world applications isstill elusive. Based on analytical work on the language of misinformation, thispaper analyzes the linguistic attributes that characterize this phenomenon andhow representative of such features some of the most popular misinformationdatasets are. We demonstrate that the appropriate use of pertinent symbolicknowledge in combination with neural language models is helpful in detectingmisleading content. Our results achieve state-of-the-art performance inmisinformation datasets across the board, showing that our approach offers avalid and robust alternative to multi-task transfer learning without requiringany additional training data. Furthermore, our results show evidence thatstructured knowledge can provide the extra boost required to address a complexand unpredictable real-world problem like misinformation detection, not only interms of accuracy but also time efficiency and resource utilization."
    },
    {
        "link": "https://arxiv.org/abs/2401.16287",
        "title": "GAPS: Geometry-Aware Problem Solver",
        "authors": [
            "Jiaxin Zhang",
            "Yinghui Jiang",
            "Yashar Moshfeghi"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Geometry problem solving presents a formidable challenge within the NLPcommunity. Existing approaches often rely on models designed for solving mathword problems, neglecting the unique characteristics of geometry math problems.Additionally, the current research predominantly focuses on geometrycalculation problems, while overlooking other essential aspects like proving.In this study, we address these limitations by proposing the Geometry-AwareProblem Solver (GAPS) model. GAPS is specifically designed to generate solutionprograms for geometry math problems of various types with the help of itsunique problem-type classifier. To achieve this, GAPS treats the solutionprogram as a composition of operators and operands, segregating theirgeneration processes. Furthermore, we introduce the geometry elementsenhancement method, which enhances the ability of GAPS to recognize geometryelements accurately. By leveraging these improvements, GAPS showcasesremarkable performance in resolving geometry math problems. Our experimentsconducted on the UniGeo dataset demonstrate the superiority of GAPS over thestate-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracyimprovement of more than 5.3% for calculation tasks and an impressive 41.1% forproving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% onproving problems, representing a significant advancement in solving geometryproving tasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.16288",
        "title": "Upper bounds on the rate of linear",
        "authors": [
            "Stefano Della Fiore",
            "Marco Dalai"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This paper presents new upper bounds on the rate of linear k-hash codes inFnq, q\u2265k, that is, codes with the property that any kdistinct codewords are all simultaneously distinct in at least one coordinate."
    },
    {
        "link": "https://arxiv.org/abs/2401.16291",
        "title": "MachineLearnAthon: An Action-Oriented Machine Learning Didactic Concept",
        "authors": [
            "Michal Tk\u00e1\u010d",
            "Jakub Sieber",
            "Lara Kuhlmann",
            "Matthias Brueggenolte",
            "Alexandru Rinciog",
            "Michael Henke",
            "Artur M. Schweidtmann",
            "Qinghe Gao",
            "Maximilian F. Theisen",
            "Radwa El Shawi"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Machine Learning (ML) techniques are encountered nowadays across disciplines,from social sciences, through natural sciences to engineering. The broadapplication of ML and the accelerated pace of its evolution lead to anincreasing need for dedicated teaching concepts aimed at making the applicationof this technology more reliable and responsible. However, teaching ML is adaunting task. Aside from the methodological complexity of ML algorithms, bothwith respect to theory and implementation, the interdisciplinary and empiricalnature of the field need to be taken into consideration. This paper introducesthe MachineLearnAthon format, an innovative didactic concept designed to beinclusive for students of different disciplines with heterogeneous levels ofmathematics, programming and domain expertise. At the heart of the concept lieML challenges, which make use of industrial data sets to solve real-worldproblems. These cover the entire ML pipeline, promoting data literacy andpractical skills, from data preparation, through deployment, to evaluation."
    },
    {
        "link": "https://arxiv.org/abs/2401.16292",
        "title": "Pilotfish: Distributed Transaction Execution for Lazy Blockchains",
        "authors": [
            "Quentin Kniep",
            "Lefteris Kokoris-Kogias",
            "Alberto Sonnino",
            "Igor Zablotchi",
            "Nuda Zhang"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Pilotfish is the first scale-out blockchain execution engine able to harnessany degree of parallelizability existing in its workload. Pilotfish allows eachvalidator to employ multiple machines, named ExecutionWorkers, under itscontrol to scale its execution layer. Given a sufficiently parallelizable andcompute-intensive load, the number of transactions that the validator canexecute increases linearly with the number of ExecutionWorkers at its disposal.In addition, Pilotfish maintains the consistency of the state, even when manyvalidators experience simultaneous machine failures. This is possible due tothe meticulous co-design of our crash-recovery protocol which leverages theexisting fault tolerance in the blockchain's consensus mechanism.Finally, Pilotfish can also be seen as the first distributed deterministicexecution engine that provides support for dynamic reads as transactions arenot required to provide a fully accurate read and write set. This loosening ofrequirements would normally reduce the parallelizability available by blockingwrite-after-write conflicts, but our novel versioned-queues schedulingalgorithm circumvents this by exploiting the lazy recovery property ofPilotfish, which only persists consistent state and re-executes any optimisticsteps taken before the crash.In order to prove our claims we implemented the common path of Pilotfish withsupport for the MoveVM and evaluated it against the parallel execution MoveVMof Sui. Our results show that our simpler scheduling algorithms outperforms Suieven with a single execution worker, but more importantly provides linearscalability up to 4 ExecutionWorkers even for simple asset-transfers and to anynumber of ExecutionWorkers for more computationally heavy workloads."
    },
    {
        "link": "https://arxiv.org/abs/2401.16293",
        "title": "Textual Entailment for Effective Triple Validation in Object Prediction",
        "authors": [
            "Andr\u00e9s Garc\u00eda-Silva",
            "Cristian Berr\u00edo",
            "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Knowledge base population seeks to expand knowledge graphs with facts thatare typically extracted from a text corpus. Recently, language modelspretrained on large corpora have been shown to contain factual knowledge thatcan be retrieved using cloze-style strategies. Such approach enables zero-shotrecall of facts, showing competitive results in object prediction compared tosupervised baselines. However, prompt-based fact retrieval can be brittle andheavily depend on the prompts and context used, which may produce results thatare unintended or hallucinatory.We propose to use textual entailment tovalidate facts extracted from language models through cloze statements. Ourresults show that triple validation based on textual entailment improveslanguage model predictions in different training regimes. Furthermore, we showthat entailment-based triple validation is also effective to validate candidatefacts extracted from other sources including existing knowledge graphs and textpassages where named entities are recognized."
    },
    {
        "link": "https://arxiv.org/abs/2401.16294",
        "title": "Dual feature-based and example-based explanation methods",
        "authors": [
            "Andrei V. Konstantinov",
            "Boris V. Kozlov",
            "Stanislav R. Kirpichenko",
            "Lev V. Utkin"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "A new approach to the local and global explanation is proposed. It is basedon selecting a convex hull constructed for the finite number of points aroundan explained instance. The convex hull allows us to consider a dualrepresentation of instances in the form of convex combinations of extremepoints of a produced polytope. Instead of perturbing new instances in theEuclidean feature space, vectors of convex combination coefficients areuniformly generated from the unit simplex, and they form a new dual dataset. Adual linear surrogate model is trained on the dual dataset. The explanationfeature importance values are computed by means of simple matrix calculations.The approach can be regarded as a modification of the well-known model LIME.The dual representation inherently allows us to get the example-basedexplanation. The neural additive model is also considered as a tool forimplementing the example-based explanation approach. Many numerical experimentswith real datasets are performed for studying the approach. The code ofproposed algorithms is available."
    },
    {
        "link": "https://arxiv.org/abs/2401.16296",
        "title": "On the Complexity of Establishing Hereditary Graph Properties via Vertex Splitting",
        "authors": [
            "Alexander Firbas",
            "Manuel Sorge"
        ],
        "primary_subject": "Computational Complexity (cs.CC)",
        "abstract": "Vertex splitting is a graph operation that replaces a vertex v with twononadjacent new vertices and makes each neighbor of v adjacent with one orboth of the introduced vertices. Vertex splitting has been used in contextsfrom circuit design to statistical analysis. In this work, we explore thecomputational complexity of achieving a given graph property \u03a0 by a limitednumber of vertex splits, formalized as the problem \u03a0 Vertex Splitting(\u03a0-VS). We focus on hereditary graph properties and contribute four groupsof results: First, we classify the classical complexity of \u03a0-VS for graphproperties characterized by forbidden subgraphs of size at most 3. Second, weprovide a framework that allows to show NP-completeness whenever one canconstruct a combination of a forbidden subgraph and prescribed vertex splitsthat satisfy certain conditions. Leveraging this framework we showNP-completeness when \u03a0 is characterized by forbidden subgraphs that aresufficiently well connected. In particular, we show that F-Free-VS isNP-complete for each biconnected graph F. Third, we study infinite familiesof forbidden subgraphs, obtaining NP-hardness for Bipartite-VS and Perfect-VS.Finally, we touch upon the parameterized complexity of \u03a0-VS with respect tothe number of allowed splits, showing para-NP-hardness for K3-Free-VS andderiving an XP-algorithm when each vertex is only allowed to be split at mostonce."
    },
    {
        "link": "https://arxiv.org/abs/2401.16298",
        "title": "Breaking the Barrier: Selective Uncertainty-based Active Learning for Medical Image Segmentation",
        "authors": [
            "Siteng Ma",
            "Haochang Wu",
            "Aonghus Lawlor",
            "Ruihai Dong"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Active learning (AL) has found wide applications in medical imagesegmentation, aiming to alleviate the annotation workload and enhanceperformance. Conventional uncertainty-based AL methods, such as entropy andBayesian, often rely on an aggregate of all pixel-level metrics. However, inimbalanced settings, these methods tend to neglect the significance of targetregions, eg., lesions, and tumors. Moreover, uncertainty-based selectionintroduces redundancy. These factors lead to unsatisfactory performance, and inmany cases, even underperform random sampling. To solve this problem, weintroduce a novel approach called the Selective Uncertainty-based AL, avoidingthe conventional practice of summing up the metrics of all pixels. Through afiltering process, our strategy prioritizes pixels within target areas andthose near decision boundaries. This resolves the aforementioned disregard fortarget areas and redundancy. Our method showed substantial improvements acrossfive different uncertainty-based methods and two distinct datasets, utilizingfewer labeled data to reach the supervised baseline and consistently achievingthe highest overall performance. Our code is available athttps://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL."
    },
    {
        "link": "https://arxiv.org/abs/2401.16299",
        "title": "Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation",
        "authors": [
            "Vishal Dey",
            "Xia Ning"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Pretrained Graph Neural Networks have been widely adopted for variousmolecular property prediction tasks. Despite their ability to encode structuraland relational features of molecules, traditional fine-tuning of suchpretrained GNNs on the target task can lead to poor generalization. To addressthis, we explore the adaptation of pretrained GNNs to the target task byjointly training them with multiple auxiliary tasks. This could enable the GNNsto learn both general and task-specific features, which may benefit the targettask. However, a major challenge is to determine the relatedness of auxiliarytasks with the target task. To address this, we investigate multiple strategiesto measure the relevance of auxiliary tasks and integrate such tasks byadaptively combining task gradients or by learning task weights via bi-leveloptimization. Additionally, we propose a novel gradient surgery-based approach,Rotation of Conflicting Gradients (RCGrad), that learns to alignconflicting auxiliary task gradients through rotation. Our experiments withstate-of-the-art pretrained GNNs demonstrate the efficacy of our proposedmethods, with improvements of up to 7.7% over fine-tuning. This suggests thatincorporating auxiliary tasks along with target task fine-tuning can be aneffective way to improve the generalizability of pretrained GNNs for molecularproperty prediction."
    },
    {
        "link": "https://arxiv.org/abs/2401.16301",
        "title": "Scalable Factor Graph-Based Heterogeneous Bayesian DDF for Dynamic Systems",
        "authors": [
            "Ofer Dagan",
            "Tycho L. Cinquini",
            "Nisar R. Ahmed"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Heterogeneous Bayesian decentralized data fusion captures the set of problemsin which two robots must combine two probability density functions overnon-equal, but overlapping sets of random variables. In the context ofmulti-robot dynamic systems, this enables robots to take a \"divide and conquer\"approach to reason and share data over complementary tasks instead of over thefull joint state space. For example, in a target tracking application, thisallows robots to track different subsets of targets and share data on onlycommon targets. This paper presents a framework by which robots can each use alocal factor graph to represent relevant partitions of a complex global jointprobability distribution, thus allowing them to avoid reasoning over theentirety of a more complex model and saving communication as well ascomputation costs. From a theoretical point of view, this paper makescontributions by casting the heterogeneous decentralized fusion problem interms of a factor graph, analyzing the challenges that arise due to dynamicfiltering, and then developing a new conservative filtering algorithm thatensures statistical correctness. From a practical point of view, we show howthis framework can be used to represent different multi-robot applications andthen test it with simulations and hardware experiments to validate anddemonstrate its statistical conservativeness, applicability, and robustness toreal-world challenges."
    },
    {
        "link": "https://arxiv.org/abs/2401.16302",
        "title": "Quantum-safe Encryption: A New Method to Reduce Complexity and/or Improve Security Level",
        "authors": [
            "Amir K. Khandani"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "This work presents some novel techniques to enhance an encryption schememotivated by classical McEliece cryptosystem. Contributions include: (1) usingmasking matrices to hide sensitive data, (2) allowing both legitimate partiesto incorporate randomness in the public key without sharing any additionalpublic information, (3) using concatenation of a repetition code for errorcorrection, permitting key recovery with a negligible decoding complexity, (4)making attacks more difficult by increasing the complexity in verifying a givenkey candidate has resulted in the actual key, (5) introducing memory in theerror sequence such that: (i) error vector is composed of a random number oferroneous bits, (ii) errors can be all corrected when used in conjunction withconcatenation of a repetition code of length 3. Proposed techniques allowgenerating significantly larger keys, at the same time, with a much lowercomplexity, as compared to known post-quantum key generation techniques relyingon randomization."
    },
    {
        "link": "https://arxiv.org/abs/2401.16304",
        "title": "Regressing Transformers for Data-efficient Visual Place Recognition",
        "authors": [
            "Mar\u00eda Leyva-Vallina",
            "Nicola Strisciuglio",
            "Nicolai Petkov"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual place recognition is a critical task in computer vision, especiallyfor localization and navigation systems. Existing methods often rely oncontrastive learning: image descriptors are trained to have small distance forsimilar images and larger distance for dissimilar ones in a latent space.However, this approach struggles to ensure accurate distance-based imagesimilarity representation, particularly when training with binary pairwiselabels, and complex re-ranking strategies are required. This work introduces afresh perspective by framing place recognition as a regression problem, usingcamera field-of-view overlap as similarity ground truth for learning. Byoptimizing image descriptors to align directly with graded similarity labels,this approach enhances ranking capabilities without expensive re-ranking,offering data-efficient training and strong generalization across severalbenchmark datasets."
    },
    {
        "link": "https://arxiv.org/abs/2401.16305",
        "title": "MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection",
        "authors": [
            "Yuxue Yang",
            "Lue Fan",
            "Zhaoxiang Zhang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Label-efficient LiDAR-based 3D object detection is currently dominated byweakly/semi-supervised methods. Instead of exclusively following one of them,we propose MixSup, a more practical paradigm simultaneously utilizing massivecheap coarse labels and a limited number of accurate labels for Mixed-grainedSupervision. We start by observing that point clouds are usually textureless,making it hard to learn semantics. However, point clouds are geometrically richand scale-invariant to the distances from sensors, making it relatively easy tolearn the geometry of objects, such as poses and shapes. Thus, MixSup leveragesmassive coarse cluster-level labels to learn semantics and a few expensivebox-level labels to learn accurate poses and shapes. We redesign the labelassignment in mainstream detectors, which allows them seamlessly integratedinto MixSup, enabling practicality and universality. We validate itseffectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing variousdetectors. MixSup achieves up to 97.31% of fully supervised performance, usingcheap cluster annotations and only 10% box annotations. Furthermore, we proposePointSAM based on the Segment Anything Model for automated coarse labeling,further reducing the annotation burden. The code is available athttps://github.com/BraveGroup/PointSAM-for-MixSup."
    },
    {
        "link": "https://arxiv.org/abs/2401.16307",
        "title": "Momentary Stressor Logging and Reflective Visualizations: Implications for Stress Management with Wearables",
        "authors": [
            "Sameer Neupane",
            "Mithun Saha",
            "Nasir Ali",
            "Timothy Hnat",
            "Shahin Alan Samiei",
            "Anandatirtha Nandugudi",
            "David M. Almeida",
            "Santosh Kumar"
        ],
        "primary_subject": "Human-Computer Interaction (cs.HC)",
        "abstract": "Commercial wearables from Fitbit, Garmin, and Whoop have recently introducedreal-time notifications based on detecting changes in physiological responsesindicating potential stress. In this paper, we investigate how these newcapabilities can be leveraged to improve stress management. We developed asmartwatch app, a smartphone app, and a cloud service, and conducted a 100-dayfield study with 122 participants who received prompts triggered byphysiological responses several times a day. They were asked whether they werestressed, and if so, to log the most likely stressor. Each week, participantsreceived new visualizations of their data to self-reflect on patterns andtrends. Participants reported better awareness of their stressors, andself-initiating fourteen kinds of behavioral changes to reduce stress in theirdaily lives. Repeated self-reports over 14 weeks showed reductions in bothstress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057weekly surveys)."
    },
    {
        "link": "https://arxiv.org/abs/2401.16310",
        "title": "Security Code Review by LLMs: A Deep Dive into Responses",
        "authors": [
            "Jiaxin Yu",
            "Peng Liang",
            "Yujia Fu",
            "Amjed Tahir",
            "Mojtaba Shahin",
            "Chong Wang",
            "Yangxiao Cai"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Security code review aims to combine automated tools and manual efforts todetect security defects during development. The rapid development of LargeLanguage Models (LLMs) has shown promising potential in software development,as well as opening up new possibilities in automated security code review. Toexplore the challenges of applying LLMs in practical code review for securitydefect detection, this study compared the detection performance of threestate-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on549 code files that contain security defects from real-world code reviews.Through analyzing 82 responses generated by the best-performing LLM-promptcombination based on 100 randomly selected code files, we extracted andcategorized quality problems present in these responses into 5 themes and 16categories. Our results indicate that the responses produced by LLMs oftensuffer from verbosity, vagueness, and incompleteness, highlighting thenecessity to enhance their conciseness, understandability, and compliance tosecurity defect detection. This work reveals the deficiencies of LLM-generatedresponses in security code review and paves the way for future optimization ofLLMs towards this task."
    },
    {
        "link": "https://arxiv.org/abs/2401.16312",
        "title": "Degradability of Modified Landau-Streater Type Low-Noise Quantum Channels in High Dimensions",
        "authors": [
            "Yun-Feng Lo",
            "Yen-Chi Lee",
            "Min-Hsiu Hsieh"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "This paper delves into the degradability of quantum channels, with a specificfocus on high-dimensional extensions of qubit depolarizing channels inlow-noise regimes. We build upon the foundation of \u03b7-approximatedegradable channels, as established by Sutter et al. and Leditzky et al., tointroduce and examine the Modified Landau-Streater (MLS) channels. Thesechannels expand upon the qubit depolarizing and the recently proposed modifiedWerner-Holevo channels by Roofeh and Karimipour, extending them tohigher-dimensional Hilbert spaces (with dimension d=2j+1, where j arepositive half-integers). Our investigation centers on their conformity to theO(\u03b52) degradability pattern, aligning with and extending Leditzkyet al.'s findings in the d=2 case. By replacing the SU(2) generators withSU(d) in our treatment, we may explore the potential inclusion of generalizedGell-Mann matrices in future research. Our results enhance the understanding ofsuper-additivity in quantum channels within the low-noise regime and lay thegroundwork for future explorations into conditions and structures that couldlead to O(\u03b52) degradability across a broader spectrum of quantumchannels."
    },
    {
        "link": "https://arxiv.org/abs/2401.16313",
        "title": "Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets",
        "authors": [
            "Nikita Moghe",
            "Arnisa Fazla",
            "Chantal Amrhein",
            "Tom Kocmi",
            "Mark Steedman",
            "Alexandra Birch",
            "Rico Sennrich",
            "Liane Guillou"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Recent machine translation (MT) metrics calibrate their effectiveness bycorrelating with human judgement but without any insights about their behaviouracross different error types. Challenge sets are used to probe specificdimensions of metric behaviour but there are very few such datasets and theyeither focus on a limited number of phenomena or a limited number of languagepairs. We introduce ACES, a contrastive challenge set spanning 146 languagepairs, aimed at discovering whether metrics can identify 68 translationaccuracy errors. These phenomena range from simple alterations at theword/character level to more complex errors based on discourse and real-worldknowledge. We conduct a large-scale study by benchmarking ACES on 50 metricssubmitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metricperformance, assess their incremental performance over successive campaigns,and measure their sensitivity to a range of linguistic phenomena. We alsoinvestigate claims that Large Language Models (LLMs) are effective as MTevaluators by evaluating on ACES. Our results demonstrate that different metricfamilies struggle with different phenomena and that LLM-based methods fail todemonstrate reliable performance. Our analyses indicate that most metricsignore the source sentence, tend to prefer surface-level overlap and end upincorporating properties of base models which are not always beneficial. Weexpand ACES to include error span annotations, denoted as SPAN-ACES and we usethis dataset to evaluate span-based error metrics showing these metrics alsoneed considerable improvement. Finally, we provide a set of recommendations forbuilding better MT metrics, including focusing on error labels instead ofscores, ensembling, designing strategies to explicitly focus on the sourcesentence, focusing on semantic content and choosing the right base model forrepresentations."
    },
    {
        "link": "https://arxiv.org/abs/2401.16314",
        "title": "Creative Telescoping for Hypergeometric Double Sums",
        "authors": [
            "Peter Paule",
            "Carsten Schneider"
        ],
        "primary_subject": "Symbolic Computation (cs.SC)",
        "abstract": "We present efficient methods for calculating linear recurrences ofhypergeometric double sums and, more generally, of multiple sums. Inparticular, we supplement this approach with the algorithmic theory ofcontiguous relations, which guarantees the applicability of our method for manyinput sums. In addition, we elaborate new techniques to optimize the underlyingkey task of our method to compute rational solutions of parameterized linearrecurrences."
    },
    {
        "link": "https://arxiv.org/abs/2401.16316",
        "title": "Convergence Analysis of a Preconditioned Steepest Descent Solver for the Cahn-Hilliard Equation with Logarithmic Potential",
        "authors": [
            "Amanda E. Diegel",
            "Cheng Wang",
            "Steven M. Wise"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we provide a theoretical analysis for a preconditionedsteepest descent (PSD) iterative solver that improves the computational time ofa finite difference numerical scheme for the Cahn-Hilliard equation withFlory-Huggins energy potential. In the numerical design, a convex splittingapproach is applied to the chemical potential such that the logarithmic and thesurface diffusion terms are treated implicitly while the expansive concave termis treated with an explicit update. The nonlinear and singular nature of thelogarithmic energy potential makes the numerical implementation verychallenging. However, the positivity-preserving property for the logarithmicarguments, unconditional energy stability, and optimal rate error estimateshave been established in a recent work and it has been shown that successfulsolvers ensure a similar positivity-preserving property at each iterationstage. Therefore, in this work, we will show that the PSD solver ensures apositivity-preserving property at each iteration stage. The PSD solver consistsof first computing a search direction (involved with solving a Poisson-likeequation) and then takes a one-parameter optimization step over the searchdirection in which the Newton iteration becomes very powerful. A theoreticalanalysis is applied to the PSD iteration solver and a geometric convergencerate is proved for the iteration. In particular, the strict separation propertyof the numerical solution, which indicates a uniform distance between thenumerical solution and the singular limit values of \u00b11 for the phasevariable, plays an essential role in the iteration convergence analysis. A fewnumerical results are presented to demonstrate the robustness and efficiency ofthe PSD solver."
    },
    {
        "link": "https://arxiv.org/abs/2401.16318",
        "title": "Defining and Extracting generalizable interaction primitives from DNNs",
        "authors": [
            "Lu Chen",
            "Siyu Lou",
            "Benhao Huang",
            "Quanshi Zhang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Faithfully summarizing the knowledge encoded by a deep neural network (DNN)into a few symbolic primitive patterns without losing much informationrepresents a core challenge in explainable AI. To this end, Ren et al. (2023c)have derived a series of theorems to prove that the inference score of a DNNcan be explained as a small set of interactions between input variables.However, the lack of generalization power makes it still hard to consider suchinteractions as faithful primitive patterns encoded by the DNN. Therefore,given different DNNs trained for the same task, we develop a new method toextract interactions that are shared by these DNNs. Experiments show that theextracted interactions can better reflect common knowledge shared by differentDNNs."
    },
    {
        "link": "https://arxiv.org/abs/2401.16321",
        "title": "Optimal Control of Renewable Energy Communities subject to Network Peak Fees with Model Predictive Control and Reinforcement Learning Algorithms",
        "authors": [
            "Samy Aittahar",
            "Adrien Bolland",
            "Guillaume Derval",
            "Damien Ernst"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "We propose in this paper an optimal control framework for renewable energycommunities (RECs) equipped with controllable assets. Such RECs allow itsmembers to exchange production surplus through an internal market. Theobjective is to control their assets in order to minimise the sum of individualelectricity bills. These bills account for the electricity exchanged throughthe REC and with the retailers. Typically, for large companies, anotherimportant part of the bills are the costs related to the power peaks; in ourframework, they are determined from the energy exchanges with the retailers. Wecompare rule-based control strategies with the two following controlalgorithms. The first one is derived from model predictive control techniques,and the second one is built with reinforcement learning techniques. We alsocompare variants of these algorithms that neglect the peak power costs. Resultsconfirm that using policies accounting for the power peaks lead to asignificantly lower sum of electricity bills and thus better control strategiesat the cost of higher computation time. Furthermore, policies trained withreinforcement learning approaches appear promising for real-time control of thecommunities, where model predictive control policies may be computationallyexpensive in practice. These findings encourage pursuing the efforts towarddevelopment of scalable control algorithms, operating from a centralisedstandpoint, for renewable energy communities equipped with controllable assets."
    },
    {
        "link": "https://arxiv.org/abs/2401.16322",
        "title": "High-order exponential integration for seismic wave modeling",
        "authors": [
            "Fernando V. Ravelo",
            "Martin Schreiber",
            "Pedro S. Peixoto"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Seismic imaging is a major challenge in geophysics with broad applications.It involves solving wave propagation equations with absorbing boundaryconditions (ABC) multiple times. This drives the need for accurate andefficient numerical methods. This study examines a collection of exponentialintegration methods, known for their good numerical properties on waverepresentation, to investigate their efficacy in solving the wave equation withABC. The purpose of this research is to assess the performance of thesemethods. We compare a recently proposed Exponential Integration based on Faberpolynomials with well-established Krylov exponential methods alongside ahigh-order Runge-Kutta scheme and low-order classical methods. Through ouranalysis, we found that the exponential integrator based on the Krylov subspaceexhibits the best convergence results among the high-order methods. We alsodiscovered that high-order methods can achieve computational efficiency similarto lower-order methods while allowing for considerably larger time steps. Mostimportantly, the possibility of undertaking large time steps could be used forimportant memory savings in full waveform inversion imaging problems."
    },
    {
        "link": "https://arxiv.org/abs/2401.16327",
        "title": "PICL: Physics Informed Contrastive Learning for Partial Differential Equations",
        "authors": [
            "Cooper Lorsung",
            "Amir Barati Farimani"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Neural operators have recently grown in popularity as Partial DifferentialEquation (PDEs) surrogate models. Learning solution functionals, rather thanfunctions, has proven to be a powerful approach to calculate fast, accuratesolutions to complex PDEs. While much work has been done evaluating neuraloperator performance on a wide variety of surrogate modeling tasks, these worksnormally evaluate performance on a single equation at a time. In this work, wedevelop a novel contrastive pretraining framework utilizing GeneralizedContrastive Loss that improves neural operator generalization across multiplegoverning equations simultaneously. Governing equation coefficients are used tomeasure ground-truth similarity between systems. A combination ofphysics-informed system evolution and latent-space model output are anchored toinput data and used in our distance function. We find that physics-informedcontrastive pretraining improves both accuracy and generalization for theFourier Neural Operator in fixed-future task, with comparable performance onthe autoregressive rollout, and superresolution tasks for the 1D Heat,Burgers', and linear advection equations."
    },
    {
        "link": "https://arxiv.org/abs/2401.16329",
        "title": "Synthesis of 3D on-air signatures with the Sigma-Lognormal model",
        "authors": [
            "Miguel A. Ferrer",
            "Moises Diaz",
            "Cristina Carmona-Duarte",
            "Jose J. Quintana Hernandez",
            "Rejean Plamondon"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Signature synthesis is a computation technique that generates artificialspecimens which can support decision making in automatic signatureverification. A lot of work has been dedicated to this subject, which centreson synthesizing dynamic and static two-dimensional handwriting on canvas. Thispaper proposes a framework to generate synthetic 3D on-air signaturesexploiting the lognormality principle, which mimics the complex neuromotorcontrol processes at play as the fingertip moves. Addressing the usual casesinvolving the development of artificial individuals and duplicated samples,this paper contributes to the synthesis of: (1) the trajectory and velocity ofentirely 3D new signatures; (2) kinematic information when only the 3Dtrajectory of the signature is known, and (3) duplicate samples of 3D realsignatures. Validation was conducted by generating synthetic 3D signaturedatabases mimicking real ones and showing that automatic signatureverifications of genuine and skilled forgeries report performances similar tothose of real and synthetic databases. We also observed that training 3Dautomatic signature verifiers with duplicates can reduce errors. We furtherdemonstrated that our proposal is also valid for synthesizing 3D air writingand gestures. Finally, a perception test confirmed the human likeness of thegenerated specimens. The databases generated are publicly available, only forresearch purposes, at ."
    },
    {
        "link": "https://arxiv.org/abs/2401.16330",
        "title": "Digital requirements engineering with an INCOSE-derived SysML meta-model",
        "authors": [
            "James S. Wheaton",
            "Daniel R. Herber"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Traditional requirements engineering tools do not readily access the systemarchitecture model defined in SysML and related Profiles, often resulting induplication of basic system model elements that nevertheless lack theconnectivity and expressive detail possible in a SysML-defined model. Withoutarchitecture model connectivity, requirements can suffer from imprecision andinconsistent terminology, and thereby frustrate communication during the systemdevelopment lifecycle. Further integration of requirements engineeringactivities with system architecture modeling contributes to the AuthoritativeSource of Truth while facilitating deep access to system architecture modelelements for V&V activities. The Model-Based Structured Requirement SysMLProfile was extended to comply with INCOSE Guide for Writing Requirements andNeeds & Requirements Manual updated in 2023 while conforming to theISO/IEC/IEEE 29148 standard requirement statement templates. Rules,Characteristics, and Attributes were defined according to the Guide tofacilitate definition and requirements V&V. The resulting SysML Profile wasapplied in two system architecture models at NASA Jet Propulsion Laboratoryallowing us to explore its applicability and value in a real-world projectenvironment. Initial results show that INCOSE-derived Model-Based StructuredRequirements complement the NASA Systems Engineering Handbook checklist andguidance, but Attribute consistency can be difficult to achieve with the systemarchitecture modeling software in use."
    },
    {
        "link": "https://arxiv.org/abs/2401.16332",
        "title": "Tradeoffs Between Alignment and Helpfulness in Language Models",
        "authors": [
            "Yotam Wolf",
            "Noam Wies",
            "Dorin Shteyman",
            "Binyamin Rothberg",
            "Yoav Levine",
            "Amnon Shashua"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Language model alignment has become an important component of AI safety,allowing safe interactions between humans and language models, by enhancingdesired behaviors and inhibiting undesired ones. It is often done by tuning themodel or inserting preset aligning prompts. Recently, representationengineering, a method which alters the model's behavior via changing itsrepresentations post-training, was shown to be effective in aligning LLMs (Zouet al., 2023a). Representation engineering yields gains in alignment orientedtasks such as resistance to adversarial attacks and reduction of social biases,but was also shown to cause a decrease in the ability of the model to performbasic tasks. In this paper we study the tradeoff between the increase inalignment and decrease in helpfulness of the model. We propose a theoreticalframework which provides bounds for these two quantities, and demonstrate theirrelevance empirically. Interestingly, we find that while the helpfulnessgenerally decreases, it does so quadratically with the norm of therepresentation engineering vector, while the alignment increases linearly withit, indicating a regime in which it is efficient to use representationengineering. We validate our findings empirically, and chart the boundaries tothe usefulness of representation engineering for alignment."
    },
    {
        "link": "https://arxiv.org/abs/2401.16335",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF",
        "authors": [
            "Banghua Zhu",
            "Michael I. Jordan",
            "Jiantao Jiao"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique thataligns language models closely with human-centric values. The initial phase ofRLHF involves learning human values using a reward model from ranking data. Itis observed that the performance of the reward model degrades after one epochof training, and optimizing too much against the learned reward modeleventually hinders the true objective. This paper delves into these issues,leveraging the theoretical insights to design improved reward learningalgorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that duringeach training epoch, we not only update the model with the data, but alsoupdate the date using the model, replacing hard labels with soft labels. Ourempirical findings highlight the superior performance of this approach over thetraditional methods."
    },
    {
        "link": "https://arxiv.org/abs/2401.16337",
        "title": "Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-free Design",
        "authors": [
            "Vassil Atanassov",
            "Jiatao Ding",
            "Jens Kober",
            "Ioannis Havoutis",
            "Cosimo Della Santina"
        ],
        "primary_subject": "Robotics (cs.RO)",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising solution tomastering explosive and versatile quadrupedal jumping skills. However, currentDRL-based frameworks usually rely on well-defined reference trajectories, whichare obtained by capturing animal motions or transferring experience fromexisting controllers. This work explores the possibility of learning dynamicjumping without imitating a reference trajectory. To this end, we incorporate acurriculum design into DRL so as to accomplish challenging tasks progressively.Starting from a vertical in-place jump, we then generalize the learned policyto forward and diagonal jumps and, finally, learn to jump across obstacles.Conditioned on the desired landing location, orientation, and obstacledimensions, the proposed approach contributes to a wide range of jumpingmotions, including omnidirectional jumping and robust jumping, alleviating theeffort to extract references in advance. Particularly, without constraints fromthe reference motion, a 90cm forward jump is achieved, exceeding previousrecords for similar robots reported in the existing literature. Additionally,continuous jumping on the soft grassy floor is accomplished, even when it isnot encountered in the training stage. A supplementary video showing ourresults can be found at https://youtu.be/nRaMCrwU5X8 ."
    },
    {
        "link": "https://arxiv.org/abs/2401.16339",
        "title": "SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing",
        "authors": [
            "Badr-Eddine Boudriki Semlali",
            "Chaker El Amrani",
            "Guadalupe Ortiz",
            "Juan Boubeta-Puig",
            "Alfonso Garcia-de-Prado"
        ],
        "primary_subject": "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Air pollution is a major problem today that causes serious damage to humanhealth. Urban areas are the most affected by the degradation of air qualitycaused by anthropogenic gas emissions. Although there are multiple proposalsfor air quality monitoring, in most cases, two limitations are imposed: theimpossibility of processing data in Near Real-Time (NRT) for remote sensingapproaches and the impossibility of reaching areas of limited accessibility orlow network coverage for ground data approaches. We propose a softwarearchitecture that efficiently combines complex event processing with remotesensing data from various satellite sensors to monitor air quality in NRT,giving support to decision-makers. We illustrate the proposed solution bycalculating the air quality levels for several areas of Morocco and Spain,extracting and processing satellite information in NRT. This study alsovalidates the air quality measured by ground stations and satellite sensordata."
    },
    {
        "link": "https://arxiv.org/abs/2401.16340",
        "title": "The role of library versions in Developer-ChatGPT conversations",
        "authors": [
            "Rachna Raj",
            "Diego Elias Costa"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "The latest breakthroughs in large language models (LLM) have empoweredsoftware development tools, such as ChatGPT, to aid developers in complextasks. Developers use ChatGPT to write code, review code changes, and evendebug their programs. In these interactions, ChatGPT often recommends codesnippets that depend on external libraries. However, code from librarieschanges over time, invalidating a once-correct code snippet and making itdifficult to reuse recommended code.In this study, we analyze DevGPT, a dataset of more than 4,000Developer-ChatGPT interactions, to understand the role of library versions incode-related conversations. We quantify how often library version constraintsare mentioned in code-related conversations and when ChatGPT recommends theinstallation of specific libraries. Our findings show that, albeit toconstantly recommend and analyze code with external dependencies, libraryversion constraints only appear in 9% of the conversations. In the majority ofconversations, the version constraints are prompted by users (as opposed tobeing specified by ChatGPT) as a method for receiving better quality responses.Moreover, we study how library version constraints are used in the conversationthrough qualitative methods, identifying several potential problems thatwarrant further research."
    },
    {
        "link": "https://arxiv.org/abs/2401.16341",
        "title": "S-HIDRA: A blockchain and SDN domain-based architecture to orchestrate fog computing environments",
        "authors": [
            "carlos N\u00fa\u00f1ez-G\u00f3mez",
            "Carmen Carri\u00f3n",
            "Blanca Caminero",
            "Francisco M. Delicado"
        ],
        "primary_subject": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Fog computing arises as a complement to cloud computing where computing andstorage are provided in a decentralized way rather than the centralizedapproach of the cloud paradigm. In addition, blockchain provides adecentralized and immutable ledger which can provide support for runningarbitrary logic thanks to smart contracts. These facts can lead to harnesssmart contracts on blockchain as the basis for a decentralized, autonomous, andresilient orchestrator for the resources in the fog. However, the potentiallyvast amount of geographically distributed fog nodes may threaten thefeasibility of the orchestration. On the other hand, fog nodes can exhibithighly dynamic workloads which may result in the orchestrator redistributingthe services among them. Thus, there is also a need to dynamically support thenetwork connections to those services independently of their location. SoftwareDefined Networking (SDN) can be integrated within the orchestrator to carry outa seamless service management. To tackle both aforementioned issues, theS-HIDRA architecture is proposed. It integrates SDN support within ablockchain-based orchestrator of container-based services for fog environments,in order to provide low network latency and high service availability. Also, adomain-based architecture is outlined \\marev{as potential scenario} to addressthe geographic distributed nature of fog environments. Results obtained from aproof-of-concept implementation assess the required functionality for S-HIDRA."
    },
    {
        "link": "https://arxiv.org/abs/2401.16342",
        "title": "On Achievable Rates for the Shotgun Sequencing Channel with Erasures",
        "authors": [
            "Hrishi Narayanan",
            "Prasad Krishnan",
            "Nita Parekh"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "In the shotgun sequencing channel, the input sequence (possibly, a long DNAsequence composed of nucleotide bases) is read into multiple fragments (called`reads') of much shorter lengths. In the context of DNA data storage, thecapacity of this channel was identified in a recent work, assuming that thereads themselves are noiseless substrings of the original sequence. Modernshotgun sequencers however also output quality scores for each base read,indicating the confidence in its identification. Bases with low quality scorescan be considered to be erased. Motivated by this, we consider the shotgunsequencing channel with erasures, where each symbol in any read can beindependently erased with some probability \u03b4. We identify achievablerates for this channel, using a random code construction and a decoder thatuses typicality-like arguments to merge the reads."
    },
    {
        "link": "https://arxiv.org/abs/2401.16347",
        "title": "Cross-Modal Coordination Across a Diverse Set of Input Modalities",
        "authors": [
            "Jorge S\u00e1nchez",
            "Rodrigo Laguna"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cross-modal retrieval is the task of retrieving samples of a given modalityby using queries of a different one. Due to the wide range of practicalapplications, the problem has been mainly focused on the vision and languagecase, e.g. text to image retrieval, where models like CLIP have proveneffective in solving such tasks. The dominant approach to learning suchcoordinated representations consists of projecting them onto a common spacewhere matching views stay close and those from non-matching pairs are pushedaway from each other. Although this cross-modal coordination has been appliedalso to other pairwise combinations, extending it to an arbitrary number ofdiverse modalities is a problem that has not been fully explored in theliterature. In this paper, we propose two different approaches to the problem.The first is based on an extension of the CLIP contrastive objective to anarbitrary number of input modalities, while the second departs from thecontrastive formulation and tackles the coordination problem by regressing thecross-modal similarities towards a target that reflects two simple andintuitive constraints of the cross-modal retrieval task. We run experiments ontwo different datasets, over different combinations of input modalities andshow that the approach is not only simple and effective but also allows fortackling the retrieval problem in novel ways. Besides capturing a more diverseset of pair-wise interactions, we show that we can use the learnedrepresentations to improve retrieval performance by combining the embeddingsfrom two or more such modalities."
    },
    {
        "link": "https://arxiv.org/abs/2401.16348",
        "title": "Beyond Automated Evaluation Metrics: Evaluating Topic Models On Practical Social Science Content Analysis Tasks",
        "authors": [
            "Zongxia Li",
            "Andrew Mao",
            "Daniel Stephens",
            "Pranav Goel",
            "Emily Walpole",
            "Alden Dima",
            "Juan Fung",
            "Jordan Boyd-Graber"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Topic models are a popular tool for understanding text collections, but theirevaluation has been a point of contention. Automated evaluation metrics such ascoherence are often used, however, their validity has been questioned forneural topic models (NTMs) and can overlook the benefits of a model in realworld applications. To this end, we conduct the first evaluation of neural,supervised and classical topic models in an interactive task based setting. Wecombine topic models with a classifier and test their ability to help humansconduct content analysis and document annotation. From simulated, real user andexpert pilot studies, the Contextual Neural Topic Model does the best oncluster evaluation metrics and human evaluations; however, LDA is competitivewith two other NTMs under our simulated experiment and user study results,contrary to what coherence scores suggest. We show that current automatedmetrics do not provide a complete picture of topic modeling capabilities, butthe right choice of NTMs can be better than classical models on practicaltasks."
    },
    {
        "link": "https://arxiv.org/abs/2401.16349",
        "title": "ConFit: Improving Resume-Job Matching using Data Augmentation and Contrastive Learning",
        "authors": [
            "Xiao Yu",
            "Jinzhong Zhang",
            "Zhou Yu"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "A reliable resume-job matching system helps a company find suitablecandidates from a pool of resumes, and helps a job seeker find relevant jobsfrom a list of job posts. However, since job seekers apply only to a few jobs,interaction records in resume-job datasets are sparse. Different from manyprior work that use complex modeling techniques, we tackle this sparsityproblem using data augmentations and a simple contrastive learning approach.ConFit first creates an augmented resume-job dataset by paraphrasing specificsections in a resume or a job post. Then, ConFit uses contrastive learning tofurther increase training samples from B pairs per batch to O(B2) perbatch. We evaluate ConFit on two real-world datasets and find it outperformsprior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31%absolute in nDCG@10 for ranking jobs and ranking resumes, respectively."
    },
    {
        "link": "https://arxiv.org/abs/2401.16350",
        "title": "FedFair^3: Unlocking Threefold Fairness in Federated Learning",
        "authors": [
            "Simin Javaherian",
            "Sanjeev Panta",
            "Shelby Williams",
            "Md Sirajul Islam",
            "Li Chen"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) is an emerging paradigm in machine learning withoutexposing clients' raw data. In practical scenarios with numerous clients,encouraging fair and efficient client participation in federated learning is ofutmost importance, which is also challenging given the heterogeneity in datadistribution and device properties. Existing works have proposed differentclient-selection methods that consider fairness; however, they fail to selectclients with high utilities while simultaneously achieving fair accuracylevels. In this paper, we propose a fair client-selection approach that unlocksthreefold fairness in federated learning. In addition to having a fairclient-selection strategy, we enforce an equitable number of rounds for clientparticipation and ensure a fair accuracy distribution over the clients. Theexperimental results demonstrate that FedFair^3, in comparison to thestate-of-the-art baselines, achieves 18.15% less accuracy variance on the IIDdata and 54.78% on the non-IID data, without decreasing the global accuracy.Furthermore, it shows 24.36% less wall-clock training time on average."
    },
    {
        "link": "https://arxiv.org/abs/2401.16352",
        "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization",
        "authors": [
            "Guang Lin",
            "Chao Li",
            "Jianhai Zhang",
            "Toshihisa Tanaka",
            "Qibin Zhao"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The deep neural networks are known to be vulnerable to well-designedadversarial attacks. The most successful defense technique based on adversarialtraining (AT) can achieve optimal robustness against particular attacks butcannot generalize well to unseen attacks. Another effective defense techniquebased on adversarial purification (AP) can enhance generalization but cannotachieve optimal robustness. Meanwhile, both methods share one common limitationon the degraded standard accuracy. To mitigate these issues, we propose a novelframework called Adversarial Training on Purification (AToP), which comprisestwo components: perturbation destruction by random transforms (RT) and purifiermodel fine-tuned (FT) by adversarial loss. RT is essential to avoidoverlearning to known attacks resulting in the robustness generalization tounseen attacks and FT is essential for the improvement of robustness. Toevaluate our method in an efficient and scalable way, we conduct extensiveexperiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that ourmethod achieves state-of-the-art results and exhibits generalization abilityagainst unseen attacks."
    },
    {
        "link": "https://arxiv.org/abs/2401.16353",
        "title": "Empirical and Theoretical Analysis of Liquid Staking Protocols",
        "authors": [
            "Krzysztof Gogol",
            "Benjamin Kraner",
            "Malte Schlosser",
            "Tao Yan",
            "Claudio Tessone",
            "Burkhard Stiller"
        ],
        "primary_subject": "Cryptography and Security (cs.CR)",
        "abstract": "Liquid staking has become the largest category of decentralized financeprotocols in terms of total value locked. However, few studies exist on itsimplementation designs or underlying risks. The liquid staking protocols allowfor earning staking rewards without the disadvantage of locking the capital atthe validators. Yet, they are seen by some as a threat to the Proof-of-Stakeblockchain security.This paper is the first work that classifies liquid staking implementations.It analyzes the historical performance of major liquid staking tokens incomparison to the traditional staking for the largest Proof-of-Stakeblockchains. Furthermore, the research investigates the impact ofcentralization, maximum extractable value and the migration of Ethereum fromProof-of-Work to Proof-of-Stake on the tokens' performance. Examining thetracking error of the liquid stacking providers to the staking rewards showsthat they are persistent and cannot be explained by macro-variables of thecurrency, such as the variance or return."
    },
    {
        "link": "https://arxiv.org/abs/2401.16355",
        "title": "PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology",
        "authors": [
            "Yuxuan Sun",
            "Hao Wu",
            "Chenglu Zhu",
            "Sunyi Zheng",
            "Qizi Chen",
            "Kai Zhang",
            "Yunlong Zhang",
            "Xiaoxiao Lan",
            "Mengyue Zheng",
            "Jingxiong Li",
            "Xinheng Lyu",
            "Tao Lin",
            "Lin Yang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The emergence of large multimodal models has unlocked remarkable potential inAI, particularly in pathology. However, the lack of specialized, high-qualitybenchmark impeded their development and precise evaluation. To address this, weintroduce PathMMU, the largest and highest-quality expert-validated pathologybenchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and21,599 images from various sources, and an explanation for the correct answeraccompanies each question. The construction of PathMMU capitalizes on therobust capabilities of GPT-4V, utilizing approximately 30,000 gatheredimage-caption pairs to generate Q\\&As. Significantly, to maximize PathMMU'sauthority, we invite six pathologists to scrutinize each question under strictstandards in PathMMU's validation and test sets, while simultaneously settingan expert-level performance benchmark for PathMMU. We conduct extensiveevaluations, including zero-shot assessments of 14 open-sourced and threeclosed-sourced LMMs and their robustness to image corruption. We also fine-tunerepresentative LMMs to assess their adaptability to PathMMU. The empiricalfindings indicate that advanced LMMs struggle with the challenging PathMMUbenchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\%zero-shot performance, significantly lower than the 71.4\\% demonstrated byhuman pathologists. After fine-tuning, even open-sourced LMMs can surpassGPT-4V with a performance of over 60\\%, but still fall short of the expertiseshown by pathologists. We hope that the PathMMU will offer valuable insightsand foster the development of more specialized, next-generation LLMs forpathology."
    },
    {
        "link": "https://arxiv.org/abs/2401.16359",
        "title": "Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus",
        "authors": [
            "Jack Culbert",
            "Anne Hobert",
            "Najko Jahn",
            "Nick Haupka",
            "Marion Schmidt",
            "Paul Donner",
            "Philipp Mayr"
        ],
        "primary_subject": "Digital Libraries (cs.DL)",
        "abstract": "OpenAlex is a promising open source of scholarly metadata, and competitor tothe established proprietary sources, the Web of Science and Scopus. As OpenAlexprovides its data freely and openly, it permits researchers to performbibliometric studies that can be reproduced in the community without licensingbarriers. However, as OpenAlex is a rapidly evolving source and the datacontained within is expanding and also quickly changing, the question naturallyarises as to the trustworthiness of its data. In this empirical paper, we willstudy the reference and metadata coverage within each database and compare themwith each other to help address this open question in bibliometrics. In ourlarge-scale study, we demonstrate that, when restricted to a cleaned dataset of16,788,282 recent publications shared by all three databases, OpenAlex hasaverage reference numbers comparable to both Web of Science and Scopus. We alsodemonstrate that the comparison of other core metadata covered by OpenAlexshows mixed results, with OpenAlex capturing more ORCID identifiers, fewerabstracts and a similar number of Open Access information per article whencompared to both Web of Science and Scopus."
    },
    {
        "link": "https://arxiv.org/abs/2401.16366",
        "title": "Choiceless Polynomial Space",
        "authors": [
            "Flavio Ferrarotti",
            "Klaus-Dieter Schewe"
        ],
        "primary_subject": "Logic in Computer Science (cs.LO)",
        "abstract": "Abstract State Machines (ASMs) provide a model of computations on structuresrather than strings. Blass, Gurevich and Shelah showed that deterministicPTIME-bounded ASMs define the choiceless fragment of PTIME, but cannot capturePTIME. In this article deterministic PSPACE-bounded ASMs are introduced, and itis proven that they cannot capture PSPACE. The key for the proof is acharacterisation by partial fixed-point formulae over the St\\\"ark/Nanchen logicfor deterministic ASMs and a construction of transitive structures, in whichsuch formulae must hold. This construction exploits that the decisive supporttheorem for choiceless polynomial time holds under slightly weaker assumptions."
    },
    {
        "link": "https://arxiv.org/abs/2401.16367",
        "title": "TQCompressor: improving tensor decomposition methods in neural networks via permutations",
        "authors": [
            "V. Abronin",
            "A. Naumov",
            "D. Mazur",
            "D. Bystrov",
            "K. Tsarova",
            "Ar. Melnikov",
            "I. Oseledets",
            "S. Dolgov",
            "R. Brasher",
            "M. Perelshtein"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "We introduce TQCompressor, a novel method for neural network modelcompression with improved tensor decompositions. We explore the challengesposed by the computational and storage demands of pre-trained language modelsin NLP tasks and propose a permutation-based enhancement to Kroneckerdecomposition. This enhancement makes it possible to reduce loss in modelexpressivity which is usually associated with factorization. We demonstratethis method applied to the GPT-2small. The result of the compression isTQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. inthe GPT-2small. We make TQCompressedGPT-2 publicly available. We furtherenhance the performance of the TQCompressedGPT-2 through a training strategyinvolving multi-step knowledge distillation, using only a 3.1% of theOpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparativeevaluations, marking an advancement in the efficient and effective deploymentof models in resource-constrained environments."
    },
    {
        "link": "https://arxiv.org/abs/2401.16368",
        "title": "A new numerical method for scalar eigenvalue problems in heterogeneous, dispersive, sign-changing materials",
        "authors": [
            "Martin Halla",
            "Thorsten Hohage",
            "Florian Oberender"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We consider time-harmonic scalar transmission problems between dielectric anddispersive materials with generalized Lorentz frequency laws. For certainfrequency ranges such equations involve a sign-change in their principle part.Due to the resulting loss of coercivity properties, the numerical simulation ofsuch problems is demanding. Furthermore, the related eigenvalue problems arenonlinear and give rise to additional challenges. We present a new finiteelement method for both of these types of problems, which is based on a weaklycoercive reformulation of the PDE. The new scheme can handleC1,1-interfaces consisting piecewise of elementary geometries. Neglectingquadrature errors, the method allows for a straightforward convergenceanalysis. In our implementation we apply a simple, but nonstandard quadraturerule to achieve negligible quadrature errors. We present computationalexperiments in 2D and 3D for both source and eigenvalue problems which confirmthe stability and convergence of the new scheme."
    },
    {
        "link": "https://arxiv.org/abs/2401.16369",
        "title": "Mixed-Order Meshes through rp-adaptivity for Surface Fitting to Implicit Geometries",
        "authors": [
            "Ketan Mittal",
            "Veselin A. Dobrev",
            "Patrick Knupp",
            "Tzanio Kolev",
            "Franck Ledoux",
            "Claire Roche",
            "Vladimir Z. Tomov"
        ],
        "primary_subject": "Mathematical Software (cs.MS)",
        "abstract": "Computational analysis with the finite element method requires geometricallyaccurate meshes. It is well known that high-order meshes can accurately capturecurved surfaces with fewer degrees of freedom in comparison to low-ordermeshes. Existing techniques for high-order mesh generation typically outputmeshes with same polynomial order for all elements. However, high orderelements away from curvilinear boundaries or interfaces increase thecomputational cost of the simulation without increasing geometric accuracy. Inprior work, we have presented one such approach for generating body-fitteduniform-order meshes that takes a given mesh and morphs it to align with thesurface of interest prescribed as the zero isocontour of a level-set function.We extend this method to generate mixed-order meshes such that curved surfacesof the domain are discretized with high-order elements, while low-orderelements are used elsewhere. Numerical experiments demonstrate the robustnessof the approach and show that it can be used to generate mixed-order meshesthat are much more efficient than high uniform-order meshes. The proposedapproach is purely algebraic, and extends to different types of elements(quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions."
    },
    {
        "link": "https://arxiv.org/abs/2401.16373",
        "title": "Bayesian optimization as a flexible and efficient design framework for sustainable process systems",
        "authors": [
            "Joel A. Paulson",
            "Calvin Tsay"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Bayesian optimization (BO) is a powerful technology for optimizing noisyexpensive-to-evaluate black-box functions, with a broad range of real-worldapplications in science, engineering, economics, manufacturing, and beyond. Inthis paper, we provide an overview of recent developments, challenges, andopportunities in BO for design of next-generation process systems. Afterdescribing several motivating applications, we discuss how advanced BO methodshave been developed to more efficiently tackle important problems in theseapplications. We conclude the paper with a summary of challenges andopportunities related to improving the quality of the probabilistic model, thechoice of internal optimization procedure used to select the next sample point,and the exploitation of problem structure to improve sample efficiency."
    },
    {
        "link": "https://arxiv.org/abs/2401.16375",
        "title": "Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator",
        "authors": [
            "Jieru Lin",
            "Danqing Huang",
            "Tiejun Zhao",
            "Dechen Zhan",
            "Chin-Yew Lin"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Layout generation is a critical step in graphic design to achieve meaningfulcompositions of elements. Most previous works view it as a sequence generationproblem by concatenating element attribute tokens (i.e., category, size,position). So far the autoregressive approach (AR) has achieved promisingresults, but is still limited in global context modeling and suffers from errorpropagation since it can only attend to the previously generated tokens. Recentnon-autoregressive attempts (NAR) have shown competitive results, whichprovides a wider context range and the flexibility to refine with iterativedecoding. However, current works only use simple heuristics to recognizeerroneous tokens for refinement which is inaccurate. This paper first conductsan in-depth analysis to better understand the difference between the AR and NARframework. Furthermore, based on our observation that pixel space is moresensitive in capturing spatial patterns of graphic layouts (e.g., overlap,alignment), we propose a learning-based locator to detect erroneous tokenswhich takes the wireframe image rendered from the generated layout sequence asinput. We show that it serves as a complementary modality to the elementsequence in object space and contributes greatly to the overall performance.Experiments on two public datasets show that our approach outperforms both ARand NAR baselines. Extensive studies further prove the effectiveness ofdifferent modules with interesting findings. Our code will be available athttps://github.com/ffffatgoose/SpotError."
    },
    {
        "link": "https://arxiv.org/abs/2401.16379",
        "title": "Exponentially Fitted Finite Difference Approximation for Singularly Perturbed Fredholm Integro-Differential Equation",
        "authors": [
            "Mehebub Alam",
            "Rajni Kant Pandey"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "In this paper, we concentrate on solving second-order singularly perturbedFredholm integro-differential equations (SPFIDEs). It is well known thatsolving these equations analytically is a challenging endeavor because of thepresence of boundary and interior layers within the domain. To overcome thesechallenges, we develop a fitted second-order difference scheme that can capturethe layer behavior of the solution accurately and efficiently, which is again,based on the integral identities with exponential basis functions, thecomposite trapezoidal rule, and an appropriate interpolating quadrature ruleswith the remainder terms in the integral form on a piecewise uniform mesh.Hence, our numerical method acts as a superior alternative to the existingmethods in the literature. Further, using appropriate techniques in erroranalysis the scheme's convergence and stability have been studied in thediscrete max norm. We have provided necessary experimental evidence thatcorroborates the theoretical results with a high degree of accuracy."
    },
    {
        "link": "https://arxiv.org/abs/2401.16380",
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
        "authors": [
            "Pratyush Maini",
            "Skyler Seto",
            "He Bai",
            "David Grangier",
            "Yizhe Zhang",
            "Navdeep Jaitly"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Large language models are trained on massive scrapes of the web, which areoften unstructured, noisy, and poorly phrased. Current scaling laws show thatlearning from such data requires an abundance of both compute and data, whichgrows with the size of the model being trained. This is infeasible both becauseof the large compute costs and duration associated with pre-training, and theimpending scarcity of high-quality data on the web. In this work, we proposeWeb Rephrase Augmented Pre-training (WRAP) that uses anoff-the-shelf instruction-tuned model prompted to paraphrase documents on theweb in specific styles such as \"like Wikipedia\" or in \"question-answer format\"to jointly pre-train LLMs on real and synthetic rephrases. First, we show thatusing WRAP on the C4 dataset, which is naturally noisy, speeds up pre-trainingby \u223c3x. At the same pre-training compute budget, it improves perplexity bymore than 10% on average across different subsets of the Pile, and improveszero-shot question answer accuracy across 13 tasks by more than 2%. Second, weinvestigate the impact of the re-phrasing style on the performance of themodel, offering insights into how the composition of the training data canimpact the performance of LLMs in OOD settings. Our gains are attributed to thefact that re-phrased synthetic data has higher utility than just real databecause it (i) incorporates style diversity that closely reflects downstreamevaluation style, and (ii) has higher 'quality' than web-scraped data."
    },
    {
        "link": "https://arxiv.org/abs/2401.16382",
        "title": "A KDM-Based Approach for Architecture Conformance Checking in Adaptive Systems",
        "authors": [
            "Daniel San Mart\u00edn",
            "Guisella Angulo",
            "Valter Vieira de Camargo"
        ],
        "primary_subject": "Software Engineering (cs.SE)",
        "abstract": "Adaptive Systems (ASs) are capable to monitor their behavior and makeadjustments when quality goals are not achieved through the MAPE-K, a widelyrecognized reference model that offers abstractions for designing ASs. Bymaking these abstractions evident in the system structure, numerous benefitsemerge, particularly in terms of enhancing the architecture's maintenance andcomprehensibility. However, it is observed that many existing ASs are notdesigned in accordance with MAPE-K, causing these abstractions to remain hiddenin their architecture. To address this issue, Architectural ConformanceChecking (ACC) emerges as a valuable technique for verifying whether thecurrent architecture (CA) of a system adheres to the rules prescribed by theplanned architecture (PA) or a reference model, such as MAPE-K. In this paper,we present REMEDY, a domain-specific approach that encompasses thespecification of the planned adaptive architecture based on the MAPE-Kreference model, the recovery of the current adaptive architecture, theconformance checking process, and architecture visualizations. Furthermore, ourapproach is specifically tailored for ASs, incorporating well-known rules fromthe MAPE-K model. The evaluation of the REMEDY DSL involves a comparison with ageneral-purpose DSL, and the results demonstrate improvements in productivity.REMEDY facilitates the identification and correction of architecturalnon-conformance issues, thereby enhancing the overall quality of adaptivesystems."
    },
    {
        "link": "https://arxiv.org/abs/2401.16383",
        "title": "Learning logic programs by finding minimal unsatisfiable subprograms",
        "authors": [
            "Andrew Cropper",
            "C\u00e9line Hocquette"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "The goal of inductive logic programming (ILP) is to search for a logicprogram that generalises training examples and background knowledge. Weintroduce an ILP approach that identifies minimal unsatisfiable subprograms(MUSPs). We show that finding MUSPs allows us to efficiently and soundly prunethe search space. Our experiments on multiple domains, including programsynthesis and game playing, show that our approach can reduce learning times by99%."
    },
    {
        "link": "https://arxiv.org/abs/2401.16386",
        "title": "Continual Learning with Pre-Trained Models: A Survey",
        "authors": [
            "Da-Wei Zhou",
            "Hai-Long Sun",
            "Jingyi Ning",
            "Han-Jia Ye",
            "De-Chuan Zhan"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Nowadays, real-world applications often face streaming data, which requiresthe learning system to absorb new knowledge as data evolves. Continual Learning(CL) aims to achieve this goal and meanwhile overcome the catastrophicforgetting of former knowledge when learning new ones. Typical CL methods buildthe model from scratch to grow with incoming data. However, the advent of thepre-trained model (PTM) era has sparked immense research interest, particularlyin leveraging PTMs' robust representational capabilities. This paper presents acomprehensive survey of the latest advancements in PTM-based CL. We categorizeexisting methodologies into three distinct groups, providing a comparativeanalysis of their similarities, differences, and respective advantages anddisadvantages. Additionally, we offer an empirical study contrasting variousstate-of-the-art methods to highlight concerns regarding fairness incomparisons. The source code to reproduce these evaluations is available at:https://github.com/sun-hailong/LAMDA-PILOT"
    },
    {
        "link": "https://arxiv.org/abs/2401.16387",
        "title": "Green Adaptation of Real-Time Web Services for Industrial CPS within a Cloud Environment",
        "authors": [
            "Teresa Higuera",
            "Jos\u00e9 L. Risco-Mart\u00edn",
            "Patricia Arroba",
            "Jos\u00e9 L. Ayala"
        ],
        "primary_subject": "Hardware Architecture (cs.AR)",
        "abstract": "Managing energy efficiency under timing constraints is an interesting and bigchallenge. This work proposes an accurate power model in data centers fortime-constrained servers in Cloud computing. This model, as opposed to previousapproaches, does not only consider the workload assigned to the processingelement, but also incorporates the need of considering the static powerconsumption and, even more interestingly, its dependency with temperature. Theproposed model has been used in a multi-objective optimization environment inwhich the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignmenthave been efficiently optimized."
    },
    {
        "link": "https://arxiv.org/abs/2401.16390",
        "title": "Quantum Private Membership Aggregation",
        "authors": [
            "Alptug Aytekin",
            "Mohamed Nomeir",
            "Sennur Ulukus"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We consider the problem of private set membership aggregation of N partiesby using an entangled quantum state. In this setting, the N parties, whichshare an entangled state, aim to \\emph{privately} know the number of times eachelement (message) is repeated among the N parties, with respect to auniversal set K. This problem has applications in privatecomparison, ranking, voting, etc. We propose an encoding algorithm that mapsthe classical information into distinguishable quantum states, along with adecoding algorithm that exploits the distinguishability of the mapped states.The proposed scheme can also be used to calculate the N party privatesummation modulo P."
    },
    {
        "link": "https://arxiv.org/abs/2401.16391",
        "title": "Practical Framework for Problem-Based Learning in an Introductory Circuit Analysis Course",
        "authors": [
            "Sebastian Martin",
            "Salvador Pineda",
            "Juan Perez-Ruiz",
            "Natalia Alguacil",
            "Antonio Ruiz-Gonzalez"
        ],
        "primary_subject": "Systems and Control (eess.SY)",
        "abstract": "Introductory courses on electric circuits at under- graduate level areusually presented in quite abstract terms, with questions and problems quitefar from practical problems. This causes the students have difficulties toapply that theory to solve practical technical problems. On the other hand,electric circuits are everywhere in our lives, so we have plenty of realpractical problems. Here we compile a selection of practical contexts suitedfor implementing Problem Based Learning approach in an introductory course oncircuit analysis. And some examples describing the gamification process thatuses these problems to build single-player role-playing games that fulfil thecourse contents and scheduling. The key point of the assessment and how it isrelated to the progress in the game is also described."
    },
    {
        "link": "https://arxiv.org/abs/2401.16393",
        "title": "Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River Contraction",
        "authors": [
            "Fabien H Wagner",
            "Samuel Favrichon",
            "Ricardo Dalagnol",
            "Mayumi CM Hirye",
            "Adugna Mullissa",
            "Sassan Saatchi"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Amazon, the world's largest rainforest, faces a severe historic drought.The Rio Negro River, one of the major Amazon River tributaries, reaches itslowest level in a century in October 2023. Here, we used a U-net deep learningmodel to map water surfaces in the Rio Negro River basin every 12 days in 2022and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. Theaccuracy of the water surface model was high with an F1-score of 0.93. The 12days mosaic time series of water surface was generated from the Sentinel-1prediction. The water surface mask demonstrated relatively consistent agreementwith the Global Surface Water (GSW) product from Joint Research Centre(F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score:0.686). The main errors of the map were omission errors in flooded woodland, inflooded shrub and because of clouds. Rio Negro water surfaces reached theirlowest level around the 25th of November 2023 and were reduced to 68.1\\%(9,559.9 km2) of the maximum water surfaces observed in the period 2022-2023(14,036.3 km2). Synthetic Aperture Radar (SAR) data, in conjunction withdeep learning techniques, can significantly improve near real-time mapping ofwater surface in tropical regions."
    },
    {
        "link": "https://arxiv.org/abs/2401.16395",
        "title": "Deciding Subtyping for Asynchronous Multiparty Sessions",
        "authors": [
            "Elaine Li",
            "Felix Stutz",
            "Thomas Wies"
        ],
        "primary_subject": "Formal Languages and Automata Theory (cs.FL)",
        "abstract": "Multiparty session types (MSTs) are a type-based approach to verifyingcommunication protocols, represented as global types in the framework. Wepresent a precise subtyping relation for asynchronous MSTs with communicatingstate machines (CSMs) as implementation model. We address two problems: whencan a local implementation safely substitute another, and when does anarbitrary CSM implement a global type? We define safety with respect to a givenglobal type, in terms of subprotocol fidelity and deadlock freedom. Ourimplementation model subsumes existing work which considers local types withrestricted choice. We exploit the connection between MST subtyping andrefinement to formulate concise conditions that are directly checkable on thecandidate implementations, and use them to show that both problems aredecidable in polynomial time."
    },
    {
        "link": "https://arxiv.org/abs/2401.16398",
        "title": "Zero-shot Imitation Policy via Search in Demonstration Dataset",
        "authors": [
            "Federco Malato",
            "Florian Leopold",
            "Andrew Melnik",
            "Ville Hautamaki"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "Behavioral cloning uses a dataset of demonstrations to learn a policy. Toovercome computationally expensive training procedures and address the policyadaptation problem, we propose to use latent spaces of pre-trained foundationmodels to index a demonstration dataset, instantly access similar relevantexperiences, and copy behavior from these situations. Actions from a selectedsimilar situation can be performed by the agent until representations of theagent's current situation and the selected experience diverge in the latentspace. Thus, we formulate our control problem as a dynamic search problem overa dataset of experts' demonstrations. We test our approach on BASALTMineRL-dataset in the latent representation of a Video Pre-Training model. Wecompare our model to state-of-the-art, Imitation Learning-based Minecraftagents. Our approach can effectively recover meaningful demonstrations and showhuman-like behavior of an agent in the Minecraft environment in a wide varietyof scenarios. Experimental results reveal that performance of our search-basedapproach clearly wins in terms of accuracy and perceptual evaluation overlearning-based models."
    },
    {
        "link": "https://arxiv.org/abs/2401.16399",
        "title": "Single-Winner Voting with Alliances: Avoiding the Spoiler Effect",
        "authors": [
            "Grzegorz Pierczy\u0144ski",
            "Stanis\u0142aw Szufa"
        ],
        "primary_subject": "Computer Science and Game Theory (cs.GT)",
        "abstract": "We study the setting of single-winner elections with ordinal preferenceswhere candidates might be members of \\emph{alliances} (which may correspond toe.g., political parties, factions, or coalitions). However, we do not assumethat candidates from the same alliance are necessarily adjacent in voters'rankings. In such case, every classical voting rule is vulnerable to thespoiler effect, i.e., the presence of a candidate may harm his or her alliance.We therefore introduce a new idea of \\emph{alliance-aware} voting rules whichextend the classical ones. We show that our approach is superior both to usingclassical cloneproof voting rules and to running primaries within alliancesbefore the election.We introduce several alliance-aware voting rules and show that they satisfythe most desirable standard properties of their classical counterparts as wellas newly introduced axioms for the model with alliances which, e.g., excludethe possibility of the spoiler effect. Our rules have natural definitions andare simple enough to explain to be used in practice."
    },
    {
        "link": "https://arxiv.org/abs/2401.16402",
        "title": "A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect",
        "authors": [
            "Yunkang Cao",
            "Xiaohao Xu",
            "Jiangning Zhang",
            "Yuqi Cheng",
            "Xiaonan Huang",
            "Guansong Pang",
            "Weiming Shen"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from theconcept of normality in visual data, widely applied across diverse domains,e.g., industrial defect inspection, and medical lesion detection. This surveycomprehensively examines recent advancements in VAD by identifying threeprimary challenges: 1) scarcity of training data, 2) diversity of visualmodalities, and 3) complexity of hierarchical anomalies. Starting with a briefoverview of the VAD background and its generic concept definitions, weprogressively categorize, emphasize, and discuss the latest VAD progress fromthe perspective of sample number, data modality, and anomaly hierarchy. Throughan in-depth analysis of the VAD field, we finally summarize future developmentsfor VAD and conclude the key findings and contributions of this survey."
    },
    {
        "link": "https://arxiv.org/abs/2401.16403",
        "title": "ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text",
        "authors": [
            "Thanh-Nhi Nguyen",
            "Thanh-Phong Le",
            "Kiet Van Nguyen"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Lexical normalization, a fundamental task in Natural Language Processing(NLP), involves the transformation of words into their canonical forms. Thisprocess has been proven to benefit various downstream NLP tasks greatly. Inthis work, we introduce Vietnamese Lexical Normalization (ViLexNorm), thefirst-ever corpus developed for the Vietnamese lexical normalization task. Thecorpus comprises over 10,000 pairs of sentences meticulously annotated by humanannotators, sourced from public comments on Vietnam's most popular social mediaplatforms. Various methods were used to evaluate our corpus, and thebest-performing system achieved a result of 57.74% using the Error ReductionRate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline.For extrinsic evaluation, employing the model trained on ViLexNorm demonstratesthe positive impact of the Vietnamese lexical normalization task on other NLPtasks. Our corpus is publicly available exclusively for research purposes."
    },
    {
        "link": "https://arxiv.org/abs/2401.16405",
        "title": "Scaling Sparse Fine-Tuning to Large Language Models",
        "authors": [
            "Alan Ansell",
            "Ivan Vuli\u0107",
            "Hannah Sterz",
            "Anna Korhonen",
            "Edoardo M. Ponti"
        ],
        "primary_subject": "Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) are difficult to fully fine-tune (e.g., withinstructions or human feedback) due to their sheer number of parameters. Afamily of parameter-efficient sparse fine-tuning (SFT) methods have provenpromising in terms of performance but their memory requirements increaseproportionally to the size of the LLMs. In this work, we scale sparsefine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any giventime, for a desired density level, we maintain an array of parameter indicesand the deltas of these parameters relative to their pretrained values. Weiterate among: (a) updating the active deltas, (b) pruning indices (based onthe change of magnitude of their deltas) and (c) regrowth of indices. Forregrowth, we explore two criteria based on either the accumulated gradients ofa few candidate parameters or their approximate momenta estimated using theefficient SM3 optimizer. We experiment with instruction-tuning of LLMs onstandard dataset mixtures, finding that SFT is often superior to popularparameter-efficient fine-tuning methods like LoRA (low-rank adaptation) interms of performance and comparable in terms of run time. We additionally showthat SFT is compatible with both quantization and efficient optimizers, tofacilitate scaling to ever-larger model sizes. We release the code for SFT athttps://github.com/AlanAnsell/peft and for the instruction-tuning experimentsat https://github.com/ducdauge/sft-llm."
    },
    {
        "link": "https://arxiv.org/abs/2401.16411",
        "title": "Sparse Discrete Empirical Interpolation Method: State Estimation from Few Sensors",
        "authors": [
            "Mohammad Farazmand"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "Discrete empirical interpolation method (DEIM) estimates a function from itsincomplete pointwise measurements. Unfortunately, DEIM suffers largeinterpolation errors when few measurements are available. Here, we introduceSparse DEIM (S-DEIM) for accurately estimating a function even when very fewmeasurements are available. To this end, S-DEIM leverages a kernel vector whichhas been neglected in previous DEIM-based methods. We derive theoretical errorestimates for S-DEIM, showing its relatively small error when an optimal kernelvector is used. When the function is generated by a continuous-time dynamicalsystem, we propose a data assimilation algorithm which approximates the optimalkernel vector using observational time series. We prove that, under certainconditions, data assimilated S-DEIM converges exponentially fast towards thetrue state. We demonstrate the efficacy of our method on two numericalexamples."
    },
    {
        "link": "https://arxiv.org/abs/2401.16412",
        "title": "Learning to Manipulate under Limited Information",
        "authors": [
            "Wesley H. Holliday",
            "Alexander Kristoffersen",
            "Eric Pacuit"
        ],
        "primary_subject": "Artificial Intelligence (cs.AI)",
        "abstract": "By classic results in social choice theory, any reasonable preferentialvoting method sometimes gives individuals an incentive to report an insincerepreference. The extent to which different voting methods are more or lessresistant to such strategic manipulation has become a key consideration forcomparing voting methods. Here we measure resistance to manipulation by whetherneural networks of varying sizes can learn to profitably manipulate a givenvoting method in expectation, given different types of limited informationabout how other voters will vote. We trained nearly 40,000 neural networks of26 sizes to manipulate against 8 different voting methods, under 6 types oflimited information, in committee-sized elections with 5-21 voters and 3-6candidates. We find that some voting methods, such as Borda, are highlymanipulable by networks with limited information, while others, such as InstantRunoff, are not, despite being quite profitably manipulated by an idealmanipulator with full information."
    },
    {
        "link": "https://arxiv.org/abs/2401.16413",
        "title": "The geometric error is less than the pollution error when solving the high-frequency Helmholtz equation with high-order FEM on curved domains",
        "authors": [
            "Th\u00e9ophile Chaumont-Frelet",
            "Euan A. Spence"
        ],
        "primary_subject": "Numerical Analysis (math.NA)",
        "abstract": "We consider the h-version of the finite-element method, where accuracy isincreased by decreasing the meshwidth h while keeping the polynomial degreep constant, applied to the Helmholtz equation. Although the question \"howquickly must h decrease as the wavenumber k increases to maintainaccuracy?\" has been studied intensively since the 1990s, none of the existingrigorous wavenumber-explicit analyses take into account the approximation ofthe geometry. In this paper we prove that for nontrapping problems solved usingstraight elements the geometric error is order kh, which is then less thanthe pollution error k(kh)2p when k is large; this fact is thenillustrated in numerical experiments. More generally, we prove that, even forproblems with strong trapping, using degree four (in 2-d) or degree five (in3-d) polynomials and isoparametric elements ensures that the geometric error issmaller than the pollution error for most large wavenumbers."
    },
    {
        "link": "https://arxiv.org/abs/2401.16414",
        "title": "A Causal Model for Quantifying Multipartite Classical and Quantum Correlations",
        "authors": [
            "Shuchan Wang",
            "Gerhard Wunder"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We give an operational definition of information-theoretic resources within agiven multipartite classical or quantum correlation. We present our causalmodel that serves as the source coding side of this correlation and introduce anovel concept of resource rate. We argue that, beyond classical secrecy,additional resources exist that are useful for the security of distributedcomputing problems, which can be captured by the resource rate. Furthermore, weestablish a relationship between resource rate and an extension of Shannon'slogarithmic information measure, namely, total correlation. Subsequently, wepresent a novel quantum secrecy monotone and investigate a quantum hybrid keydistribution system as an extension of our causal model. Finally, we discusssome connections to optimal transport (OT) problem."
    },
    {
        "link": "https://arxiv.org/abs/2401.16416",
        "title": "Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting",
        "authors": [
            "Yiming Huang",
            "Beilei Cui",
            "Long Bai",
            "Ziqi Guo",
            "Mengya Xu",
            "Hongliang Ren"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the realm of robot-assisted minimally invasive surgery, dynamic scenereconstruction can significantly enhance downstream tasks and improve surgicaloutcomes. Neural Radiance Fields (NeRF)-based methods have recently risen toprominence for their exceptional ability to reconstruct scenes. Nonetheless,these methods are hampered by slow inference, prolonged training, andsubstantial computational demands. Additionally, some rely on stereo depthestimation, which is often infeasible due to the high costs and logisticalchallenges associated with stereo cameras. Moreover, the monocularreconstruction quality for deformable scenes is currently inadequate. Toovercome these obstacles, we present Endo-4DGS, an innovative, real-timeendoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting(GS) and requires no ground truth depth data. This method extends 3D GS byincorporating a temporal component and leverages a lightweight MLP to capturetemporal Gaussian deformations. This effectively facilitates the reconstructionof dynamic surgical scenes with variable conditions. We also integrateDepth-Anything to generate pseudo-depth maps from monocular views, enhancingthe depth-guided reconstruction process. Our approach has been validated on twosurgical datasets, where it has proven to render in real-time, computeefficiently, and reconstruct with remarkable accuracy. These results underlinethe vast potential of Endo-4DGS to improve surgical assistance."
    },
    {
        "link": "https://arxiv.org/abs/2401.16417",
        "title": "Channel Coding with Mean and Variance Cost Constraints",
        "authors": [
            "Adeel Mahmood",
            "Aaron B. Wagner"
        ],
        "primary_subject": "Information Theory (cs.IT)",
        "abstract": "We consider channel coding for discrete memoryless channels (DMCs) with anovel cost constraint that constrains both the mean and the variance of thecost of the codewords. We show that the maximum (asymptotically) achievablerate under the new cost formulation is equal to the capacity-cost function; inparticular, the strong converse holds. We further characterize the optimalsecond-order coding rate of these cost-constrained codes; in particular, theoptimal second-order coding rate is finite. We then show that the second-ordercoding performance is strictly improved with feedback using a new variation oftimid/bold coding, significantly broadening the applicability of timid/boldcoding schemes from unconstrained compound-dispersion channels to allcost-constrained channels. Equivalent results on the minimum averageprobability of error are also given."
    },
    {
        "link": "https://arxiv.org/abs/2401.16419",
        "title": "Semi-parametric Expert Bayesian Network Learning with Gaussian Processes and Horseshoe Priors",
        "authors": [
            "Yidou Weng",
            "Finale Doshi-Velez"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "This paper proposes a model learning Semi-parametric rela- tionships in anExpert Bayesian Network (SEBN) with linear parameter and structure constraints.We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin-ear components. To prioritize modifying the expert graph over adding new edges,we optimize differential Horseshoe scales. In real-world datasets with unknowntruth, we gen- erate diverse graphs to accommodate user input, addressingidentifiability issues and enhancing interpretability. Evalua- tion onsynthetic and UCI Liver Disorders datasets, using metrics like structuralHamming Distance and test likelihood, demonstrates our models outperformstate-of-the-art semi- parametric Bayesian Network model."
    },
    {
        "link": "https://arxiv.org/abs/2401.16420",
        "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
        "authors": [
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Bin Wang",
            "Linke Ouyang",
            "Xilin Wei",
            "Songyang Zhang",
            "Haodong Duan",
            "Maosong Cao",
            "Wenwei Zhang",
            "Yining Li",
            "Hang Yan",
            "Yang Gao",
            "Xinyue Zhang",
            "Wei Li",
            "Jingwen Li",
            "Kai Chen",
            "Conghui He",
            "Xingcheng Zhang",
            "Yu Qiao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce InternLM-XComposer2, a cutting-edge vision-language modelexcelling in free-form text-image composition and comprehension. This modelgoes beyond conventional vision-language understanding, adeptly craftinginterleaved text-image content from diverse inputs like outlines, detailedtextual specifications, and reference images, enabling highly customizablecontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approachthat applies additional LoRA parameters exclusively to image tokens to preservethe integrity of pre-trained language knowledge, striking a balance betweenprecise vision understanding and text composition with literary talent.Experimental results demonstrate the superiority of InternLM-XComposer2 basedon InternLM2-7B in producing high-quality long-text multi-modal content and itsexceptional vision-language understanding performance across variousbenchmarks, where it not only significantly outperforms existing multimodalmodels but also matches or even surpasses GPT-4V and Gemini Pro in certainassessments. This highlights its remarkable proficiency in the realm ofmultimodal understanding. The InternLM-XComposer2 model series with 7Bparameters are publicly available athttps://github.com/InternLM/InternLM-XComposer."
    },
    {
        "link": "https://arxiv.org/abs/2401.16421",
        "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
        "authors": [
            "Zhenyu He",
            "Guhao Feng",
            "Shengjie Luo",
            "Kai Yang",
            "Di He",
            "Jingjing Xu",
            "Zhi Zhang",
            "Hongxia Yang",
            "Liwei Wang"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "In this work, we leverage the intrinsic segmentation of language sequencesand design a new positional encoding method called Bilevel Positional Encoding(BiPE). For each position, our BiPE blends an intra-segment encoding and aninter-segment encoding. The intra-segment encoding identifies the locationswithin a segment and helps the model capture the semantic information thereinvia absolute positional encoding. The inter-segment encoding specifies thesegment index, models the relationships between segments, and aims to improveextrapolation capabilities via relative positional encoding. Theoreticalanalysis shows this disentanglement of positional information makes learningmore effective. The empirical results also show that our BiPE has superiorlength extrapolation capabilities across a wide range of tasks in diverse textmodalities."
    },
    {
        "link": "https://arxiv.org/abs/2401.16422",
        "title": "Strategic Usage in a Multi-Learner Setting",
        "authors": [
            "Eliot Shekhtman",
            "Sarah Dean"
        ],
        "primary_subject": "Machine Learning (cs.LG)",
        "abstract": "Real-world systems often involve some pool of users choosing between a set ofservices. With the increase in popularity of online learning algorithms, theseservices can now self-optimize, leveraging data collected on users to maximizesome reward such as service quality. On the flipside, users may strategicallychoose which services to use in order to pursue their own reward functions, inthe process wielding power over which services can see and use their data.Extensive prior research has been conducted on the effects of strategic usersin single-service settings, with strategic behavior manifesting in themanipulation of observable features to achieve a desired classification;however, this can often be costly or unattainable for users and fails tocapture the full behavior of multi-service dynamic systems. As such, we analyzea setting in which strategic users choose among several available services inorder to pursue positive classifications, while services seek to minimize lossfunctions on their observations. We focus our analysis on realizable settings,and show that naive retraining can still lead to oscillation even if all usersare observed at different times; however, if this retraining uses memory ofpast observations, convergent behavior can be guaranteed for certain lossfunction classes. We provide results obtained from synthetic and real-worlddata to empirically validate our theoretical findings."
    },
    {
        "link": "https://arxiv.org/abs/2401.16423",
        "title": "Synchformer: Efficient Synchronization from Sparse Cues",
        "authors": [
            "Vladimir Iashin",
            "Weidi Xie",
            "Esa Rahtu",
            "Andrew Zisserman"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Our objective is audio-visual synchronization with a focus on 'in-the-wild'videos, such as those on YouTube, where synchronization cues can be sparse. Ourcontributions include a novel audio-visual synchronization model, and trainingthat decouples feature extraction from synchronization modelling throughmulti-modal segment-level contrastive pre-training. This approach achievesstate-of-the-art performance in both dense and sparse settings. We also extendsynchronization model training to AudioSet a million-scale 'in-the-wild'dataset, investigate evidence attribution techniques for interpretability, andexplore a new capability for synchronization models: audio-visualsynchronizability."
    },
    {
        "link": "https://arxiv.org/abs/2401.16424",
        "title": "Computer Vision for Primate Behavior Analysis in the Wild",
        "authors": [
            "Richard Vogg",
            "Timo L\u00fcddecke",
            "Jonathan Henrich",
            "Sharmita Dey",
            "Matthias Nuske",
            "Valentin Hassler",
            "Derek Murphy",
            "Julia Fischer",
            "Julia Ostner",
            "Oliver Sch\u00fclke",
            "Peter M. Kappeler",
            "Claudia Fichtel",
            "Alexander Gail",
            "Stefan Treue",
            "Hansj\u00f6rg Scherberger",
            "Florentin W\u00f6rg\u00f6tter",
            "Alexander S. Ecker"
        ],
        "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Advances in computer vision as well as increasingly widespread video-basedbehavioral monitoring have great potential for transforming how we study animalcognition and behavior. However, there is still a fairly large gap between theexciting prospects and what can actually be achieved in practice today,especially in videos from the wild. With this perspective paper, we want tocontribute towards closing this gap, by guiding behavioral scientists in whatcan be expected from current methods and steering computer vision researcherstowards problems that are relevant to advance research in animal behavior. Westart with a survey of the state-of-the-art methods for computer visionproblems that are directly relevant to the video-based study of animalbehavior, including object detection, multi-individual tracking, (inter)actionrecognition and individual identification. We then review methods foreffort-efficient learning, which is one of the biggest challenges from apractical perspective. Finally, we close with an outlook into the future of theemerging field of computer vision for animal behavior, where we argue that thefield should move fast beyond the common frame-by-frame processing and treatvideo as a first-class citizen."
    }
]